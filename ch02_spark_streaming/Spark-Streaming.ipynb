{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Initialize pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing pyspark\n",
      ":: loading settings :: url = jar:file:/usr/local/spark-3.2.1-bin-hadoop3.2/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.apache.spark#spark-streaming-kafka-0-8_2.11 added as a dependency\n",
      "org.mongodb.spark#mongo-spark-connector_2.11 added as a dependency\n",
      "com.datastax.spark#spark-cassandra-connector_2.11 added as a dependency\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      "org.apache.spark#spark-avro_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-fe9c56c6-5c49-468e-9db3-ecb2941e3f67;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-streaming-kafka-0-8_2.11;2.0.2 in central\n",
      "\tfound org.apache.kafka#kafka_2.11;0.8.2.1 in central\n",
      "\tfound org.scala-lang.modules#scala-xml_2.11;1.0.2 in central\n",
      "\tfound com.yammer.metrics#metrics-core;2.2.0 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.16 in spark-list\n",
      "\tfound org.scala-lang.modules#scala-parser-combinators_2.11;1.0.2 in central\n",
      "\tfound com.101tec#zkclient;0.3 in central\n",
      "\tfound log4j#log4j;1.2.17 in spark-list\n",
      "\tfound org.apache.kafka#kafka-clients;0.8.2.1 in central\n",
      "\tfound net.jpountz.lz4#lz4;1.3.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.2.6 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in spark-list\n",
      "\tfound org.mongodb.spark#mongo-spark-connector_2.11;2.4.3 in central\n",
      "\tfound org.mongodb#mongo-java-driver;3.12.5 in central\n",
      "\tfound com.datastax.spark#spark-cassandra-connector_2.11;2.5.2 in central\n",
      "\tfound com.datastax.spark#spark-cassandra-connector-driver_2.11;2.5.2 in central\n",
      "\tfound com.datastax.oss#java-driver-core-shaded;4.10.0 in central\n",
      "\tfound com.datastax.oss#native-protocol;1.4.12 in central\n",
      "\tfound com.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 in central\n",
      "\tfound com.typesafe#config;1.4.1 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.26 in central\n",
      "\tfound io.dropwizard.metrics#metrics-core;4.1.16 in central\n",
      "\tfound org.hdrhistogram#HdrHistogram;2.1.12 in central\n",
      "\tfound org.reactivestreams#reactive-streams;1.0.3 in central\n",
      "\tfound com.github.stephenc.jcip#jcip-annotations;1.0-1 in central\n",
      "\tfound com.github.spotbugs#spotbugs-annotations;3.1.12 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.2 in spark-list\n",
      "\tfound com.datastax.oss#java-driver-mapper-runtime;4.10.0 in central\n",
      "\tfound com.datastax.oss#java-driver-query-builder;4.10.0 in central\n",
      "\tfound org.apache.commons#commons-lang3;3.5 in spark-list\n",
      "\tfound com.thoughtworks.paranamer#paranamer;2.8 in spark-list\n",
      "\tfound org.scala-lang#scala-reflect;2.11.12 in spark-list\n",
      "\t[2.11.12] org.scala-lang#scala-reflect;2.11.12\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.2.1 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.2.1 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.8.0 in central\n",
      "\tfound org.lz4#lz4-java;1.7.1 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.4 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.1 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.1 in central\n",
      "\tfound org.apache.htrace#htrace-core4;4.1.0-incubating in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.6.2 in central\n",
      "\tfound org.apache.spark#spark-avro_2.12;3.2.1 in central\n",
      "\tfound org.tukaani#xz;1.8 in central\n",
      ":: resolution report :: resolve 3528ms :: artifacts dl 21ms\n",
      "\t:: modules in use:\n",
      "\tcom.101tec#zkclient;0.3 from central in [default]\n",
      "\tcom.datastax.oss#java-driver-core-shaded;4.10.0 from central in [default]\n",
      "\tcom.datastax.oss#java-driver-mapper-runtime;4.10.0 from central in [default]\n",
      "\tcom.datastax.oss#java-driver-query-builder;4.10.0 from central in [default]\n",
      "\tcom.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 from central in [default]\n",
      "\tcom.datastax.oss#native-protocol;1.4.12 from central in [default]\n",
      "\tcom.datastax.spark#spark-cassandra-connector-driver_2.11;2.5.2 from central in [default]\n",
      "\tcom.datastax.spark#spark-cassandra-connector_2.11;2.5.2 from central in [default]\n",
      "\tcom.github.spotbugs#spotbugs-annotations;3.1.12 from central in [default]\n",
      "\tcom.github.stephenc.jcip#jcip-annotations;1.0-1 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.2 from spark-list in [default]\n",
      "\tcom.thoughtworks.paranamer#paranamer;2.8 from spark-list in [default]\n",
      "\tcom.typesafe#config;1.4.1 from central in [default]\n",
      "\tcom.yammer.metrics#metrics-core;2.2.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\tio.dropwizard.metrics#metrics-core;4.1.16 from central in [default]\n",
      "\tlog4j#log4j;1.2.17 from spark-list in [default]\n",
      "\torg.apache.commons#commons-lang3;3.5 from spark-list in [default]\n",
      "\torg.apache.commons#commons-pool2;2.6.2 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.1 from central in [default]\n",
      "\torg.apache.htrace#htrace-core4;4.1.0-incubating from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.8.0 from central in [default]\n",
      "\torg.apache.kafka#kafka_2.11;0.8.2.1 from central in [default]\n",
      "\torg.apache.spark#spark-avro_2.12;3.2.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.2.1 from central in [default]\n",
      "\torg.apache.spark#spark-streaming-kafka-0-8_2.11;2.0.2 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.2.1 from central in [default]\n",
      "\torg.hdrhistogram#HdrHistogram;2.1.12 from central in [default]\n",
      "\torg.lz4#lz4-java;1.7.1 from central in [default]\n",
      "\torg.mongodb#mongo-java-driver;3.12.5 from central in [default]\n",
      "\torg.mongodb.spark#mongo-spark-connector_2.11;2.4.3 from central in [default]\n",
      "\torg.reactivestreams#reactive-streams;1.0.3 from central in [default]\n",
      "\torg.scala-lang#scala-reflect;2.11.12 from spark-list in [default]\n",
      "\torg.scala-lang.modules#scala-parser-combinators_2.11;1.0.2 from central in [default]\n",
      "\torg.scala-lang.modules#scala-xml_2.11;1.0.2 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from spark-list in [default]\n",
      "\torg.tukaani#xz;1.8 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.4 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\torg.slf4j#slf4j-api;1.7.16 by [org.slf4j#slf4j-api;1.7.26] in [default]\n",
      "\torg.apache.kafka#kafka-clients;0.8.2.1 by [org.apache.kafka#kafka-clients;2.8.0] in [default]\n",
      "\tnet.jpountz.lz4#lz4;1.3.0 transitively in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.2.6 by [org.xerial.snappy#snappy-java;1.1.8.4] in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.26 by [org.slf4j#slf4j-api;1.7.30] in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 by [com.google.code.findbugs#jsr305;3.0.2] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   46  |   6   |   6   |   6   ||   40  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      "\n",
      ":: problems summary ::\n",
      ":::: ERRORS\n",
      "\tunknown resolver null\n",
      "\n",
      "\tunknown resolver null\n",
      "\n",
      "\tunknown resolver null\n",
      "\n",
      "\tunknown resolver null\n",
      "\n",
      "\tunknown resolver null\n",
      "\n",
      "\tunknown resolver null\n",
      "\n",
      "\n",
      ":: USE VERBOSE OR DEBUG MESSAGE LEVEL FOR MORE DETAILS\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-fe9c56c6-5c49-468e-9db3-ecb2941e3f67\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 40 already retrieved (0kB/11ms)\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pyspark initialized\n"
     ]
    }
   ],
   "source": [
    "import os, sys, json, io\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.utils import StreamingQueryException\n",
    "import sys\n",
    "import json\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = '/usr/bin/python3'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = '/usr/bin/python3'\n",
    "sys.path.append('/class')\n",
    "\n",
    "# Kafka variables\n",
    "brokers = 'localhost:9092'\n",
    "kafka_topic = 'stocks-json'\n",
    "receiver_sleep_time = 4\n",
    "\n",
    "# Connect to Spark \n",
    "if not 'sc' in locals():\n",
    "    from initspark import initspark\n",
    "    sc, spark, config = initspark()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create a helper function to stream to a memory table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_memory(df, queryname = 'debug', mode = \"append\"):\n",
    "    # modes are: complete, update, append\n",
    "\n",
    "    # if queryname in spark.catalog.listTables():\n",
    "    #     spark.catalog.dropTempView(queryname)\n",
    "    \n",
    "    query = (df.writeStream \n",
    "            .format(\"memory\")\n",
    "            .queryName(queryname)\n",
    "            .outputMode(mode)\n",
    "            .start()\n",
    "            )\n",
    "    return query\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a streaming source and create a temp view to receive the results for debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (spark.readStream \n",
    "    .format(\"kafka\") \n",
    "    .option(\"kafka.bootstrap.servers\", brokers) \n",
    "    .option(\"subscribe\", kafka_topic) \n",
    "    .option(\"startingOffsets\", \"earliest\")\n",
    "    .option(\"failOnDataLoss\", False)\n",
    "    .load()\n",
    "    )\n",
    "\n",
    "# df.createOrReplaceTempView('table')\n",
    "# df1 = spark.sql(\"\"\"SELECT 'new data' as newfield, * from table\"\"\")\n",
    "\n",
    "df1 = df.selectExpr(\"UPPER(CAST(value AS STRING)) as value\")\n",
    "\n",
    "debug1 = write_memory(df1, 'debug1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query from the memory stream like it's a tempory view using `spark.sql`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(value='{\"EVENT_TIME\": \"2022-02-18 01:18:17\", \"SYMBOL\": \"AAPL\", \"PRICE\": 268.36, \"QUANTITY\": 621}'),\n",
       " Row(value='{\"EVENT_TIME\": \"2022-02-18 01:18:17\", \"SYMBOL\": \"GOOG\", \"PRICE\": 287.68, \"QUANTITY\": 462}'),\n",
       " Row(value='{\"EVENT_TIME\": \"2022-02-18 01:18:17\", \"SYMBOL\": \"MSFT\", \"PRICE\": 253.72, \"QUANTITY\": 123}'),\n",
       " Row(value='{\"EVENT_TIME\": \"2022-02-18 01:18:21\", \"SYMBOL\": \"MSFT\", \"PRICE\": 164.02, \"QUANTITY\": 296}'),\n",
       " Row(value='{\"EVENT_TIME\": \"2022-02-18 01:18:21\", \"SYMBOL\": \"AAPL\", \"PRICE\": 233.43, \"QUANTITY\": 732}'),\n",
       " Row(value='{\"EVENT_TIME\": \"2022-02-18 01:18:21\", \"SYMBOL\": \"GOOG\", \"PRICE\": 175.03, \"QUANTITY\": 475}'),\n",
       " Row(value='{\"EVENT_TIME\": \"2022-02-18 01:18:25\", \"SYMBOL\": \"GOOG\", \"PRICE\": 137.65, \"QUANTITY\": 650}'),\n",
       " Row(value='{\"EVENT_TIME\": \"2022-02-18 01:18:25\", \"SYMBOL\": \"AAPL\", \"PRICE\": 256.29, \"QUANTITY\": 641}'),\n",
       " Row(value='{\"EVENT_TIME\": \"2022-02-18 01:18:25\", \"SYMBOL\": \"MSFT\", \"PRICE\": 177.63, \"QUANTITY\": 390}'),\n",
       " Row(value='{\"EVENT_TIME\": \"2022-02-18 01:18:29\", \"SYMBOL\": \"GOOG\", \"PRICE\": 133.6, \"QUANTITY\": 888}')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"select * from debug1\").take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You can stop and restart a memory stream whenever you like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug1.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug1 = write_memory(df1, 'debug1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from debug1\").take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark SQL magic is also quite helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext sparksql_magic\n",
    "# pip install sparksql-magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr style=\"border-bottom: 1px solid\"><td style=\"font-weight: bold\">value</td></tr><tr><td>{&quot;EVENT_TIME&quot;: &quot;2022-02-18 01:18:17&quot;, &quot;SYMBOL&quot;: &quot;AAPL&quot;, &quot;PRICE&quot;: 268.36, &quot;QUANTITY&quot;: 621}</td></tr><tr><td>{&quot;EVENT_TIME&quot;: &quot;2022-02-18 01:18:17&quot;, &quot;SYMBOL&quot;: &quot;GOOG&quot;, &quot;PRICE&quot;: 287.68, &quot;QUANTITY&quot;: 462}</td></tr><tr><td>{&quot;EVENT_TIME&quot;: &quot;2022-02-18 01:18:17&quot;, &quot;SYMBOL&quot;: &quot;MSFT&quot;, &quot;PRICE&quot;: 253.72, &quot;QUANTITY&quot;: 123}</td></tr><tr><td>{&quot;EVENT_TIME&quot;: &quot;2022-02-18 01:18:21&quot;, &quot;SYMBOL&quot;: &quot;AAPL&quot;, &quot;PRICE&quot;: 233.43, &quot;QUANTITY&quot;: 732}</td></tr><tr><td>{&quot;EVENT_TIME&quot;: &quot;2022-02-18 01:18:21&quot;, &quot;SYMBOL&quot;: &quot;GOOG&quot;, &quot;PRICE&quot;: 175.03, &quot;QUANTITY&quot;: 475}</td></tr><tr><td>{&quot;EVENT_TIME&quot;: &quot;2022-02-18 01:18:21&quot;, &quot;SYMBOL&quot;: &quot;MSFT&quot;, &quot;PRICE&quot;: 164.02, &quot;QUANTITY&quot;: 296}</td></tr><tr><td>{&quot;EVENT_TIME&quot;: &quot;2022-02-18 01:18:25&quot;, &quot;SYMBOL&quot;: &quot;AAPL&quot;, &quot;PRICE&quot;: 256.29, &quot;QUANTITY&quot;: 641}</td></tr><tr><td>{&quot;EVENT_TIME&quot;: &quot;2022-02-18 01:18:25&quot;, &quot;SYMBOL&quot;: &quot;GOOG&quot;, &quot;PRICE&quot;: 137.65, &quot;QUANTITY&quot;: 650}</td></tr><tr><td>{&quot;EVENT_TIME&quot;: &quot;2022-02-18 01:18:25&quot;, &quot;SYMBOL&quot;: &quot;MSFT&quot;, &quot;PRICE&quot;: 177.63, &quot;QUANTITY&quot;: 390}</td></tr><tr><td>{&quot;EVENT_TIME&quot;: &quot;2022-02-18 01:18:29&quot;, &quot;SYMBOL&quot;: &quot;AAPL&quot;, &quot;PRICE&quot;: 197.82, &quot;QUANTITY&quot;: 918}</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sparksql\n",
    "select * from debug1 order by value limit 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop a memory stream when you don't need it, as it can consume a lot of memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug1.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's try reading AVRO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stock_schema {\n",
      "    \"namespace\": \"stock.avro\",\n",
      "    \"type\": \"record\",\n",
      "    \"name\": \"Stock\",\n",
      "    \"fields\": [\n",
      "        {\"name\": \"event_time\", \"type\": \"string\"},\n",
      "        {\"name\": \"symbol\",  \"type\": \"string\"},\n",
      "        {\"name\": \"price\", \"type\": \"float\"},\n",
      "        {\"name\": \"quantity\", \"type\": \"int\"}\n",
      "    ]\n",
      "}\n",
      "stock_struct StructType(List(StructField(event_time,StringType,true),StructField(symbol,StringType,true),StructField(price,FloatType,true),StructField(quantity,IntegerType,true)))\n"
     ]
    }
   ],
   "source": [
    "stock_schema = open(\"stock.avsc\", \"r\").read()\n",
    "print('stock_schema', stock_schema)\n",
    "stock_struct = spark.read.format(\"avro\").option(\"avroSchema\", stock_schema).load().schema\n",
    "print('stock_struct', stock_struct)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df DataFrame[key: binary, value: binary, topic: string, partition: int, offset: bigint, timestamp: timestamp, timestampType: int]\n"
     ]
    }
   ],
   "source": [
    "brokers = 'localhost:9092'\n",
    "kafka_topic = 'stocks-avro'\n",
    "receiver_sleep_time = 4\n",
    "\n",
    "df = (spark.readStream \n",
    "    .format(\"kafka\") \n",
    "    .option(\"kafka.bootstrap.servers\", brokers) \n",
    "    .option(\"subscribe\", kafka_topic) \n",
    "    .option(\"startingOffsets\", \"latest\")\n",
    "    .option(\"failOnDataLoss\", False)\n",
    "    .load()\n",
    "    )\n",
    "print('df', df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug2 = write_memory(df, 'debug2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr style=\"border-bottom: 1px solid\"><td style=\"font-weight: bold\">timestamp</td><td style=\"font-weight: bold\">key</td><td style=\"font-weight: bold\">value</td></tr><tr><td>2022-02-19 03:54:37.857000</td><td>bytearray(b&#x27;\\xdaV\\x8f\\x16B\\xf6H\\x94\\xbctZ&quot;\\x8fD[\\xdb&#x27;)</td><td>bytearray(b&#x27;Obj\\x01\\x04\\x14avro.codec\\x08null\\x16avro.schema\\xc6\\x03{&quot;type&quot;: &quot;record&quot;, &quot;name&quot;: &quot;Stock&quot;, &quot;namespace&quot;: &quot;stock.avro&quot;, &quot;fields&quot;: [{&quot;type&quot;: &quot;string&quot;, &quot;name&quot;: &quot;event_time&quot;}, {&quot;type&quot;: &quot;string&quot;, &quot;name&quot;: &quot;symbol&quot;}, {&quot;type&quot;: &quot;float&quot;, &quot;name&quot;: &quot;price&quot;}, {&quot;type&quot;: &quot;int&quot;, &quot;name&quot;: &quot;quantity&quot;}]}\\x00-\\xa7nw\\xd73o\\x968\\x99C\\xc2s#\\x8bJ\\x02&gt;&amp;2022-02-19 03:54:37\\x08GOOGq\\xdd\\x8eC\\xb6\\x0e-\\xa7nw\\xd73o\\x968\\x99C\\xc2s#\\x8bJ&#x27;)</td></tr><tr><td>2022-02-19 03:54:36.897000</td><td>bytearray(b&#x27;\\xc4j\\n\\xaa\\x86\\xe6B\\x0b\\xb6\\xde\\xd8\\xb4\\x08Wl &#x27;)</td><td>bytearray(b&#x27;Obj\\x01\\x04\\x14avro.codec\\x08null\\x16avro.schema\\xc6\\x03{&quot;type&quot;: &quot;record&quot;, &quot;name&quot;: &quot;Stock&quot;, &quot;namespace&quot;: &quot;stock.avro&quot;, &quot;fields&quot;: [{&quot;type&quot;: &quot;string&quot;, &quot;name&quot;: &quot;event_time&quot;}, {&quot;type&quot;: &quot;string&quot;, &quot;name&quot;: &quot;symbol&quot;}, {&quot;type&quot;: &quot;float&quot;, &quot;name&quot;: &quot;price&quot;}, {&quot;type&quot;: &quot;int&quot;, &quot;name&quot;: &quot;quantity&quot;}]}\\x00\\xa8\\x98\\x86/H\\xf3B\\xc5\\x1a\\x03Z\\xad\\xe7/S\\x9a\\x02&gt;&amp;2022-02-19 03:54:36\\x08GOOG\\x85k-C\\xdc\\t\\xa8\\x98\\x86/H\\xf3B\\xc5\\x1a\\x03Z\\xad\\xe7/S\\x9a&#x27;)</td></tr><tr><td>2022-02-19 03:54:36.890000</td><td>bytearray(b&#x27;\\xfbnDK\\xed\\x8bH\\xcc\\x92\\xe3+\\x15\\x95\\xfd\\xb3\\x80&#x27;)</td><td>bytearray(b&#x27;Obj\\x01\\x04\\x14avro.codec\\x08null\\x16avro.schema\\xc6\\x03{&quot;type&quot;: &quot;record&quot;, &quot;name&quot;: &quot;Stock&quot;, &quot;namespace&quot;: &quot;stock.avro&quot;, &quot;fields&quot;: [{&quot;type&quot;: &quot;string&quot;, &quot;name&quot;: &quot;event_time&quot;}, {&quot;type&quot;: &quot;string&quot;, &quot;name&quot;: &quot;symbol&quot;}, {&quot;type&quot;: &quot;float&quot;, &quot;name&quot;: &quot;price&quot;}, {&quot;type&quot;: &quot;int&quot;, &quot;name&quot;: &quot;quantity&quot;}]}\\x00\\xb5\\xad\\xba\\x1b ,\\xfe!\\xff)\\xe8\\xf1\\x82ndV\\x02&gt;&amp;2022-02-19 03:54:36\\x08AAPL\\xd7cwC\\xa0\\x05\\xb5\\xad\\xba\\x1b ,\\xfe!\\xff)\\xe8\\xf1\\x82ndV&#x27;)</td></tr><tr><td>2022-02-19 03:54:36.846000</td><td>bytearray(b&#x27;\\xe2L&quot;\\xb5`0J\\xa8\\x87Bj%\\xa2F\\xe7\\xe7&#x27;)</td><td>bytearray(b&#x27;Obj\\x01\\x04\\x14avro.codec\\x08null\\x16avro.schema\\xc6\\x03{&quot;type&quot;: &quot;record&quot;, &quot;name&quot;: &quot;Stock&quot;, &quot;namespace&quot;: &quot;stock.avro&quot;, &quot;fields&quot;: [{&quot;type&quot;: &quot;string&quot;, &quot;name&quot;: &quot;event_time&quot;}, {&quot;type&quot;: &quot;string&quot;, &quot;name&quot;: &quot;symbol&quot;}, {&quot;type&quot;: &quot;float&quot;, &quot;name&quot;: &quot;price&quot;}, {&quot;type&quot;: &quot;int&quot;, &quot;name&quot;: &quot;quantity&quot;}]}\\x00\\x03A&gt;\\xe4c\\x92E\\x1f\\xfa\\xdfR!\\xe1\\xc7\\xf9\\xa2\\x02&gt;&amp;2022-02-19 03:54:36\\x08MSFT\\xaeGKC\\xec\\x04\\x03A&gt;\\xe4c\\x92E\\x1f\\xfa\\xdfR!\\xe1\\xc7\\xf9\\xa2&#x27;)</td></tr><tr><td>2022-02-19 03:54:34.062000</td><td>bytearray(b&#x27;3\\x8e\\xf7\\xa1l.II\\xb1*\\x13(R%QF&#x27;)</td><td>bytearray(b&#x27;Obj\\x01\\x04\\x14avro.codec\\x08null\\x16avro.schema\\xc6\\x03{&quot;type&quot;: &quot;record&quot;, &quot;name&quot;: &quot;Stock&quot;, &quot;namespace&quot;: &quot;stock.avro&quot;, &quot;fields&quot;: [{&quot;type&quot;: &quot;string&quot;, &quot;name&quot;: &quot;event_time&quot;}, {&quot;type&quot;: &quot;string&quot;, &quot;name&quot;: &quot;symbol&quot;}, {&quot;type&quot;: &quot;float&quot;, &quot;name&quot;: &quot;price&quot;}, {&quot;type&quot;: &quot;int&quot;, &quot;name&quot;: &quot;quantity&quot;}]}\\x00\\xd0&lt;\\x0e\\x96eY\\xe0;=\\xee&quot;\\xfd\\xdd\\xf1\\xcb=\\x02&lt;&amp;2022-02-19 03:54:34\\x08MSFT\\xd7#\\xfcBZ\\xd0&lt;\\x0e\\x96eY\\xe0;=\\xee&quot;\\xfd\\xdd\\xf1\\xcb=&#x27;)</td></tr><tr><td>2022-02-19 03:54:33.889000</td><td>bytearray(b&#x27;\\x00\\xb2\\xcd\\xe4\\xd6\\xddC\\x16\\xaeLC \\x980\\x8cQ&#x27;)</td><td>bytearray(b&#x27;Obj\\x01\\x04\\x14avro.codec\\x08null\\x16avro.schema\\xc6\\x03{&quot;type&quot;: &quot;record&quot;, &quot;name&quot;: &quot;Stock&quot;, &quot;namespace&quot;: &quot;stock.avro&quot;, &quot;fields&quot;: [{&quot;type&quot;: &quot;string&quot;, &quot;name&quot;: &quot;event_time&quot;}, {&quot;type&quot;: &quot;string&quot;, &quot;name&quot;: &quot;symbol&quot;}, {&quot;type&quot;: &quot;float&quot;, &quot;name&quot;: &quot;price&quot;}, {&quot;type&quot;: &quot;int&quot;, &quot;name&quot;: &quot;quantity&quot;}]}\\x00\\x87\\xae\\xafn0(b4\\x12=\\xab\\x14\\x17\\xf9M\\xcb\\x02&gt;&amp;2022-02-19 03:54:33\\x08AAPL)\\x9cWC\\xde\\x05\\x87\\xae\\xafn0(b4\\x12=\\xab\\x14\\x17\\xf9M\\xcb&#x27;)</td></tr><tr><td>2022-02-19 03:54:33.853000</td><td>bytearray(b&#x27;\\x85\\xfc\\xac\\xad5\\x8cJ\\xc5\\xbc7\\x0e\\xd0pU\\xc8 &#x27;)</td><td>bytearray(b&#x27;Obj\\x01\\x04\\x14avro.codec\\x08null\\x16avro.schema\\xc6\\x03{&quot;type&quot;: &quot;record&quot;, &quot;name&quot;: &quot;Stock&quot;, &quot;namespace&quot;: &quot;stock.avro&quot;, &quot;fields&quot;: [{&quot;type&quot;: &quot;string&quot;, &quot;name&quot;: &quot;event_time&quot;}, {&quot;type&quot;: &quot;string&quot;, &quot;name&quot;: &quot;symbol&quot;}, {&quot;type&quot;: &quot;float&quot;, &quot;name&quot;: &quot;price&quot;}, {&quot;type&quot;: &quot;int&quot;, &quot;name&quot;: &quot;quantity&quot;}]}\\x00\\xaf2hBM\\x1c\\xdb*+F\\xfd5\\xaf\\xdb\\x06+\\x02&gt;&amp;2022-02-19 03:54:33\\x08GOOG\\x9aYAC\\xd2\\x04\\xaf2hBM\\x1c\\xdb*+F\\xfd5\\xaf\\xdb\\x06+&#x27;)</td></tr><tr><td>2022-02-19 03:54:32.894000</td><td>bytearray(b&#x27;\\xf2\\x13B\\x95\\xc4uI\\x12\\x89\\x08)V\\x1eB\\xf5\\xf9&#x27;)</td><td>bytearray(b&#x27;Obj\\x01\\x04\\x14avro.codec\\x08null\\x16avro.schema\\xc6\\x03{&quot;type&quot;: &quot;record&quot;, &quot;name&quot;: &quot;Stock&quot;, &quot;namespace&quot;: &quot;stock.avro&quot;, &quot;fields&quot;: [{&quot;type&quot;: &quot;string&quot;, &quot;name&quot;: &quot;event_time&quot;}, {&quot;type&quot;: &quot;string&quot;, &quot;name&quot;: &quot;symbol&quot;}, {&quot;type&quot;: &quot;float&quot;, &quot;name&quot;: &quot;price&quot;}, {&quot;type&quot;: &quot;int&quot;, &quot;name&quot;: &quot;quantity&quot;}]}\\x00\\x04\\xd4\\x98\\x1d\\xab\\xd35C$\\xe3\\x1d\\xd1\\xac\\x80\\x08^\\x02&gt;&amp;2022-02-19 03:54:32\\x08GOOG\\xec\\x91&amp;C\\x86\\n\\x04\\xd4\\x98\\x1d\\xab\\xd35C$\\xe3\\x1d\\xd1\\xac\\x80\\x08^&#x27;)</td></tr><tr><td>2022-02-19 03:54:32.885000</td><td>bytearray(b&#x27;\\x87w\\xc9Tu\\x96N\\x96\\xa2/aZ\\xd3\\xca\\x1eN&#x27;)</td><td>bytearray(b&#x27;Obj\\x01\\x04\\x14avro.codec\\x08null\\x16avro.schema\\xc6\\x03{&quot;type&quot;: &quot;record&quot;, &quot;name&quot;: &quot;Stock&quot;, &quot;namespace&quot;: &quot;stock.avro&quot;, &quot;fields&quot;: [{&quot;type&quot;: &quot;string&quot;, &quot;name&quot;: &quot;event_time&quot;}, {&quot;type&quot;: &quot;string&quot;, &quot;name&quot;: &quot;symbol&quot;}, {&quot;type&quot;: &quot;float&quot;, &quot;name&quot;: &quot;price&quot;}, {&quot;type&quot;: &quot;int&quot;, &quot;name&quot;: &quot;quantity&quot;}]}\\x00\\x1b!\\xbc\\x1bL\\xe0-s*\\xe2\\xb85lI\\xe7\\x11\\x02&gt;&amp;2022-02-19 03:54:32\\x08AAPL\\xe1:%C\\xb6\\x02\\x1b!\\xbc\\x1bL\\xe0-s*\\xe2\\xb85lI\\xe7\\x11&#x27;)</td></tr><tr><td>2022-02-19 03:54:32.841000</td><td>bytearray(b&#x27;\\x01\\xb8v|\\xa5qKe\\x9a\\xea|T\\xf2c\\x19[&#x27;)</td><td>bytearray(b&#x27;Obj\\x01\\x04\\x14avro.codec\\x08null\\x16avro.schema\\xc6\\x03{&quot;type&quot;: &quot;record&quot;, &quot;name&quot;: &quot;Stock&quot;, &quot;namespace&quot;: &quot;stock.avro&quot;, &quot;fields&quot;: [{&quot;type&quot;: &quot;string&quot;, &quot;name&quot;: &quot;event_time&quot;}, {&quot;type&quot;: &quot;string&quot;, &quot;name&quot;: &quot;symbol&quot;}, {&quot;type&quot;: &quot;float&quot;, &quot;name&quot;: &quot;price&quot;}, {&quot;type&quot;: &quot;int&quot;, &quot;name&quot;: &quot;quantity&quot;}]}\\x00\\x01{\\xc1\\xf5\\xbbu\\xe3\\xd7\\x01\\xaa\\xd7E{\\xe3_L\\x02&gt;&amp;2022-02-19 03:54:32\\x08MSFT\\n\\x17\\&#x27;C\\xfe\\n\\x01{\\xc1\\xf5\\xbbu\\xe3\\xd7\\x01\\xaa\\xd7E{\\xe3_L&#x27;)</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%sparksql\n",
    "select timestamp, key, value from debug2 order by timestamp desc limit 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConsumerRecord(topic='stocks-avro', partition=0, offset=40679, timestamp=1645243362535, timestamp_type=0, key=b'\\xd6\\x0cgMs<By\\xb8\\xcaR\\x02\\xe0\\xfa\\x93\\x14', value=b'Obj\\x01\\x04\\x14avro.codec\\x08null\\x16avro.schema\\xc6\\x03{\"type\": \"record\", \"name\": \"Stock\", \"namespace\": \"stock.avro\", \"fields\": [{\"type\": \"string\", \"name\": \"event_time\"}, {\"type\": \"string\", \"name\": \"symbol\"}, {\"type\": \"float\", \"name\": \"price\"}, {\"type\": \"int\", \"name\": \"quantity\"}]}\\x00$\\x7f#w\\xeaD\\xdchK\\xc5!\\xf85\\x10\\xc8J\\x02>&2022-02-19 04:02:42\\x08MSFT\\xa4p\\xdfB\\xf0\\t$\\x7f#w\\xeaD\\xdchK\\xc5!\\xf85\\x10\\xc8J', headers=[], checksum=None, serialized_key_size=16, serialized_value_size=328, serialized_header_size=-1)\n",
    "ConsumerRecord(topic='stocks-avro', partition=0, offset=40679, timestamp=1645243362535, timestamp_type=0, key=b'\\xd6\\x0cgMs<By\\xb8\\xcaR\\x02\\xe0\\xfa\\x93\\x14', value=b'Obj\\x01\\x04\\x14avro.codec\\x08null\\x16avro.schema\\xc6\\x03{\"type\": \"record\", \"name\": \"Stock\", \"namespace\": \"stock.avro\", \"fields\": [{\"type\": \"string\", \"name\": \"event_time\"}, {\"type\": \"string\", \"name\": \"symbol\"}, {\"type\": \"float\", \"name\": \"price\"}, {\"type\": \"int\", \"name\": \"quantity\"}]}\\x00$\\x7f#w\\xeaD\\xdchK\\xc5!\\xf85\\x10\\xc8J\\x02>&2022-02-19 04:02:42\\x08MSFT\\xa4p\\xdfB\\xf0\\t$\\x7f#w\\xeaD\\xdchK\\xc5!\\xf85\\x10\\xc8J', headers=[], checksum=None, serialized_key_size=16, serialized_value_size=328, serialized_header_size=-1)\n",
    "\n",
    "ConsumerRecord(topic='stocks-avro', partition=0, offset=40814, timestamp=1645243470645, timestamp_type=0, key=b'\\xc8\\xeb\\xc2\\xe9O\\xaaJ\\x86\\x83\\x85\\xb9\\xd7\\xf46\\xea\\x8f', value=b'Obj\\x01\\x04\\x14avro.codec\\x08null\\x16avro.schema\\xc6\\x03{\"type\": \"record\", \"name\": \"Stock\", \"namespace\": \"stock.avro\", \"fields\": [{\"type\": \"string\", \"name\": \"event_time\"}, {\"type\": \"string\", \"name\": \"symbol\"}, {\"type\": \"float\", \"name\": \"price\"}, {\"type\": \"int\", \"name\": \"quantity\"}]}\\x00\\xe8\\xf5x\\r\\xbf\\x8aC\\x98&\\xaf\\x13iz\\x9dp\\x13\\x02>&2022-02-19 04:04:30\\x08MSFT\\xc3\\xf5\\x11C\\xac\\x04\\xe8\\xf5x\\r\\xbf\\x8aC\\x98&\\xaf\\x13iz\\x9dp\\x13', headers=[], checksum=None, serialized_key_size=16, serialized_value_size=328, serialized_header_size=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/02/19 03:59:03 ERROR WriteToDataSourceV2Exec: Data source write support org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite@65c97253 is aborting.\n",
      "22/02/19 03:59:03 ERROR WriteToDataSourceV2Exec: Data source write support org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite@65c97253 aborted.\n",
      "22/02/19 03:59:03 ERROR Utils: Aborting task\n",
      "org.apache.spark.TaskKilledException\n",
      "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:216)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:36)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:412)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:457)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:358)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "22/02/19 03:59:03 ERROR DataWritingSparkTask: Aborting commit for partition 0 (task 542, attempt 0, stage 542.0)\n",
      "22/02/19 03:59:03 ERROR DataWritingSparkTask: Aborted commit for partition 0 (task 542, attempt 0, stage 542.0)\n"
     ]
    }
   ],
   "source": [
    "debug2.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df3 DataFrame[timestamp: timestamp, key: binary, value: struct<event_time:string,symbol:string,price:float,quantity:int>]\n"
     ]
    }
   ],
   "source": [
    "if 'debug3' in locals():\n",
    "    debug3.stop()\n",
    "    \n",
    "from pyspark.sql.avro.functions import from_avro, to_avro\n",
    "stock_schema = \"\"\"{\n",
    "    \"type\": \"record\",\n",
    "    \"name\": \"Stock\",\n",
    "    \"fields\": [\n",
    "        {\"name\": \"event_time\", \"type\": \"string\"},\n",
    "        {\"name\": \"symbol\",  \"type\": \"string\"},\n",
    "        {\"name\": \"price\", \"type\": \"float\"},\n",
    "        {\"name\": \"quantity\", \"type\": \"int\"}\n",
    "    ]\n",
    "}\"\"\"\n",
    "\n",
    "df3 = df.select(\"timestamp\", \"key\", from_avro(df.value, stock_schema, options = {\"mode\":\"PERMISSIVE\"}).alias(\"value\"))\n",
    "print('df3', df3)\n",
    "debug3 = write_memory(df3, 'debug3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr style=\"border-bottom: 1px solid\"><td style=\"font-weight: bold\">timestamp</td><td style=\"font-weight: bold\">key</td><td style=\"font-weight: bold\">value</td></tr><tr><td>2022-02-19 04:09:26.907000</td><td>bytearray(b&#x27;\\x8c&lt;E\\xa1\\x1f7O\\xfb\\x86\\xc2\\x97\\xadd\\x8d\\xe7B&#x27;)</td><td>Row(event_time=None, symbol=None, price=None, quantity=None)</td></tr><tr><td>2022-02-19 04:09:26.739000</td><td>bytearray(b&#x27;\\xe1c\\xbc\\xbb\\x8e\\x8fI\\x93\\xa6\\xfcC\\xcbM\\xae\\xb78&#x27;)</td><td>Row(event_time=None, symbol=None, price=None, quantity=None)</td></tr><tr><td>2022-02-19 04:09:26.658000</td><td>bytearray(b&#x27;\\xfd\\xe6\\x9d\\x90\\xc6\\x9aM{\\xac\\x11\\x93VE\\x00H\\x81&#x27;)</td><td>Row(event_time=None, symbol=None, price=None, quantity=None)</td></tr><tr><td>2022-02-19 04:09:22.905000</td><td>bytearray(b&#x27;D\\xb6)1=\\x8aC\\x88\\xa2p]\\x8as\\xe4/k&#x27;)</td><td>Row(event_time=None, symbol=None, price=None, quantity=None)</td></tr><tr><td>2022-02-19 04:09:22.734000</td><td>bytearray(b&#x27;\\xa1Z\\xa5w\\x105I\\xeb\\xb7\\x8c\\x94\\rE\\xd5\\x10}&#x27;)</td><td>Row(event_time=None, symbol=None, price=None, quantity=None)</td></tr><tr><td>2022-02-19 04:09:22.653000</td><td>bytearray(b&#x27;}bV\\xbe\\xbc\\xbbI|\\xa4\\xca\\x9f\\xc0\\ny~\\x8f&#x27;)</td><td>Row(event_time=None, symbol=None, price=None, quantity=None)</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%sparksql\n",
    "select * from debug3 order by timestamp desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug3.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's read some data from the JSON stream and fix it up to make it more usable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stock_schema {\n",
      "    \"namespace\": \"stock.avro\",\n",
      "    \"type\": \"record\",\n",
      "    \"name\": \"Stock\",\n",
      "    \"fields\": [\n",
      "        {\"name\": \"event_time\", \"type\": \"string\"},\n",
      "        {\"name\": \"symbol\",  \"type\": \"string\"},\n",
      "        {\"name\": \"price\", \"type\": \"float\"},\n",
      "        {\"name\": \"quantity\", \"type\": \"int\"}\n",
      "    ]\n",
      "}\n",
      "stock_struct StructType(List(StructField(event_time,StringType,true),StructField(symbol,StringType,true),StructField(price,FloatType,true),StructField(quantity,IntegerType,true)))\n",
      "df DataFrame[key: binary, value: binary, topic: string, partition: int, offset: bigint, timestamp: timestamp, timestampType: int]\n",
      "df1 DataFrame[key: string, timestamp: timestamp, value: string]\n",
      "df2 DataFrame[key: string, timestamp: timestamp, value2: struct<event_time:string,symbol:string,price:float,quantity:int>]\n",
      "df3 DataFrame[key: string, timestamp: timestamp, event_time: string, symbol: string, price: float, quantity: int]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "import uuid\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "brokers = 'localhost:9092'\n",
    "kafka_topic = 'stocks-json'\n",
    "receiver_sleep_time = 4\n",
    "\n",
    "stock_schema = open(\"stock.avsc\", \"r\").read()\n",
    "print('stock_schema', stock_schema)\n",
    "\n",
    "stock_struct = spark.read.format(\"avro\").option(\"avroSchema\", stock_schema).load().schema\n",
    "print('stock_struct', stock_struct)\n",
    "\n",
    "df = (spark.readStream \n",
    "    .format(\"kafka\") \n",
    "    .option(\"kafka.bootstrap.servers\", brokers) \n",
    "    .option(\"subscribe\", kafka_topic) \n",
    "    .option(\"startingOffsets\", \"earliest\")\n",
    "    .option(\"failOnDataLoss\", False)\n",
    "    .load()\n",
    "    )\n",
    "print('df', df)\n",
    "\n",
    "\n",
    "def convert_uuid(value):\n",
    "    # value is a bytearray in this case coming from spark\n",
    "    ret = uuid.UUID(bytes = bytes(value))\n",
    "    return str(ret)\n",
    "\n",
    "convert_uuid_udf = udf(convert_uuid, StringType())\n",
    "\n",
    "# keep the key and timestamp and convert the value from bytes to string\n",
    "#df1 = df.select(col(\"key\"), \"timestamp\", expr(\"CAST(value AS STRING) as value\"))\n",
    "df1 = df.select(convert_uuid_udf(col(\"key\")).alias(\"key\"), \"timestamp\", expr(\"CAST(value AS STRING) as value\"))\n",
    "print('df1', df1)\n",
    "\n",
    "# cast the string json to a struct\n",
    "# keep all the columns we selected and convery the JSON string into a struct object and remove the string version\n",
    "df2 = df1.select(*df1.columns, from_json(df1.value, stock_struct).alias(\"value2\")).drop('value')\n",
    "print('df2', df2)\n",
    "\n",
    "# flatten the struct to a normal DataFrame\n",
    "df3 = df2.select(*(df2.columns), col(\"value2.*\")).drop('value2')\n",
    "print('df3', df3)\n",
    "\n",
    "if 'debug4' in locals():\n",
    "    debug4.stop()\n",
    "    \n",
    "debug4 = write_memory(df3, 'debug4')\n",
    "\n",
    "\n",
    "# # gather the columns you want into a struct\n",
    "# df4 = df3.select(\"key\", struct('event_time','symbol','price','quantity').alias('value'))\n",
    "\n",
    "# # or SQL\n",
    "# '''\n",
    "# df3.createOrReplaceTempView('data')\n",
    "# df4 = spark.sql(\"\"\"\n",
    "# SELECT key, NAMED_STRUCT('event_time', event_time, 'symbol', symbol, 'price', price, 'quantity', quantity) AS value\n",
    "# FROM data\n",
    "# \"\"\")\n",
    "# '''\n",
    "# print('df4', df4)\n",
    "\n",
    "# # df5 = df4.select(\"key\", to_json(\"value\").alias(\"value\"))\n",
    "# # print('df5', df5)\n",
    "\n",
    "# df6 = df4.select(\"key\", to_avro(\"value\", stock_schema).alias(\"value\"))\n",
    "# print('df6', df6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr style=\"border-bottom: 1px solid\"><td style=\"font-weight: bold\">key</td><td style=\"font-weight: bold\">timestamp</td><td style=\"font-weight: bold\">event_time</td><td style=\"font-weight: bold\">symbol</td><td style=\"font-weight: bold\">price</td><td style=\"font-weight: bold\">quantity</td></tr><tr><td>e7a1402b-f0b1-42d2-b6d0-a36d7f74d978</td><td>2022-02-18 01:18:17.985000</td><td>2022-02-18 01:18:17</td><td>AAPL</td><td>268.3599853515625</td><td>621</td></tr><tr><td>d3f0bcbe-d405-4665-8470-5be4daa3d461</td><td>2022-02-18 01:18:17.985000</td><td>2022-02-18 01:18:17</td><td>GOOG</td><td>287.67999267578125</td><td>462</td></tr><tr><td>14e1d92d-adcd-4f8a-b888-2ad4e11d21fd</td><td>2022-02-18 01:18:17.985000</td><td>2022-02-18 01:18:17</td><td>MSFT</td><td>253.72000122070312</td><td>123</td></tr><tr><td>6ae5cf0b-5906-44ea-ba3b-d8cd2eb2a339</td><td>2022-02-18 01:18:21.989000</td><td>2022-02-18 01:18:21</td><td>MSFT</td><td>164.02000427246094</td><td>296</td></tr><tr><td>49b91518-5c1d-4498-8437-af2afec72947</td><td>2022-02-18 01:18:21.990000</td><td>2022-02-18 01:18:21</td><td>AAPL</td><td>233.42999267578125</td><td>732</td></tr><tr><td>f961118e-730c-48cb-8f34-3900b937c1dd</td><td>2022-02-18 01:18:21.990000</td><td>2022-02-18 01:18:21</td><td>GOOG</td><td>175.02999877929688</td><td>475</td></tr><tr><td>7b6b19c4-3978-4339-8454-bee9c111ddcd</td><td>2022-02-18 01:18:25.991000</td><td>2022-02-18 01:18:25</td><td>GOOG</td><td>137.64999389648438</td><td>650</td></tr><tr><td>115419c7-491e-4113-9827-e1673168763c</td><td>2022-02-18 01:18:25.993000</td><td>2022-02-18 01:18:25</td><td>AAPL</td><td>256.2900085449219</td><td>641</td></tr><tr><td>36da8d2a-cdf1-463e-9b81-80fece3cb612</td><td>2022-02-18 01:18:25.994000</td><td>2022-02-18 01:18:25</td><td>MSFT</td><td>177.6300048828125</td><td>390</td></tr><tr><td>8ffb4a55-57b1-4179-a65a-7b285454d6e4</td><td>2022-02-18 01:18:29.996000</td><td>2022-02-18 01:18:29</td><td>GOOG</td><td>133.60000610351562</td><td>888</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sparksql\n",
    "select * from debug4 limit 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now that we have a normal DataFrame, let's manipulate it how we want and write the results out to another stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bytearray(b'\\xe7\\xa1@+\\xf0\\xb1B\\xd2\\xb6\\xd0\\xa3m\\x7ft\\xd9x') b'\\xe7\\xa1@+\\xf0\\xb1B\\xd2\\xb6\\xd0\\xa3m\\x7ft\\xd9x'\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e7a1402b-f0b1-42d2-b6d0-a36d7f74d978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
