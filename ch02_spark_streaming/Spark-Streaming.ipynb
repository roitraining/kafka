{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Initialize pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, json, io\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.utils import StreamingQueryException\n",
    "import sys\n",
    "import json\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = '/usr/bin/python3'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = '/usr/bin/python3'\n",
    "sys.path.append('/class')\n",
    "\n",
    "# Kafka variables\n",
    "brokers = 'localhost:9092'\n",
    "kafka_topic = 'stocks-json'\n",
    "receiver_sleep_time = 4\n",
    "\n",
    "# Connect to Spark \n",
    "if not 'sc' in locals():\n",
    "    from initspark import initspark\n",
    "    sc, spark, config = initspark()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Create a helper function to stream to a memory table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_memory(df, queryname = 'debug', mode = \"append\"):\n",
    "    # modes are: complete, update, append\n",
    "\n",
    "    # if queryname in spark.catalog.listTables():\n",
    "    #     spark.catalog.dropTempView(queryname)\n",
    "    \n",
    "    query = (df.writeStream \n",
    "            .format(\"memory\")\n",
    "            .queryName(queryname)\n",
    "            .outputMode(mode)\n",
    "            .start()\n",
    "            )\n",
    "    return query\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Define a streaming source and create a temp view to receive the results for debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (spark.readStream \n",
    "    .format(\"kafka\") \n",
    "    .option(\"kafka.bootstrap.servers\", brokers) \n",
    "    .option(\"subscribe\", kafka_topic) \n",
    "    .option(\"startingOffsets\", \"earliest\")\n",
    "    .option(\"failOnDataLoss\", False)\n",
    "    .load()\n",
    "    )\n",
    "\n",
    "# df.createOrReplaceTempView('table')\n",
    "# df1 = spark.sql(\"\"\"SELECT 'new data' as newfield, * from table\"\"\")\n",
    "\n",
    "df1 = df.selectExpr(\"UPPER(CAST(value AS STRING)) as value\")\n",
    "\n",
    "debug1 = write_memory(df1, 'debug1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Query from the memory stream like it's a tempory view using `spark.sql`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from debug1\").take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## You can stop and restart a memory stream whenever you like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug1.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug1 = write_memory(df1, 'debug1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from debug1\").take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Spark SQL magic is also quite helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext sparksql_magic\n",
    "# pip install sparksql-magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sparksql\n",
    "select * from debug1 order by value limit 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Stop a memory stream when you don't need it, as it can consume a lot of memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug1.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's try reading AVRO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_schema = open(\"stock.avsc\", \"r\").read()\n",
    "print('stock_schema', stock_schema)\n",
    "stock_struct = spark.read.format(\"avro\").option(\"avroSchema\", stock_schema).load().schema\n",
    "print('stock_struct', stock_struct)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brokers = 'localhost:9092'\n",
    "kafka_topic = 'stocks-avro'\n",
    "receiver_sleep_time = 4\n",
    "\n",
    "df = (spark.readStream \n",
    "    .format(\"kafka\") \n",
    "    .option(\"kafka.bootstrap.servers\", brokers) \n",
    "    .option(\"subscribe\", kafka_topic) \n",
    "    .option(\"startingOffsets\", \"latest\")\n",
    "    .option(\"failOnDataLoss\", False)\n",
    "    .load()\n",
    "    )\n",
    "print('df', df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug2 = write_memory(df, 'debug2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sparksql\n",
    "select timestamp, key, value from debug2 order by timestamp desc limit 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ConsumerRecord(topic='stocks-avro', partition=0, offset=40679, timestamp=1645243362535, timestamp_type=0, key=b'\\xd6\\x0cgMs<By\\xb8\\xcaR\\x02\\xe0\\xfa\\x93\\x14', value=b'Obj\\x01\\x04\\x14avro.codec\\x08null\\x16avro.schema\\xc6\\x03{\"type\": \"record\", \"name\": \"Stock\", \"namespace\": \"stock.avro\", \"fields\": [{\"type\": \"string\", \"name\": \"event_time\"}, {\"type\": \"string\", \"name\": \"symbol\"}, {\"type\": \"float\", \"name\": \"price\"}, {\"type\": \"int\", \"name\": \"quantity\"}]}\\x00$\\x7f#w\\xeaD\\xdchK\\xc5!\\xf85\\x10\\xc8J\\x02>&2022-02-19 04:02:42\\x08MSFT\\xa4p\\xdfB\\xf0\\t$\\x7f#w\\xeaD\\xdchK\\xc5!\\xf85\\x10\\xc8J', headers=[], checksum=None, serialized_key_size=16, serialized_value_size=328, serialized_header_size=-1)\n",
    "ConsumerRecord(topic='stocks-avro', partition=0, offset=40679, timestamp=1645243362535, timestamp_type=0, key=b'\\xd6\\x0cgMs<By\\xb8\\xcaR\\x02\\xe0\\xfa\\x93\\x14', value=b'Obj\\x01\\x04\\x14avro.codec\\x08null\\x16avro.schema\\xc6\\x03{\"type\": \"record\", \"name\": \"Stock\", \"namespace\": \"stock.avro\", \"fields\": [{\"type\": \"string\", \"name\": \"event_time\"}, {\"type\": \"string\", \"name\": \"symbol\"}, {\"type\": \"float\", \"name\": \"price\"}, {\"type\": \"int\", \"name\": \"quantity\"}]}\\x00$\\x7f#w\\xeaD\\xdchK\\xc5!\\xf85\\x10\\xc8J\\x02>&2022-02-19 04:02:42\\x08MSFT\\xa4p\\xdfB\\xf0\\t$\\x7f#w\\xeaD\\xdchK\\xc5!\\xf85\\x10\\xc8J', headers=[], checksum=None, serialized_key_size=16, serialized_value_size=328, serialized_header_size=-1)\n",
    "\n",
    "ConsumerRecord(topic='stocks-avro', partition=0, offset=40814, timestamp=1645243470645, timestamp_type=0, key=b'\\xc8\\xeb\\xc2\\xe9O\\xaaJ\\x86\\x83\\x85\\xb9\\xd7\\xf46\\xea\\x8f', value=b'Obj\\x01\\x04\\x14avro.codec\\x08null\\x16avro.schema\\xc6\\x03{\"type\": \"record\", \"name\": \"Stock\", \"namespace\": \"stock.avro\", \"fields\": [{\"type\": \"string\", \"name\": \"event_time\"}, {\"type\": \"string\", \"name\": \"symbol\"}, {\"type\": \"float\", \"name\": \"price\"}, {\"type\": \"int\", \"name\": \"quantity\"}]}\\x00\\xe8\\xf5x\\r\\xbf\\x8aC\\x98&\\xaf\\x13iz\\x9dp\\x13\\x02>&2022-02-19 04:04:30\\x08MSFT\\xc3\\xf5\\x11C\\xac\\x04\\xe8\\xf5x\\r\\xbf\\x8aC\\x98&\\xaf\\x13iz\\x9dp\\x13', headers=[], checksum=None, serialized_key_size=16, serialized_value_size=328, serialized_header_size=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug2.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'debug3' in locals():\n",
    "    debug3.stop()\n",
    "    \n",
    "from pyspark.sql.avro.functions import from_avro, to_avro\n",
    "stock_schema = \"\"\"{\n",
    "    \"type\": \"record\",\n",
    "    \"name\": \"Stock\",\n",
    "    \"fields\": [\n",
    "        {\"name\": \"event_time\", \"type\": \"string\"},\n",
    "        {\"name\": \"symbol\",  \"type\": \"string\"},\n",
    "        {\"name\": \"price\", \"type\": \"float\"},\n",
    "        {\"name\": \"quantity\", \"type\": \"int\"}\n",
    "    ]\n",
    "}\"\"\"\n",
    "\n",
    "df3 = df.select(\"timestamp\", \"key\", from_avro(df.value, stock_schema, options = {\"mode\":\"PERMISSIVE\"}).alias(\"value\"))\n",
    "print('df3', df3)\n",
    "debug3 = write_memory(df3, 'debug3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sparksql\n",
    "select * from debug3 order by timestamp desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug3.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's read some data from the JSON stream and fix it up to make it more usable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "import uuid\n",
    "\n",
    "brokers = 'localhost:9092'\n",
    "kafka_topic = 'stocks-json'\n",
    "receiver_sleep_time = 4\n",
    "\n",
    "stock_schema = open(\"stock.avsc\", \"r\").read()\n",
    "print('stock_schema', stock_schema)\n",
    "\n",
    "stock_struct = spark.read.format(\"avro\").option(\"avroSchema\", stock_schema).load().schema\n",
    "print('stock_struct', stock_struct)\n",
    "\n",
    "df = (spark.readStream \n",
    "    .format(\"kafka\") \n",
    "    .option(\"kafka.bootstrap.servers\", brokers) \n",
    "    .option(\"subscribe\", kafka_topic) \n",
    "    .option(\"startingOffsets\", \"earliest\")\n",
    "    .option(\"failOnDataLoss\", False)\n",
    "#    .option(\"kafka.group.id\", \"stock-json-spark-group\")\n",
    "    .load()\n",
    "    )\n",
    "print('df', df)\n",
    "\n",
    "\n",
    "def convert_uuid(value):\n",
    "    # value is a bytearray in this case coming from spark\n",
    "    ret = uuid.UUID(bytes = bytes(value))\n",
    "    return str(ret)\n",
    "\n",
    "convert_uuid_udf = udf(convert_uuid, StringType())\n",
    "\n",
    "# keep the key and timestamp and convert the value from bytes to string\n",
    "#df1 = df.select(col(\"key\"), \"timestamp\", expr(\"CAST(value AS STRING) as value\"))\n",
    "df1 = df.select(convert_uuid_udf(col(\"key\")).alias(\"key\"), \"timestamp\", expr(\"CAST(value AS STRING) as value\"))\n",
    "print('df1', df1)\n",
    "\n",
    "# cast the string json to a struct\n",
    "# keep all the columns we selected and convery the JSON string into a struct object and remove the string version\n",
    "df2 = df1.select(*df1.columns, from_json(df1.value, stock_struct).alias(\"value2\")).drop('value')\n",
    "print('df2', df2)\n",
    "\n",
    "# flatten the struct to a normal DataFrame\n",
    "df4 = df2.select(*(df2.columns), col(\"value2.*\")).drop('value2')\n",
    "print('df4', df4)\n",
    "\n",
    "if 'debug4' in locals():\n",
    "    debug4.stop()\n",
    "    \n",
    "debug4 = write_memory(df4, 'debug4')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sparksql \n",
    "select * from debug4 order by event_time desc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug4.stop()\n",
    "debug4 = write_memory(df4, 'debug4')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now that we have a normal DataFrame, let's manipulate it how we want and write the results out to another stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_window = (df4.select(\"timestamp\", \"symbol\",\"quantity\")\n",
    "        .withWatermark(\"timestamp\", \"10 seconds\") \n",
    "        .groupBy(window(\"timestamp\", \"10 seconds\").alias(\"window\"), \"symbol\") \n",
    "        .agg(sum(\"quantity\").alias(\"sum\"))\n",
    "        )\n",
    "print(fixed_window)\n",
    "\n",
    "debug5 = write_memory(fixed_window, 'debug5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can see that we get aggregate by symbol every ten seconds. This data can be written off somewhere like a SQL or NoSQL database or forwarded as a new message to create a streaming aggregation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sparksql\n",
    "select * from debug5 order by window desc, symbol limit 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug5.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sliding windows are similar except you give it two parameters, the first is the total length of the window and the second is the refresh interval. In this case the windows will overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sliding_window = (df4.select(\"timestamp\", \"symbol\",\"quantity\")\n",
    "        .withWatermark(\"timestamp\", \"10 seconds\") \n",
    "        .groupBy(window(\"timestamp\", \"30 seconds\", \"10 seconds\").alias(\"window\"), \"symbol\") \n",
    "        .agg(sum(\"quantity\").alias(\"sum\"))\n",
    "        )\n",
    "print(sliding_window)\n",
    "\n",
    "debug6 = write_memory(sliding_window, 'debug6')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sparksql\n",
    "select * from debug6 order by window desc, symbol limit 21\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug6.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session Window is similar but used to group data that represents a continuous stream of activity. The time specifies a timeout period or period of inactivity that indicates when a session should end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_window = (df4.select(\"timestamp\", \"symbol\",\"quantity\")\n",
    "        .withWatermark(\"timestamp\", \"10 seconds\") \n",
    "        .groupBy(session_window(\"timestamp\", \"5 minutes\").alias(\"window\"), \"symbol\") \n",
    "        .agg(sum(\"quantity\").alias(\"sum\"))\n",
    "        )\n",
    "print(session_window)\n",
    "\n",
    "debug7 = write_memory(session_window, 'debug7')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's join the streaming aggregation with a static reference table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = sc.parallelize([('AAPL', 'Apple'), ('MSFT', 'Microsoft'), ('GOOG','Google')])\n",
    "stocks = spark.createDataFrame(x, 'symbol:string, name:string')\n",
    "stocks.createOrReplaceTempView('stocks')\n",
    "fixed_window.createOrReplaceTempView('trades')\n",
    "\n",
    "joined_aggregate = spark.sql(\"\"\"\n",
    "SELECT t.*, s.name\n",
    "FROM trades as t\n",
    "JOIN stocks as s on t.symbol = s.symbol\n",
    "\"\"\")\n",
    "\n",
    "debug8 = write_memory(joined_aggregate, 'debug8')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sparksql\n",
    "select * from debug8 order by window desc, symbol limit 9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug8.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
