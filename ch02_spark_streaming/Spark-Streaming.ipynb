{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing pyspark\n",
      ":: loading settings :: url = jar:file:/usr/local/spark-3.2.1-bin-hadoop3.2/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.apache.spark#spark-streaming-kafka-0-8_2.11 added as a dependency\n",
      "org.mongodb.spark#mongo-spark-connector_2.11 added as a dependency\n",
      "com.datastax.spark#spark-cassandra-connector_2.11 added as a dependency\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      "org.apache.spark#spark-avro_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-d4f74595-3f20-4de5-ab63-c18af4213cbd;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-streaming-kafka-0-8_2.11;2.0.2 in central\n",
      "\tfound org.apache.kafka#kafka_2.11;0.8.2.1 in central\n",
      "\tfound org.scala-lang.modules#scala-xml_2.11;1.0.2 in central\n",
      "\tfound com.yammer.metrics#metrics-core;2.2.0 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.16 in spark-list\n",
      "\tfound org.scala-lang.modules#scala-parser-combinators_2.11;1.0.2 in central\n",
      "\tfound com.101tec#zkclient;0.3 in central\n",
      "\tfound log4j#log4j;1.2.17 in spark-list\n",
      "\tfound org.apache.kafka#kafka-clients;0.8.2.1 in central\n",
      "\tfound net.jpountz.lz4#lz4;1.3.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.2.6 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in spark-list\n",
      "\tfound org.mongodb.spark#mongo-spark-connector_2.11;2.4.3 in central\n",
      "\tfound org.mongodb#mongo-java-driver;3.12.5 in central\n",
      "\tfound com.datastax.spark#spark-cassandra-connector_2.11;2.5.2 in central\n",
      "\tfound com.datastax.spark#spark-cassandra-connector-driver_2.11;2.5.2 in central\n",
      "\tfound com.datastax.oss#java-driver-core-shaded;4.10.0 in central\n",
      "\tfound com.datastax.oss#native-protocol;1.4.12 in central\n",
      "\tfound com.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 in central\n",
      "\tfound com.typesafe#config;1.4.1 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.26 in central\n",
      "\tfound io.dropwizard.metrics#metrics-core;4.1.16 in central\n",
      "\tfound org.hdrhistogram#HdrHistogram;2.1.12 in central\n",
      "\tfound org.reactivestreams#reactive-streams;1.0.3 in central\n",
      "\tfound com.github.stephenc.jcip#jcip-annotations;1.0-1 in central\n",
      "\tfound com.github.spotbugs#spotbugs-annotations;3.1.12 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.2 in spark-list\n",
      "\tfound com.datastax.oss#java-driver-mapper-runtime;4.10.0 in central\n",
      "\tfound com.datastax.oss#java-driver-query-builder;4.10.0 in central\n",
      "\tfound org.apache.commons#commons-lang3;3.5 in spark-list\n",
      "\tfound com.thoughtworks.paranamer#paranamer;2.8 in spark-list\n",
      "\tfound org.scala-lang#scala-reflect;2.11.12 in spark-list\n",
      "\t[2.11.12] org.scala-lang#scala-reflect;2.11.12\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.2.1 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.2.1 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.8.0 in central\n",
      "\tfound org.lz4#lz4-java;1.7.1 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.4 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.1 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.1 in central\n",
      "\tfound org.apache.htrace#htrace-core4;4.1.0-incubating in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.6.2 in central\n",
      "\tfound org.apache.spark#spark-avro_2.12;3.2.1 in central\n",
      "\tfound org.tukaani#xz;1.8 in central\n",
      ":: resolution report :: resolve 3664ms :: artifacts dl 16ms\n",
      "\t:: modules in use:\n",
      "\tcom.101tec#zkclient;0.3 from central in [default]\n",
      "\tcom.datastax.oss#java-driver-core-shaded;4.10.0 from central in [default]\n",
      "\tcom.datastax.oss#java-driver-mapper-runtime;4.10.0 from central in [default]\n",
      "\tcom.datastax.oss#java-driver-query-builder;4.10.0 from central in [default]\n",
      "\tcom.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 from central in [default]\n",
      "\tcom.datastax.oss#native-protocol;1.4.12 from central in [default]\n",
      "\tcom.datastax.spark#spark-cassandra-connector-driver_2.11;2.5.2 from central in [default]\n",
      "\tcom.datastax.spark#spark-cassandra-connector_2.11;2.5.2 from central in [default]\n",
      "\tcom.github.spotbugs#spotbugs-annotations;3.1.12 from central in [default]\n",
      "\tcom.github.stephenc.jcip#jcip-annotations;1.0-1 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.2 from spark-list in [default]\n",
      "\tcom.thoughtworks.paranamer#paranamer;2.8 from spark-list in [default]\n",
      "\tcom.typesafe#config;1.4.1 from central in [default]\n",
      "\tcom.yammer.metrics#metrics-core;2.2.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\tio.dropwizard.metrics#metrics-core;4.1.16 from central in [default]\n",
      "\tlog4j#log4j;1.2.17 from spark-list in [default]\n",
      "\torg.apache.commons#commons-lang3;3.5 from spark-list in [default]\n",
      "\torg.apache.commons#commons-pool2;2.6.2 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.1 from central in [default]\n",
      "\torg.apache.htrace#htrace-core4;4.1.0-incubating from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.8.0 from central in [default]\n",
      "\torg.apache.kafka#kafka_2.11;0.8.2.1 from central in [default]\n",
      "\torg.apache.spark#spark-avro_2.12;3.2.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.2.1 from central in [default]\n",
      "\torg.apache.spark#spark-streaming-kafka-0-8_2.11;2.0.2 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.2.1 from central in [default]\n",
      "\torg.hdrhistogram#HdrHistogram;2.1.12 from central in [default]\n",
      "\torg.lz4#lz4-java;1.7.1 from central in [default]\n",
      "\torg.mongodb#mongo-java-driver;3.12.5 from central in [default]\n",
      "\torg.mongodb.spark#mongo-spark-connector_2.11;2.4.3 from central in [default]\n",
      "\torg.reactivestreams#reactive-streams;1.0.3 from central in [default]\n",
      "\torg.scala-lang#scala-reflect;2.11.12 from spark-list in [default]\n",
      "\torg.scala-lang.modules#scala-parser-combinators_2.11;1.0.2 from central in [default]\n",
      "\torg.scala-lang.modules#scala-xml_2.11;1.0.2 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from spark-list in [default]\n",
      "\torg.tukaani#xz;1.8 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.4 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\torg.slf4j#slf4j-api;1.7.16 by [org.slf4j#slf4j-api;1.7.26] in [default]\n",
      "\torg.apache.kafka#kafka-clients;0.8.2.1 by [org.apache.kafka#kafka-clients;2.8.0] in [default]\n",
      "\tnet.jpountz.lz4#lz4;1.3.0 transitively in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.2.6 by [org.xerial.snappy#snappy-java;1.1.8.4] in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.26 by [org.slf4j#slf4j-api;1.7.30] in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 by [com.google.code.findbugs#jsr305;3.0.2] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   46  |   6   |   6   |   6   ||   40  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      "\n",
      ":: problems summary ::\n",
      ":::: ERRORS\n",
      "\tunknown resolver null\n",
      "\n",
      "\tunknown resolver null\n",
      "\n",
      "\tunknown resolver null\n",
      "\n",
      "\tunknown resolver null\n",
      "\n",
      "\tunknown resolver null\n",
      "\n",
      "\tunknown resolver null\n",
      "\n",
      "\n",
      ":: USE VERBOSE OR DEBUG MESSAGE LEVEL FOR MORE DETAILS\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-d4f74595-3f20-4de5-ab63-c18af4213cbd\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 40 already retrieved (0kB/12ms)\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pyspark initialized\n"
     ]
    }
   ],
   "source": [
    "import os, sys, json, io\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.utils import StreamingQueryException\n",
    "import sys\n",
    "import json\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = '/usr/bin/python3'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = '/usr/bin/python3'\n",
    "sys.path.append('/class')\n",
    "\n",
    "# Kafka variables\n",
    "brokers = 'localhost:9092'\n",
    "kafka_topic = 'stocks-json'\n",
    "receiver_sleep_time = 4\n",
    "\n",
    "# Connect to Spark \n",
    "if not 'sc' in locals():\n",
    "    from initspark import initspark\n",
    "    sc, spark, config = initspark()\n",
    "\n",
    "def write_memory(df, queryname = 'debug'):\n",
    "    query = (df.writeStream \n",
    "            .format(\"memory\")\n",
    "            .queryName(queryname)\n",
    "#            .outputMode(\"complete\")\n",
    "            .start()\n",
    "            )\n",
    "    return query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7f5961fb86d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = (spark.readStream \n",
    "    .format(\"kafka\") \n",
    "    .option(\"kafka.bootstrap.servers\", brokers) \n",
    "    .option(\"subscribe\", kafka_topic) \n",
    "    .option(\"startingOffsets\", \"earliest\")\n",
    "    .option(\"failOnDataLoss\", False)\n",
    "    .load()\n",
    "    )\n",
    "\n",
    "# df.createOrReplaceTempView('table')\n",
    "# df1 = spark.sql(\"\"\"SELECT 'new data' as newfield, * from table\"\"\")\n",
    "df1 = df.selectExpr(\"UPPER(CAST(value AS STRING)) as value\")\n",
    "\n",
    "write_memory(df1, 'debug1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(value='{\"EVENT_TIME\": \"2022-02-18 01:18:17\", \"SYMBOL\": \"AAPL\", \"PRICE\": 268.36, \"QUANTITY\": 621}'),\n",
       " Row(value='{\"EVENT_TIME\": \"2022-02-18 01:18:17\", \"SYMBOL\": \"GOOG\", \"PRICE\": 287.68, \"QUANTITY\": 462}'),\n",
       " Row(value='{\"EVENT_TIME\": \"2022-02-18 01:18:17\", \"SYMBOL\": \"MSFT\", \"PRICE\": 253.72, \"QUANTITY\": 123}'),\n",
       " Row(value='{\"EVENT_TIME\": \"2022-02-18 01:18:21\", \"SYMBOL\": \"MSFT\", \"PRICE\": 164.02, \"QUANTITY\": 296}'),\n",
       " Row(value='{\"EVENT_TIME\": \"2022-02-18 01:18:21\", \"SYMBOL\": \"AAPL\", \"PRICE\": 233.43, \"QUANTITY\": 732}'),\n",
       " Row(value='{\"EVENT_TIME\": \"2022-02-18 01:18:21\", \"SYMBOL\": \"GOOG\", \"PRICE\": 175.03, \"QUANTITY\": 475}'),\n",
       " Row(value='{\"EVENT_TIME\": \"2022-02-18 01:18:25\", \"SYMBOL\": \"GOOG\", \"PRICE\": 137.65, \"QUANTITY\": 650}'),\n",
       " Row(value='{\"EVENT_TIME\": \"2022-02-18 01:18:25\", \"SYMBOL\": \"AAPL\", \"PRICE\": 256.29, \"QUANTITY\": 641}'),\n",
       " Row(value='{\"EVENT_TIME\": \"2022-02-18 01:18:25\", \"SYMBOL\": \"MSFT\", \"PRICE\": 177.63, \"QUANTITY\": 390}'),\n",
       " Row(value='{\"EVENT_TIME\": \"2022-02-18 01:18:29\", \"SYMBOL\": \"GOOG\", \"PRICE\": 133.6, \"QUANTITY\": 888}')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"select * from debug1\").take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, json, io\n",
    "os.environ['PYSPARK_PYTHON'] = '/usr/bin/python3'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = '/usr/bin/python3'\n",
    "sys.path.append('/class')\n",
    "\n",
    "#from pyspark.streaming import StreamingContext\n",
    "#from pyspark.streaming.kafka import KafkaUtils\n",
    "import fastavro\n",
    "import avro.io\n",
    "import avro.schema\n",
    "import avro.datafile\n",
    "#import spark.sql.avro\n",
    "\n",
    "from pyspark.sql.avro.functions import from_avro, to_avro\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Kafka variables\n",
    "brokers = 'localhost:9092'\n",
    "kafka_topic = 'avro-stocks'\n",
    "receiver_sleep_time = 4\n",
    "stock_schema = open(\"stock.avsc\", \"r\").read()\n",
    "\n",
    "from initspark import initspark\n",
    "sc, spark, config = initspark()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df: DataFrame = (spark.readStream \n",
    "    .format(\"kafka\") \n",
    "    .option(\"kafka.bootstrap.servers\", brokers) \n",
    "    .option(\"subscribe\", kafka_topic) \n",
    "    .option(\"startingOffsets\", \"earliest\")\n",
    "    .load()\n",
    "    )\n",
    "\n",
    "# extract the binary value of the message and convert it to the schema read from the avsc file\n",
    "df1 = df.withColumn('value', from_avro(\"value\", stock_schema))\n",
    "# flatten out the value struct and remove it\n",
    "df2 = df1.select(*df.columns, col(\"value.*\")).drop(\"value\")\n",
    "\n",
    "# pick the columns we want to write to sql\n",
    "df3 = df2.selectExpr(\"key as kafka_key\", \"timestamp as kafka_timestamp\", \"event_time\", \"symbol\", \"price\", \"quantity\")\n",
    "\n",
    "\n",
    "\n",
    "#df4 = df3.select(\"symbol\",\"quantity\").groupBy(window(\"symbol\", \"10 seconds\")).sum(\"quantity\")\n",
    "\n",
    "df4 = (df3.select(\"kafka_timestamp\", \"symbol\",\"quantity\")\n",
    "        .withWatermark(\"kafka_timestamp\", \"10 seconds\") \n",
    "        .groupBy(window(\"kafka_timestamp\", \"10 seconds\"), \"symbol\")\n",
    "        .agg(sum(\"quantity\").alias(\"sum\"))\n",
    "        )\n",
    "print(df4)\n",
    "\n",
    "\n",
    "df4 = (df3.select(\"kafka_timestamp\", \"symbol\",\"quantity\")\n",
    "        .withWatermark(\"kafka_timestamp\", \"10 seconds\") \n",
    "        .groupBy(window(\"kafka_timestamp\", \"10 seconds\")) #.alias(\"group\"))\n",
    "        .agg(sum(\"quantity\").alias(\"sum\"))\n",
    "        )\n",
    "print(df4)\n",
    "\n",
    "def write_memory(df, queryname = 'debug'):\n",
    "    query = (df.writeStream \n",
    "            .format(\"memory\")\n",
    "            .queryName(queryname)\n",
    "            .outputMode(\"complete\")\n",
    "            .start()\n",
    "            )\n",
    "    return query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_memory(df4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = (df3.select(\"kafka_timestamp\", \"symbol\",\"quantity\")\n",
    "        .withWatermark(\"kafka_timestamp\", \"10 seconds\") \n",
    "        .groupBy(\"symbol\")\n",
    "        .agg(sum(\"quantity\").alias(\"sum\"))\n",
    "        )\n",
    "print(df4)\n",
    "\n",
    "def write_memory(df, queryname = 'debug'):\n",
    "    query = (df.writeStream \n",
    "            .format(\"memory\")\n",
    "            .queryName(queryname)\n",
    "            .outputMode(\"complete\")\n",
    "            .start()\n",
    "            )\n",
    "    return query\n",
    "\n",
    "write_memory(df4, 'debug4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = spark.sql(\"select * from debug4\")\n",
    "x.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.dropTempView(\"debug\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.listTables()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
