{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Initialize pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing pyspark\n",
      ":: loading settings :: url = jar:file:/usr/local/spark-3.2.1-bin-hadoop3.2/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.apache.spark#spark-streaming-kafka-0-8_2.11 added as a dependency\n",
      "org.mongodb.spark#mongo-spark-connector_2.11 added as a dependency\n",
      "com.datastax.spark#spark-cassandra-connector_2.11 added as a dependency\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      "org.apache.spark#spark-avro_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-fe9c56c6-5c49-468e-9db3-ecb2941e3f67;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-streaming-kafka-0-8_2.11;2.0.2 in central\n",
      "\tfound org.apache.kafka#kafka_2.11;0.8.2.1 in central\n",
      "\tfound org.scala-lang.modules#scala-xml_2.11;1.0.2 in central\n",
      "\tfound com.yammer.metrics#metrics-core;2.2.0 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.16 in spark-list\n",
      "\tfound org.scala-lang.modules#scala-parser-combinators_2.11;1.0.2 in central\n",
      "\tfound com.101tec#zkclient;0.3 in central\n",
      "\tfound log4j#log4j;1.2.17 in spark-list\n",
      "\tfound org.apache.kafka#kafka-clients;0.8.2.1 in central\n",
      "\tfound net.jpountz.lz4#lz4;1.3.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.2.6 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in spark-list\n",
      "\tfound org.mongodb.spark#mongo-spark-connector_2.11;2.4.3 in central\n",
      "\tfound org.mongodb#mongo-java-driver;3.12.5 in central\n",
      "\tfound com.datastax.spark#spark-cassandra-connector_2.11;2.5.2 in central\n",
      "\tfound com.datastax.spark#spark-cassandra-connector-driver_2.11;2.5.2 in central\n",
      "\tfound com.datastax.oss#java-driver-core-shaded;4.10.0 in central\n",
      "\tfound com.datastax.oss#native-protocol;1.4.12 in central\n",
      "\tfound com.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 in central\n",
      "\tfound com.typesafe#config;1.4.1 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.26 in central\n",
      "\tfound io.dropwizard.metrics#metrics-core;4.1.16 in central\n",
      "\tfound org.hdrhistogram#HdrHistogram;2.1.12 in central\n",
      "\tfound org.reactivestreams#reactive-streams;1.0.3 in central\n",
      "\tfound com.github.stephenc.jcip#jcip-annotations;1.0-1 in central\n",
      "\tfound com.github.spotbugs#spotbugs-annotations;3.1.12 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.2 in spark-list\n",
      "\tfound com.datastax.oss#java-driver-mapper-runtime;4.10.0 in central\n",
      "\tfound com.datastax.oss#java-driver-query-builder;4.10.0 in central\n",
      "\tfound org.apache.commons#commons-lang3;3.5 in spark-list\n",
      "\tfound com.thoughtworks.paranamer#paranamer;2.8 in spark-list\n",
      "\tfound org.scala-lang#scala-reflect;2.11.12 in spark-list\n",
      "\t[2.11.12] org.scala-lang#scala-reflect;2.11.12\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.2.1 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.2.1 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.8.0 in central\n",
      "\tfound org.lz4#lz4-java;1.7.1 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.4 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.1 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.1 in central\n",
      "\tfound org.apache.htrace#htrace-core4;4.1.0-incubating in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.6.2 in central\n",
      "\tfound org.apache.spark#spark-avro_2.12;3.2.1 in central\n",
      "\tfound org.tukaani#xz;1.8 in central\n",
      ":: resolution report :: resolve 3528ms :: artifacts dl 21ms\n",
      "\t:: modules in use:\n",
      "\tcom.101tec#zkclient;0.3 from central in [default]\n",
      "\tcom.datastax.oss#java-driver-core-shaded;4.10.0 from central in [default]\n",
      "\tcom.datastax.oss#java-driver-mapper-runtime;4.10.0 from central in [default]\n",
      "\tcom.datastax.oss#java-driver-query-builder;4.10.0 from central in [default]\n",
      "\tcom.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 from central in [default]\n",
      "\tcom.datastax.oss#native-protocol;1.4.12 from central in [default]\n",
      "\tcom.datastax.spark#spark-cassandra-connector-driver_2.11;2.5.2 from central in [default]\n",
      "\tcom.datastax.spark#spark-cassandra-connector_2.11;2.5.2 from central in [default]\n",
      "\tcom.github.spotbugs#spotbugs-annotations;3.1.12 from central in [default]\n",
      "\tcom.github.stephenc.jcip#jcip-annotations;1.0-1 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.2 from spark-list in [default]\n",
      "\tcom.thoughtworks.paranamer#paranamer;2.8 from spark-list in [default]\n",
      "\tcom.typesafe#config;1.4.1 from central in [default]\n",
      "\tcom.yammer.metrics#metrics-core;2.2.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\tio.dropwizard.metrics#metrics-core;4.1.16 from central in [default]\n",
      "\tlog4j#log4j;1.2.17 from spark-list in [default]\n",
      "\torg.apache.commons#commons-lang3;3.5 from spark-list in [default]\n",
      "\torg.apache.commons#commons-pool2;2.6.2 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.1 from central in [default]\n",
      "\torg.apache.htrace#htrace-core4;4.1.0-incubating from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.8.0 from central in [default]\n",
      "\torg.apache.kafka#kafka_2.11;0.8.2.1 from central in [default]\n",
      "\torg.apache.spark#spark-avro_2.12;3.2.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.2.1 from central in [default]\n",
      "\torg.apache.spark#spark-streaming-kafka-0-8_2.11;2.0.2 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.2.1 from central in [default]\n",
      "\torg.hdrhistogram#HdrHistogram;2.1.12 from central in [default]\n",
      "\torg.lz4#lz4-java;1.7.1 from central in [default]\n",
      "\torg.mongodb#mongo-java-driver;3.12.5 from central in [default]\n",
      "\torg.mongodb.spark#mongo-spark-connector_2.11;2.4.3 from central in [default]\n",
      "\torg.reactivestreams#reactive-streams;1.0.3 from central in [default]\n",
      "\torg.scala-lang#scala-reflect;2.11.12 from spark-list in [default]\n",
      "\torg.scala-lang.modules#scala-parser-combinators_2.11;1.0.2 from central in [default]\n",
      "\torg.scala-lang.modules#scala-xml_2.11;1.0.2 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from spark-list in [default]\n",
      "\torg.tukaani#xz;1.8 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.4 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\torg.slf4j#slf4j-api;1.7.16 by [org.slf4j#slf4j-api;1.7.26] in [default]\n",
      "\torg.apache.kafka#kafka-clients;0.8.2.1 by [org.apache.kafka#kafka-clients;2.8.0] in [default]\n",
      "\tnet.jpountz.lz4#lz4;1.3.0 transitively in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.2.6 by [org.xerial.snappy#snappy-java;1.1.8.4] in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.26 by [org.slf4j#slf4j-api;1.7.30] in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 by [com.google.code.findbugs#jsr305;3.0.2] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   46  |   6   |   6   |   6   ||   40  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      "\n",
      ":: problems summary ::\n",
      ":::: ERRORS\n",
      "\tunknown resolver null\n",
      "\n",
      "\tunknown resolver null\n",
      "\n",
      "\tunknown resolver null\n",
      "\n",
      "\tunknown resolver null\n",
      "\n",
      "\tunknown resolver null\n",
      "\n",
      "\tunknown resolver null\n",
      "\n",
      "\n",
      ":: USE VERBOSE OR DEBUG MESSAGE LEVEL FOR MORE DETAILS\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-fe9c56c6-5c49-468e-9db3-ecb2941e3f67\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 40 already retrieved (0kB/11ms)\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pyspark initialized\n"
     ]
    }
   ],
   "source": [
    "import os, sys, json, io\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.utils import StreamingQueryException\n",
    "import sys\n",
    "import json\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = '/usr/bin/python3'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = '/usr/bin/python3'\n",
    "sys.path.append('/class')\n",
    "\n",
    "# Kafka variables\n",
    "brokers = 'localhost:9092'\n",
    "kafka_topic = 'stocks-json'\n",
    "receiver_sleep_time = 4\n",
    "\n",
    "# Connect to Spark \n",
    "if not 'sc' in locals():\n",
    "    from initspark import initspark\n",
    "    sc, spark, config = initspark()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Create a helper function to stream to a memory table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_memory(df, queryname = 'debug', mode = \"append\"):\n",
    "    # modes are: complete, update, append\n",
    "\n",
    "    # if queryname in spark.catalog.listTables():\n",
    "    #     spark.catalog.dropTempView(queryname)\n",
    "    \n",
    "    query = (df.writeStream \n",
    "            .format(\"memory\")\n",
    "            .queryName(queryname)\n",
    "            .outputMode(mode)\n",
    "            .start()\n",
    "            )\n",
    "    return query\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Define a streaming source and create a temp view to receive the results for debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (spark.readStream \n",
    "    .format(\"kafka\") \n",
    "    .option(\"kafka.bootstrap.servers\", brokers) \n",
    "    .option(\"subscribe\", kafka_topic) \n",
    "    .option(\"startingOffsets\", \"earliest\")\n",
    "    .option(\"failOnDataLoss\", False)\n",
    "    .load()\n",
    "    )\n",
    "\n",
    "# df.createOrReplaceTempView('table')\n",
    "# df1 = spark.sql(\"\"\"SELECT 'new data' as newfield, * from table\"\"\")\n",
    "\n",
    "df1 = df.selectExpr(\"UPPER(CAST(value AS STRING)) as value\")\n",
    "\n",
    "debug1 = write_memory(df1, 'debug1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Query from the memory stream like it's a tempory view using `spark.sql`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(value='{\"EVENT_TIME\": \"2022-02-18 01:18:17\", \"SYMBOL\": \"AAPL\", \"PRICE\": 268.36, \"QUANTITY\": 621}'),\n",
       " Row(value='{\"EVENT_TIME\": \"2022-02-18 01:18:17\", \"SYMBOL\": \"GOOG\", \"PRICE\": 287.68, \"QUANTITY\": 462}'),\n",
       " Row(value='{\"EVENT_TIME\": \"2022-02-18 01:18:17\", \"SYMBOL\": \"MSFT\", \"PRICE\": 253.72, \"QUANTITY\": 123}'),\n",
       " Row(value='{\"EVENT_TIME\": \"2022-02-18 01:18:21\", \"SYMBOL\": \"MSFT\", \"PRICE\": 164.02, \"QUANTITY\": 296}'),\n",
       " Row(value='{\"EVENT_TIME\": \"2022-02-18 01:18:21\", \"SYMBOL\": \"AAPL\", \"PRICE\": 233.43, \"QUANTITY\": 732}'),\n",
       " Row(value='{\"EVENT_TIME\": \"2022-02-18 01:18:21\", \"SYMBOL\": \"GOOG\", \"PRICE\": 175.03, \"QUANTITY\": 475}'),\n",
       " Row(value='{\"EVENT_TIME\": \"2022-02-18 01:18:25\", \"SYMBOL\": \"GOOG\", \"PRICE\": 137.65, \"QUANTITY\": 650}'),\n",
       " Row(value='{\"EVENT_TIME\": \"2022-02-18 01:18:25\", \"SYMBOL\": \"AAPL\", \"PRICE\": 256.29, \"QUANTITY\": 641}'),\n",
       " Row(value='{\"EVENT_TIME\": \"2022-02-18 01:18:25\", \"SYMBOL\": \"MSFT\", \"PRICE\": 177.63, \"QUANTITY\": 390}'),\n",
       " Row(value='{\"EVENT_TIME\": \"2022-02-18 01:18:29\", \"SYMBOL\": \"GOOG\", \"PRICE\": 133.6, \"QUANTITY\": 888}')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"select * from debug1\").take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## You can stop and restart a memory stream whenever you like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug1.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug1 = write_memory(df1, 'debug1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from debug1\").take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Spark SQL magic is also quite helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext sparksql_magic\n",
    "# pip install sparksql-magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr style=\"border-bottom: 1px solid\"><td style=\"font-weight: bold\">value</td></tr><tr><td>{&quot;EVENT_TIME&quot;: &quot;2022-02-18 01:18:17&quot;, &quot;SYMBOL&quot;: &quot;AAPL&quot;, &quot;PRICE&quot;: 268.36, &quot;QUANTITY&quot;: 621}</td></tr><tr><td>{&quot;EVENT_TIME&quot;: &quot;2022-02-18 01:18:17&quot;, &quot;SYMBOL&quot;: &quot;GOOG&quot;, &quot;PRICE&quot;: 287.68, &quot;QUANTITY&quot;: 462}</td></tr><tr><td>{&quot;EVENT_TIME&quot;: &quot;2022-02-18 01:18:17&quot;, &quot;SYMBOL&quot;: &quot;MSFT&quot;, &quot;PRICE&quot;: 253.72, &quot;QUANTITY&quot;: 123}</td></tr><tr><td>{&quot;EVENT_TIME&quot;: &quot;2022-02-18 01:18:21&quot;, &quot;SYMBOL&quot;: &quot;AAPL&quot;, &quot;PRICE&quot;: 233.43, &quot;QUANTITY&quot;: 732}</td></tr><tr><td>{&quot;EVENT_TIME&quot;: &quot;2022-02-18 01:18:21&quot;, &quot;SYMBOL&quot;: &quot;GOOG&quot;, &quot;PRICE&quot;: 175.03, &quot;QUANTITY&quot;: 475}</td></tr><tr><td>{&quot;EVENT_TIME&quot;: &quot;2022-02-18 01:18:21&quot;, &quot;SYMBOL&quot;: &quot;MSFT&quot;, &quot;PRICE&quot;: 164.02, &quot;QUANTITY&quot;: 296}</td></tr><tr><td>{&quot;EVENT_TIME&quot;: &quot;2022-02-18 01:18:25&quot;, &quot;SYMBOL&quot;: &quot;AAPL&quot;, &quot;PRICE&quot;: 256.29, &quot;QUANTITY&quot;: 641}</td></tr><tr><td>{&quot;EVENT_TIME&quot;: &quot;2022-02-18 01:18:25&quot;, &quot;SYMBOL&quot;: &quot;GOOG&quot;, &quot;PRICE&quot;: 137.65, &quot;QUANTITY&quot;: 650}</td></tr><tr><td>{&quot;EVENT_TIME&quot;: &quot;2022-02-18 01:18:25&quot;, &quot;SYMBOL&quot;: &quot;MSFT&quot;, &quot;PRICE&quot;: 177.63, &quot;QUANTITY&quot;: 390}</td></tr><tr><td>{&quot;EVENT_TIME&quot;: &quot;2022-02-18 01:18:29&quot;, &quot;SYMBOL&quot;: &quot;AAPL&quot;, &quot;PRICE&quot;: 197.82, &quot;QUANTITY&quot;: 918}</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sparksql\n",
    "select * from debug1 order by value limit 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Stop a memory stream when you don't need it, as it can consume a lot of memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug1.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's try reading AVRO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stock_schema {\n",
      "    \"namespace\": \"stock.avro\",\n",
      "    \"type\": \"record\",\n",
      "    \"name\": \"Stock\",\n",
      "    \"fields\": [\n",
      "        {\"name\": \"event_time\", \"type\": \"string\"},\n",
      "        {\"name\": \"symbol\",  \"type\": \"string\"},\n",
      "        {\"name\": \"price\", \"type\": \"float\"},\n",
      "        {\"name\": \"quantity\", \"type\": \"int\"}\n",
      "    ]\n",
      "}\n",
      "stock_struct StructType(List(StructField(event_time,StringType,true),StructField(symbol,StringType,true),StructField(price,FloatType,true),StructField(quantity,IntegerType,true)))\n"
     ]
    }
   ],
   "source": [
    "stock_schema = open(\"stock.avsc\", \"r\").read()\n",
    "print('stock_schema', stock_schema)\n",
    "stock_struct = spark.read.format(\"avro\").option(\"avroSchema\", stock_schema).load().schema\n",
    "print('stock_struct', stock_struct)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df DataFrame[key: binary, value: binary, topic: string, partition: int, offset: bigint, timestamp: timestamp, timestampType: int]\n"
     ]
    }
   ],
   "source": [
    "brokers = 'localhost:9092'\n",
    "kafka_topic = 'stocks-avro'\n",
    "receiver_sleep_time = 4\n",
    "\n",
    "df = (spark.readStream \n",
    "    .format(\"kafka\") \n",
    "    .option(\"kafka.bootstrap.servers\", brokers) \n",
    "    .option(\"subscribe\", kafka_topic) \n",
    "    .option(\"startingOffsets\", \"latest\")\n",
    "    .option(\"failOnDataLoss\", False)\n",
    "    .load()\n",
    "    )\n",
    "print('df', df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug2 = write_memory(df, 'debug2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr style=\"border-bottom: 1px solid\"><td style=\"font-weight: bold\">timestamp</td><td style=\"font-weight: bold\">key</td><td style=\"font-weight: bold\">value</td></tr><tr><td>2022-02-19 03:54:37.857000</td><td>bytearray(b&#x27;\\xdaV\\x8f\\x16B\\xf6H\\x94\\xbctZ&quot;\\x8fD[\\xdb&#x27;)</td><td>bytearray(b&#x27;Obj\\x01\\x04\\x14avro.codec\\x08null\\x16avro.schema\\xc6\\x03{&quot;type&quot;: &quot;record&quot;, &quot;name&quot;: &quot;Stock&quot;, &quot;namespace&quot;: &quot;stock.avro&quot;, &quot;fields&quot;: [{&quot;type&quot;: &quot;string&quot;, &quot;name&quot;: &quot;event_time&quot;}, {&quot;type&quot;: &quot;string&quot;, &quot;name&quot;: &quot;symbol&quot;}, {&quot;type&quot;: &quot;float&quot;, &quot;name&quot;: &quot;price&quot;}, {&quot;type&quot;: &quot;int&quot;, &quot;name&quot;: &quot;quantity&quot;}]}\\x00-\\xa7nw\\xd73o\\x968\\x99C\\xc2s#\\x8bJ\\x02&gt;&amp;2022-02-19 03:54:37\\x08GOOGq\\xdd\\x8eC\\xb6\\x0e-\\xa7nw\\xd73o\\x968\\x99C\\xc2s#\\x8bJ&#x27;)</td></tr><tr><td>2022-02-19 03:54:36.897000</td><td>bytearray(b&#x27;\\xc4j\\n\\xaa\\x86\\xe6B\\x0b\\xb6\\xde\\xd8\\xb4\\x08Wl &#x27;)</td><td>bytearray(b&#x27;Obj\\x01\\x04\\x14avro.codec\\x08null\\x16avro.schema\\xc6\\x03{&quot;type&quot;: &quot;record&quot;, &quot;name&quot;: &quot;Stock&quot;, &quot;namespace&quot;: &quot;stock.avro&quot;, &quot;fields&quot;: [{&quot;type&quot;: &quot;string&quot;, &quot;name&quot;: &quot;event_time&quot;}, {&quot;type&quot;: &quot;string&quot;, &quot;name&quot;: &quot;symbol&quot;}, {&quot;type&quot;: &quot;float&quot;, &quot;name&quot;: &quot;price&quot;}, {&quot;type&quot;: &quot;int&quot;, &quot;name&quot;: &quot;quantity&quot;}]}\\x00\\xa8\\x98\\x86/H\\xf3B\\xc5\\x1a\\x03Z\\xad\\xe7/S\\x9a\\x02&gt;&amp;2022-02-19 03:54:36\\x08GOOG\\x85k-C\\xdc\\t\\xa8\\x98\\x86/H\\xf3B\\xc5\\x1a\\x03Z\\xad\\xe7/S\\x9a&#x27;)</td></tr><tr><td>2022-02-19 03:54:36.890000</td><td>bytearray(b&#x27;\\xfbnDK\\xed\\x8bH\\xcc\\x92\\xe3+\\x15\\x95\\xfd\\xb3\\x80&#x27;)</td><td>bytearray(b&#x27;Obj\\x01\\x04\\x14avro.codec\\x08null\\x16avro.schema\\xc6\\x03{&quot;type&quot;: &quot;record&quot;, &quot;name&quot;: &quot;Stock&quot;, &quot;namespace&quot;: &quot;stock.avro&quot;, &quot;fields&quot;: [{&quot;type&quot;: &quot;string&quot;, &quot;name&quot;: &quot;event_time&quot;}, {&quot;type&quot;: &quot;string&quot;, &quot;name&quot;: &quot;symbol&quot;}, {&quot;type&quot;: &quot;float&quot;, &quot;name&quot;: &quot;price&quot;}, {&quot;type&quot;: &quot;int&quot;, &quot;name&quot;: &quot;quantity&quot;}]}\\x00\\xb5\\xad\\xba\\x1b ,\\xfe!\\xff)\\xe8\\xf1\\x82ndV\\x02&gt;&amp;2022-02-19 03:54:36\\x08AAPL\\xd7cwC\\xa0\\x05\\xb5\\xad\\xba\\x1b ,\\xfe!\\xff)\\xe8\\xf1\\x82ndV&#x27;)</td></tr><tr><td>2022-02-19 03:54:36.846000</td><td>bytearray(b&#x27;\\xe2L&quot;\\xb5`0J\\xa8\\x87Bj%\\xa2F\\xe7\\xe7&#x27;)</td><td>bytearray(b&#x27;Obj\\x01\\x04\\x14avro.codec\\x08null\\x16avro.schema\\xc6\\x03{&quot;type&quot;: &quot;record&quot;, &quot;name&quot;: &quot;Stock&quot;, &quot;namespace&quot;: &quot;stock.avro&quot;, &quot;fields&quot;: [{&quot;type&quot;: &quot;string&quot;, &quot;name&quot;: &quot;event_time&quot;}, {&quot;type&quot;: &quot;string&quot;, &quot;name&quot;: &quot;symbol&quot;}, {&quot;type&quot;: &quot;float&quot;, &quot;name&quot;: &quot;price&quot;}, {&quot;type&quot;: &quot;int&quot;, &quot;name&quot;: &quot;quantity&quot;}]}\\x00\\x03A&gt;\\xe4c\\x92E\\x1f\\xfa\\xdfR!\\xe1\\xc7\\xf9\\xa2\\x02&gt;&amp;2022-02-19 03:54:36\\x08MSFT\\xaeGKC\\xec\\x04\\x03A&gt;\\xe4c\\x92E\\x1f\\xfa\\xdfR!\\xe1\\xc7\\xf9\\xa2&#x27;)</td></tr><tr><td>2022-02-19 03:54:34.062000</td><td>bytearray(b&#x27;3\\x8e\\xf7\\xa1l.II\\xb1*\\x13(R%QF&#x27;)</td><td>bytearray(b&#x27;Obj\\x01\\x04\\x14avro.codec\\x08null\\x16avro.schema\\xc6\\x03{&quot;type&quot;: &quot;record&quot;, &quot;name&quot;: &quot;Stock&quot;, &quot;namespace&quot;: &quot;stock.avro&quot;, &quot;fields&quot;: [{&quot;type&quot;: &quot;string&quot;, &quot;name&quot;: &quot;event_time&quot;}, {&quot;type&quot;: &quot;string&quot;, &quot;name&quot;: &quot;symbol&quot;}, {&quot;type&quot;: &quot;float&quot;, &quot;name&quot;: &quot;price&quot;}, {&quot;type&quot;: &quot;int&quot;, &quot;name&quot;: &quot;quantity&quot;}]}\\x00\\xd0&lt;\\x0e\\x96eY\\xe0;=\\xee&quot;\\xfd\\xdd\\xf1\\xcb=\\x02&lt;&amp;2022-02-19 03:54:34\\x08MSFT\\xd7#\\xfcBZ\\xd0&lt;\\x0e\\x96eY\\xe0;=\\xee&quot;\\xfd\\xdd\\xf1\\xcb=&#x27;)</td></tr><tr><td>2022-02-19 03:54:33.889000</td><td>bytearray(b&#x27;\\x00\\xb2\\xcd\\xe4\\xd6\\xddC\\x16\\xaeLC \\x980\\x8cQ&#x27;)</td><td>bytearray(b&#x27;Obj\\x01\\x04\\x14avro.codec\\x08null\\x16avro.schema\\xc6\\x03{&quot;type&quot;: &quot;record&quot;, &quot;name&quot;: &quot;Stock&quot;, &quot;namespace&quot;: &quot;stock.avro&quot;, &quot;fields&quot;: [{&quot;type&quot;: &quot;string&quot;, &quot;name&quot;: &quot;event_time&quot;}, {&quot;type&quot;: &quot;string&quot;, &quot;name&quot;: &quot;symbol&quot;}, {&quot;type&quot;: &quot;float&quot;, &quot;name&quot;: &quot;price&quot;}, {&quot;type&quot;: &quot;int&quot;, &quot;name&quot;: &quot;quantity&quot;}]}\\x00\\x87\\xae\\xafn0(b4\\x12=\\xab\\x14\\x17\\xf9M\\xcb\\x02&gt;&amp;2022-02-19 03:54:33\\x08AAPL)\\x9cWC\\xde\\x05\\x87\\xae\\xafn0(b4\\x12=\\xab\\x14\\x17\\xf9M\\xcb&#x27;)</td></tr><tr><td>2022-02-19 03:54:33.853000</td><td>bytearray(b&#x27;\\x85\\xfc\\xac\\xad5\\x8cJ\\xc5\\xbc7\\x0e\\xd0pU\\xc8 &#x27;)</td><td>bytearray(b&#x27;Obj\\x01\\x04\\x14avro.codec\\x08null\\x16avro.schema\\xc6\\x03{&quot;type&quot;: &quot;record&quot;, &quot;name&quot;: &quot;Stock&quot;, &quot;namespace&quot;: &quot;stock.avro&quot;, &quot;fields&quot;: [{&quot;type&quot;: &quot;string&quot;, &quot;name&quot;: &quot;event_time&quot;}, {&quot;type&quot;: &quot;string&quot;, &quot;name&quot;: &quot;symbol&quot;}, {&quot;type&quot;: &quot;float&quot;, &quot;name&quot;: &quot;price&quot;}, {&quot;type&quot;: &quot;int&quot;, &quot;name&quot;: &quot;quantity&quot;}]}\\x00\\xaf2hBM\\x1c\\xdb*+F\\xfd5\\xaf\\xdb\\x06+\\x02&gt;&amp;2022-02-19 03:54:33\\x08GOOG\\x9aYAC\\xd2\\x04\\xaf2hBM\\x1c\\xdb*+F\\xfd5\\xaf\\xdb\\x06+&#x27;)</td></tr><tr><td>2022-02-19 03:54:32.894000</td><td>bytearray(b&#x27;\\xf2\\x13B\\x95\\xc4uI\\x12\\x89\\x08)V\\x1eB\\xf5\\xf9&#x27;)</td><td>bytearray(b&#x27;Obj\\x01\\x04\\x14avro.codec\\x08null\\x16avro.schema\\xc6\\x03{&quot;type&quot;: &quot;record&quot;, &quot;name&quot;: &quot;Stock&quot;, &quot;namespace&quot;: &quot;stock.avro&quot;, &quot;fields&quot;: [{&quot;type&quot;: &quot;string&quot;, &quot;name&quot;: &quot;event_time&quot;}, {&quot;type&quot;: &quot;string&quot;, &quot;name&quot;: &quot;symbol&quot;}, {&quot;type&quot;: &quot;float&quot;, &quot;name&quot;: &quot;price&quot;}, {&quot;type&quot;: &quot;int&quot;, &quot;name&quot;: &quot;quantity&quot;}]}\\x00\\x04\\xd4\\x98\\x1d\\xab\\xd35C$\\xe3\\x1d\\xd1\\xac\\x80\\x08^\\x02&gt;&amp;2022-02-19 03:54:32\\x08GOOG\\xec\\x91&amp;C\\x86\\n\\x04\\xd4\\x98\\x1d\\xab\\xd35C$\\xe3\\x1d\\xd1\\xac\\x80\\x08^&#x27;)</td></tr><tr><td>2022-02-19 03:54:32.885000</td><td>bytearray(b&#x27;\\x87w\\xc9Tu\\x96N\\x96\\xa2/aZ\\xd3\\xca\\x1eN&#x27;)</td><td>bytearray(b&#x27;Obj\\x01\\x04\\x14avro.codec\\x08null\\x16avro.schema\\xc6\\x03{&quot;type&quot;: &quot;record&quot;, &quot;name&quot;: &quot;Stock&quot;, &quot;namespace&quot;: &quot;stock.avro&quot;, &quot;fields&quot;: [{&quot;type&quot;: &quot;string&quot;, &quot;name&quot;: &quot;event_time&quot;}, {&quot;type&quot;: &quot;string&quot;, &quot;name&quot;: &quot;symbol&quot;}, {&quot;type&quot;: &quot;float&quot;, &quot;name&quot;: &quot;price&quot;}, {&quot;type&quot;: &quot;int&quot;, &quot;name&quot;: &quot;quantity&quot;}]}\\x00\\x1b!\\xbc\\x1bL\\xe0-s*\\xe2\\xb85lI\\xe7\\x11\\x02&gt;&amp;2022-02-19 03:54:32\\x08AAPL\\xe1:%C\\xb6\\x02\\x1b!\\xbc\\x1bL\\xe0-s*\\xe2\\xb85lI\\xe7\\x11&#x27;)</td></tr><tr><td>2022-02-19 03:54:32.841000</td><td>bytearray(b&#x27;\\x01\\xb8v|\\xa5qKe\\x9a\\xea|T\\xf2c\\x19[&#x27;)</td><td>bytearray(b&#x27;Obj\\x01\\x04\\x14avro.codec\\x08null\\x16avro.schema\\xc6\\x03{&quot;type&quot;: &quot;record&quot;, &quot;name&quot;: &quot;Stock&quot;, &quot;namespace&quot;: &quot;stock.avro&quot;, &quot;fields&quot;: [{&quot;type&quot;: &quot;string&quot;, &quot;name&quot;: &quot;event_time&quot;}, {&quot;type&quot;: &quot;string&quot;, &quot;name&quot;: &quot;symbol&quot;}, {&quot;type&quot;: &quot;float&quot;, &quot;name&quot;: &quot;price&quot;}, {&quot;type&quot;: &quot;int&quot;, &quot;name&quot;: &quot;quantity&quot;}]}\\x00\\x01{\\xc1\\xf5\\xbbu\\xe3\\xd7\\x01\\xaa\\xd7E{\\xe3_L\\x02&gt;&amp;2022-02-19 03:54:32\\x08MSFT\\n\\x17\\&#x27;C\\xfe\\n\\x01{\\xc1\\xf5\\xbbu\\xe3\\xd7\\x01\\xaa\\xd7E{\\xe3_L&#x27;)</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%sparksql\n",
    "select timestamp, key, value from debug2 order by timestamp desc limit 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ConsumerRecord(topic='stocks-avro', partition=0, offset=40679, timestamp=1645243362535, timestamp_type=0, key=b'\\xd6\\x0cgMs<By\\xb8\\xcaR\\x02\\xe0\\xfa\\x93\\x14', value=b'Obj\\x01\\x04\\x14avro.codec\\x08null\\x16avro.schema\\xc6\\x03{\"type\": \"record\", \"name\": \"Stock\", \"namespace\": \"stock.avro\", \"fields\": [{\"type\": \"string\", \"name\": \"event_time\"}, {\"type\": \"string\", \"name\": \"symbol\"}, {\"type\": \"float\", \"name\": \"price\"}, {\"type\": \"int\", \"name\": \"quantity\"}]}\\x00$\\x7f#w\\xeaD\\xdchK\\xc5!\\xf85\\x10\\xc8J\\x02>&2022-02-19 04:02:42\\x08MSFT\\xa4p\\xdfB\\xf0\\t$\\x7f#w\\xeaD\\xdchK\\xc5!\\xf85\\x10\\xc8J', headers=[], checksum=None, serialized_key_size=16, serialized_value_size=328, serialized_header_size=-1)\n",
    "ConsumerRecord(topic='stocks-avro', partition=0, offset=40679, timestamp=1645243362535, timestamp_type=0, key=b'\\xd6\\x0cgMs<By\\xb8\\xcaR\\x02\\xe0\\xfa\\x93\\x14', value=b'Obj\\x01\\x04\\x14avro.codec\\x08null\\x16avro.schema\\xc6\\x03{\"type\": \"record\", \"name\": \"Stock\", \"namespace\": \"stock.avro\", \"fields\": [{\"type\": \"string\", \"name\": \"event_time\"}, {\"type\": \"string\", \"name\": \"symbol\"}, {\"type\": \"float\", \"name\": \"price\"}, {\"type\": \"int\", \"name\": \"quantity\"}]}\\x00$\\x7f#w\\xeaD\\xdchK\\xc5!\\xf85\\x10\\xc8J\\x02>&2022-02-19 04:02:42\\x08MSFT\\xa4p\\xdfB\\xf0\\t$\\x7f#w\\xeaD\\xdchK\\xc5!\\xf85\\x10\\xc8J', headers=[], checksum=None, serialized_key_size=16, serialized_value_size=328, serialized_header_size=-1)\n",
    "\n",
    "ConsumerRecord(topic='stocks-avro', partition=0, offset=40814, timestamp=1645243470645, timestamp_type=0, key=b'\\xc8\\xeb\\xc2\\xe9O\\xaaJ\\x86\\x83\\x85\\xb9\\xd7\\xf46\\xea\\x8f', value=b'Obj\\x01\\x04\\x14avro.codec\\x08null\\x16avro.schema\\xc6\\x03{\"type\": \"record\", \"name\": \"Stock\", \"namespace\": \"stock.avro\", \"fields\": [{\"type\": \"string\", \"name\": \"event_time\"}, {\"type\": \"string\", \"name\": \"symbol\"}, {\"type\": \"float\", \"name\": \"price\"}, {\"type\": \"int\", \"name\": \"quantity\"}]}\\x00\\xe8\\xf5x\\r\\xbf\\x8aC\\x98&\\xaf\\x13iz\\x9dp\\x13\\x02>&2022-02-19 04:04:30\\x08MSFT\\xc3\\xf5\\x11C\\xac\\x04\\xe8\\xf5x\\r\\xbf\\x8aC\\x98&\\xaf\\x13iz\\x9dp\\x13', headers=[], checksum=None, serialized_key_size=16, serialized_value_size=328, serialized_header_size=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/02/19 03:59:03 ERROR WriteToDataSourceV2Exec: Data source write support org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite@65c97253 is aborting.\n",
      "22/02/19 03:59:03 ERROR WriteToDataSourceV2Exec: Data source write support org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite@65c97253 aborted.\n",
      "22/02/19 03:59:03 ERROR Utils: Aborting task\n",
      "org.apache.spark.TaskKilledException\n",
      "\tat org.apache.spark.TaskContextImpl.killTaskIfInterrupted(TaskContextImpl.scala:216)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:36)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.$anonfun$run$1(WriteToDataSourceV2Exec.scala:412)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:457)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:358)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "22/02/19 03:59:03 ERROR DataWritingSparkTask: Aborting commit for partition 0 (task 542, attempt 0, stage 542.0)\n",
      "22/02/19 03:59:03 ERROR DataWritingSparkTask: Aborted commit for partition 0 (task 542, attempt 0, stage 542.0)\n"
     ]
    }
   ],
   "source": [
    "debug2.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df3 DataFrame[timestamp: timestamp, key: binary, value: struct<event_time:string,symbol:string,price:float,quantity:int>]\n"
     ]
    }
   ],
   "source": [
    "if 'debug3' in locals():\n",
    "    debug3.stop()\n",
    "    \n",
    "from pyspark.sql.avro.functions import from_avro, to_avro\n",
    "stock_schema = \"\"\"{\n",
    "    \"type\": \"record\",\n",
    "    \"name\": \"Stock\",\n",
    "    \"fields\": [\n",
    "        {\"name\": \"event_time\", \"type\": \"string\"},\n",
    "        {\"name\": \"symbol\",  \"type\": \"string\"},\n",
    "        {\"name\": \"price\", \"type\": \"float\"},\n",
    "        {\"name\": \"quantity\", \"type\": \"int\"}\n",
    "    ]\n",
    "}\"\"\"\n",
    "\n",
    "df3 = df.select(\"timestamp\", \"key\", from_avro(df.value, stock_schema, options = {\"mode\":\"PERMISSIVE\"}).alias(\"value\"))\n",
    "print('df3', df3)\n",
    "debug3 = write_memory(df3, 'debug3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr style=\"border-bottom: 1px solid\"><td style=\"font-weight: bold\">timestamp</td><td style=\"font-weight: bold\">key</td><td style=\"font-weight: bold\">value</td></tr><tr><td>2022-02-19 04:09:26.907000</td><td>bytearray(b&#x27;\\x8c&lt;E\\xa1\\x1f7O\\xfb\\x86\\xc2\\x97\\xadd\\x8d\\xe7B&#x27;)</td><td>Row(event_time=None, symbol=None, price=None, quantity=None)</td></tr><tr><td>2022-02-19 04:09:26.739000</td><td>bytearray(b&#x27;\\xe1c\\xbc\\xbb\\x8e\\x8fI\\x93\\xa6\\xfcC\\xcbM\\xae\\xb78&#x27;)</td><td>Row(event_time=None, symbol=None, price=None, quantity=None)</td></tr><tr><td>2022-02-19 04:09:26.658000</td><td>bytearray(b&#x27;\\xfd\\xe6\\x9d\\x90\\xc6\\x9aM{\\xac\\x11\\x93VE\\x00H\\x81&#x27;)</td><td>Row(event_time=None, symbol=None, price=None, quantity=None)</td></tr><tr><td>2022-02-19 04:09:22.905000</td><td>bytearray(b&#x27;D\\xb6)1=\\x8aC\\x88\\xa2p]\\x8as\\xe4/k&#x27;)</td><td>Row(event_time=None, symbol=None, price=None, quantity=None)</td></tr><tr><td>2022-02-19 04:09:22.734000</td><td>bytearray(b&#x27;\\xa1Z\\xa5w\\x105I\\xeb\\xb7\\x8c\\x94\\rE\\xd5\\x10}&#x27;)</td><td>Row(event_time=None, symbol=None, price=None, quantity=None)</td></tr><tr><td>2022-02-19 04:09:22.653000</td><td>bytearray(b&#x27;}bV\\xbe\\xbc\\xbbI|\\xa4\\xca\\x9f\\xc0\\ny~\\x8f&#x27;)</td><td>Row(event_time=None, symbol=None, price=None, quantity=None)</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%sparksql\n",
    "select * from debug3 order by timestamp desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug3.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's read some data from the JSON stream and fix it up to make it more usable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stock_schema {\n",
      "    \"namespace\": \"stock.avro\",\n",
      "    \"type\": \"record\",\n",
      "    \"name\": \"Stock\",\n",
      "    \"fields\": [\n",
      "        {\"name\": \"event_time\", \"type\": \"string\"},\n",
      "        {\"name\": \"symbol\",  \"type\": \"string\"},\n",
      "        {\"name\": \"price\", \"type\": \"float\"},\n",
      "        {\"name\": \"quantity\", \"type\": \"int\"}\n",
      "    ]\n",
      "}\n",
      "stock_struct StructType(List(StructField(event_time,StringType,true),StructField(symbol,StringType,true),StructField(price,FloatType,true),StructField(quantity,IntegerType,true)))\n",
      "df DataFrame[key: binary, value: binary, topic: string, partition: int, offset: bigint, timestamp: timestamp, timestampType: int]\n",
      "df1 DataFrame[key: string, timestamp: timestamp, value: string]\n",
      "df2 DataFrame[key: string, timestamp: timestamp, value2: struct<event_time:string,symbol:string,price:float,quantity:int>]\n",
      "df4 DataFrame[key: string, timestamp: timestamp, event_time: string, symbol: string, price: float, quantity: int]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "import uuid\n",
    "\n",
    "brokers = 'localhost:9092'\n",
    "kafka_topic = 'stocks-json'\n",
    "receiver_sleep_time = 4\n",
    "\n",
    "stock_schema = open(\"stock.avsc\", \"r\").read()\n",
    "print('stock_schema', stock_schema)\n",
    "\n",
    "stock_struct = spark.read.format(\"avro\").option(\"avroSchema\", stock_schema).load().schema\n",
    "print('stock_struct', stock_struct)\n",
    "\n",
    "df = (spark.readStream \n",
    "    .format(\"kafka\") \n",
    "    .option(\"kafka.bootstrap.servers\", brokers) \n",
    "    .option(\"subscribe\", kafka_topic) \n",
    "    .option(\"startingOffsets\", \"earliest\")\n",
    "    .option(\"failOnDataLoss\", False)\n",
    "#    .option(\"kafka.group.id\", \"stock-json-spark-group\")\n",
    "    .load()\n",
    "    )\n",
    "print('df', df)\n",
    "\n",
    "\n",
    "def convert_uuid(value):\n",
    "    # value is a bytearray in this case coming from spark\n",
    "    ret = uuid.UUID(bytes = bytes(value))\n",
    "    return str(ret)\n",
    "\n",
    "convert_uuid_udf = udf(convert_uuid, StringType())\n",
    "\n",
    "# keep the key and timestamp and convert the value from bytes to string\n",
    "#df1 = df.select(col(\"key\"), \"timestamp\", expr(\"CAST(value AS STRING) as value\"))\n",
    "df1 = df.select(convert_uuid_udf(col(\"key\")).alias(\"key\"), \"timestamp\", expr(\"CAST(value AS STRING) as value\"))\n",
    "print('df1', df1)\n",
    "\n",
    "# cast the string json to a struct\n",
    "# keep all the columns we selected and convery the JSON string into a struct object and remove the string version\n",
    "df2 = df1.select(*df1.columns, from_json(df1.value, stock_struct).alias(\"value2\")).drop('value')\n",
    "print('df2', df2)\n",
    "\n",
    "# flatten the struct to a normal DataFrame\n",
    "df4 = df2.select(*(df2.columns), col(\"value2.*\")).drop('value2')\n",
    "print('df4', df4)\n",
    "\n",
    "if 'debug4' in locals():\n",
    "    debug4.stop()\n",
    "    \n",
    "debug4 = write_memory(df4, 'debug4')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "only showing top 20 row(s)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table><tr style=\"border-bottom: 1px solid\"><td style=\"font-weight: bold\">key</td><td style=\"font-weight: bold\">timestamp</td><td style=\"font-weight: bold\">event_time</td><td style=\"font-weight: bold\">symbol</td><td style=\"font-weight: bold\">price</td><td style=\"font-weight: bold\">quantity</td></tr><tr><td>a75b2000-988e-4334-a7c1-272e5f2e7553</td><td>2022-02-19 05:17:21.537000</td><td>2022-02-19 05:17:21</td><td>AAPL</td><td>134.86000061035156</td><td>96</td></tr><tr><td>7ea8125a-e910-4255-bba7-d4c4edacca33</td><td>2022-02-19 05:17:21.529000</td><td>2022-02-19 05:17:21</td><td>GOOG</td><td>213.97000122070312</td><td>97</td></tr><tr><td>b985e72a-6969-4577-8182-30d8f60996cf</td><td>2022-02-19 05:17:21.513000</td><td>2022-02-19 05:17:21</td><td>MSFT</td><td>182.24000549316406</td><td>308</td></tr><tr><td>af609968-cfc4-41ab-894e-f6b016839bde</td><td>2022-02-19 05:17:17.509000</td><td>2022-02-19 05:17:17</td><td>MSFT</td><td>273.6600036621094</td><td>413</td></tr><tr><td>d4ae6ca4-594d-4bb9-8283-841fa2b31a83</td><td>2022-02-19 05:17:17.533000</td><td>2022-02-19 05:17:17</td><td>AAPL</td><td>277.17999267578125</td><td>624</td></tr><tr><td>82d075e4-50b6-4d09-b5e0-b97465ce87e8</td><td>2022-02-19 05:17:17.525000</td><td>2022-02-19 05:17:17</td><td>GOOG</td><td>273.9599914550781</td><td>863</td></tr><tr><td>3425ce13-f4c5-4cff-955e-1e2f4df428b5</td><td>2022-02-19 05:17:13.521000</td><td>2022-02-19 05:17:13</td><td>GOOG</td><td>268.4100036621094</td><td>160</td></tr><tr><td>75467653-dcad-4801-a50f-9e72fb81aac9</td><td>2022-02-19 05:17:13.529000</td><td>2022-02-19 05:17:13</td><td>AAPL</td><td>248.05999755859375</td><td>474</td></tr><tr><td>db9944f4-5937-433f-a53c-a8ae0e40ac29</td><td>2022-02-19 05:17:13.505000</td><td>2022-02-19 05:17:13</td><td>MSFT</td><td>216.8000030517578</td><td>500</td></tr><tr><td>0df83ea6-d4e7-40f6-ab00-94c4a7c659ad</td><td>2022-02-19 05:17:09.525000</td><td>2022-02-19 05:17:09</td><td>AAPL</td><td>188.86000061035156</td><td>997</td></tr><tr><td>7a138931-737e-4f3b-9633-f40491de7752</td><td>2022-02-19 05:17:09.517000</td><td>2022-02-19 05:17:09</td><td>GOOG</td><td>149.58999633789062</td><td>86</td></tr><tr><td>59a5c101-4866-4327-b253-5080e363a48f</td><td>2022-02-19 05:17:09.501000</td><td>2022-02-19 05:17:09</td><td>MSFT</td><td>143.52000427246094</td><td>50</td></tr><tr><td>df0e6ced-cdc6-4cf7-a3b9-cc17b0e170a4</td><td>2022-02-19 05:17:05.497000</td><td>2022-02-19 05:17:05</td><td>MSFT</td><td>185.27999877929688</td><td>413</td></tr><tr><td>17852399-0c2e-48a7-a7b0-513b7cc8801b</td><td>2022-02-19 05:17:05.513000</td><td>2022-02-19 05:17:05</td><td>GOOG</td><td>286.19000244140625</td><td>536</td></tr><tr><td>02f4158f-b36d-45ea-a188-70a7711aaab2</td><td>2022-02-19 05:17:05.521000</td><td>2022-02-19 05:17:05</td><td>AAPL</td><td>278.7200012207031</td><td>415</td></tr><tr><td>c6e068b7-4b31-4941-8781-30ebaa04fd37</td><td>2022-02-19 05:17:01.509000</td><td>2022-02-19 05:17:01</td><td>GOOG</td><td>294.8699951171875</td><td>626</td></tr><tr><td>62c364ac-40ad-4598-9677-6d62033d17bb</td><td>2022-02-19 05:17:01.517000</td><td>2022-02-19 05:17:01</td><td>AAPL</td><td>144.6300048828125</td><td>513</td></tr><tr><td>e3a5ec39-1a25-4fae-bea2-5ca7a86f1f7c</td><td>2022-02-19 05:17:01.496000</td><td>2022-02-19 05:17:01</td><td>MSFT</td><td>277.489990234375</td><td>908</td></tr><tr><td>1a062831-b305-445b-aaf4-b2c38df4f2ac</td><td>2022-02-19 05:16:57.493000</td><td>2022-02-19 05:16:57</td><td>MSFT</td><td>102.02999877929688</td><td>868</td></tr><tr><td>e7104622-aa5b-4bb3-a2d3-735077508899</td><td>2022-02-19 05:16:57.505000</td><td>2022-02-19 05:16:57</td><td>GOOG</td><td>192.4199981689453</td><td>50</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sparksql \n",
    "select * from debug4 order by event_time desc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug4.stop()\n",
    "debug4 = write_memory(df4, 'debug4')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now that we have a normal DataFrame, let's manipulate it how we want and write the results out to another stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[window: struct<start:timestamp,end:timestamp>, symbol: string, sum: bigint]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4448:======>     (104 + 1) / 200][Stage 4449:>               (0 + 0) / 1]\r"
     ]
    }
   ],
   "source": [
    "fixed_window = (df4.select(\"timestamp\", \"symbol\",\"quantity\")\n",
    "        .withWatermark(\"timestamp\", \"10 seconds\") \n",
    "        .groupBy(window(\"timestamp\", \"10 seconds\").alias(\"window\"), \"symbol\") \n",
    "        .agg(sum(\"quantity\").alias(\"sum\"))\n",
    "        )\n",
    "print(fixed_window)\n",
    "\n",
    "debug5 = write_memory(fixed_window, 'debug5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can see that we get aggregate by symbol every ten seconds. This data can be written off somewhere like a SQL or NoSQL database or forwarded as a new message to create a streaming aggregation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr style=\"border-bottom: 1px solid\"><td style=\"font-weight: bold\">window</td><td style=\"font-weight: bold\">symbol</td><td style=\"font-weight: bold\">sum</td></tr><tr><td>Row(start=datetime.datetime(2022, 2, 19, 5, 36, 30), end=datetime.datetime(2022, 2, 19, 5, 36, 40))</td><td>AAPL</td><td>818</td></tr><tr><td>Row(start=datetime.datetime(2022, 2, 19, 5, 36, 30), end=datetime.datetime(2022, 2, 19, 5, 36, 40))</td><td>GOOG</td><td>952</td></tr><tr><td>Row(start=datetime.datetime(2022, 2, 19, 5, 36, 30), end=datetime.datetime(2022, 2, 19, 5, 36, 40))</td><td>MSFT</td><td>1571</td></tr><tr><td>Row(start=datetime.datetime(2022, 2, 19, 5, 36, 20), end=datetime.datetime(2022, 2, 19, 5, 36, 30))</td><td>AAPL</td><td>781</td></tr><tr><td>Row(start=datetime.datetime(2022, 2, 19, 5, 36, 20), end=datetime.datetime(2022, 2, 19, 5, 36, 30))</td><td>GOOG</td><td>1138</td></tr><tr><td>Row(start=datetime.datetime(2022, 2, 19, 5, 36, 20), end=datetime.datetime(2022, 2, 19, 5, 36, 30))</td><td>MSFT</td><td>724</td></tr><tr><td>Row(start=datetime.datetime(2022, 2, 19, 5, 36, 10), end=datetime.datetime(2022, 2, 19, 5, 36, 20))</td><td>AAPL</td><td>2227</td></tr><tr><td>Row(start=datetime.datetime(2022, 2, 19, 5, 36, 10), end=datetime.datetime(2022, 2, 19, 5, 36, 20))</td><td>GOOG</td><td>1639</td></tr><tr><td>Row(start=datetime.datetime(2022, 2, 19, 5, 36, 10), end=datetime.datetime(2022, 2, 19, 5, 36, 20))</td><td>MSFT</td><td>647</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sparksql\n",
    "select * from debug5 order by window desc, symbol limit 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "debug5.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sliding windows are similar except you give it two parameters, the first is the total length of the window and the second is the refresh interval. In this case the windows will overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[key: string, timestamp: timestamp, event_time: string, symbol: string, price: float, quantity: int]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4106:======>      (98 + 1) / 200][Stage 4107:>               (0 + 0) / 1]\r"
     ]
    }
   ],
   "source": [
    "sliding_window = (df4.select(\"timestamp\", \"symbol\",\"quantity\")\n",
    "        .withWatermark(\"timestamp\", \"10 seconds\") \n",
    "        .groupBy(window(\"timestamp\", \"30 seconds\", \"10 seconds\").alias(\"window\"), \"symbol\") \n",
    "        .agg(sum(\"quantity\").alias(\"sum\"))\n",
    "        )\n",
    "print(sliding_window)\n",
    "\n",
    "debug6 = write_memory(sliding_window, 'debug6')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "only showing top 20 row(s)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table><tr style=\"border-bottom: 1px solid\"><td style=\"font-weight: bold\">window</td><td style=\"font-weight: bold\">symbol</td><td style=\"font-weight: bold\">sum</td></tr><tr><td>Row(start=datetime.datetime(2022, 2, 19, 5, 27, 20), end=datetime.datetime(2022, 2, 19, 5, 27, 50))</td><td>AAPL</td><td>3332</td></tr><tr><td>Row(start=datetime.datetime(2022, 2, 19, 5, 27, 20), end=datetime.datetime(2022, 2, 19, 5, 27, 50))</td><td>GOOG</td><td>3656</td></tr><tr><td>Row(start=datetime.datetime(2022, 2, 19, 5, 27, 20), end=datetime.datetime(2022, 2, 19, 5, 27, 50))</td><td>MSFT</td><td>4305</td></tr><tr><td>Row(start=datetime.datetime(2022, 2, 19, 5, 27, 10), end=datetime.datetime(2022, 2, 19, 5, 27, 40))</td><td>AAPL</td><td>5031</td></tr><tr><td>Row(start=datetime.datetime(2022, 2, 19, 5, 27, 10), end=datetime.datetime(2022, 2, 19, 5, 27, 40))</td><td>GOOG</td><td>5215</td></tr><tr><td>Row(start=datetime.datetime(2022, 2, 19, 5, 27, 10), end=datetime.datetime(2022, 2, 19, 5, 27, 40))</td><td>MSFT</td><td>4219</td></tr><tr><td>Row(start=datetime.datetime(2022, 2, 19, 5, 27), end=datetime.datetime(2022, 2, 19, 5, 27, 30))</td><td>AAPL</td><td>4103</td></tr><tr><td>Row(start=datetime.datetime(2022, 2, 19, 5, 27), end=datetime.datetime(2022, 2, 19, 5, 27, 30))</td><td>GOOG</td><td>4407</td></tr><tr><td>Row(start=datetime.datetime(2022, 2, 19, 5, 27), end=datetime.datetime(2022, 2, 19, 5, 27, 30))</td><td>MSFT</td><td>3525</td></tr><tr><td>Row(start=datetime.datetime(2022, 2, 19, 5, 26, 50), end=datetime.datetime(2022, 2, 19, 5, 27, 20))</td><td>AAPL</td><td>4921</td></tr><tr><td>Row(start=datetime.datetime(2022, 2, 19, 5, 26, 50), end=datetime.datetime(2022, 2, 19, 5, 27, 20))</td><td>GOOG</td><td>5813</td></tr><tr><td>Row(start=datetime.datetime(2022, 2, 19, 5, 26, 50), end=datetime.datetime(2022, 2, 19, 5, 27, 20))</td><td>MSFT</td><td>3781</td></tr><tr><td>Row(start=datetime.datetime(2022, 2, 19, 5, 26, 40), end=datetime.datetime(2022, 2, 19, 5, 27, 10))</td><td>AAPL</td><td>3651</td></tr><tr><td>Row(start=datetime.datetime(2022, 2, 19, 5, 26, 40), end=datetime.datetime(2022, 2, 19, 5, 27, 10))</td><td>GOOG</td><td>5217</td></tr><tr><td>Row(start=datetime.datetime(2022, 2, 19, 5, 26, 40), end=datetime.datetime(2022, 2, 19, 5, 27, 10))</td><td>MSFT</td><td>4189</td></tr><tr><td>Row(start=datetime.datetime(2022, 2, 19, 5, 26, 30), end=datetime.datetime(2022, 2, 19, 5, 27))</td><td>AAPL</td><td>4706</td></tr><tr><td>Row(start=datetime.datetime(2022, 2, 19, 5, 26, 30), end=datetime.datetime(2022, 2, 19, 5, 27))</td><td>GOOG</td><td>4589</td></tr><tr><td>Row(start=datetime.datetime(2022, 2, 19, 5, 26, 30), end=datetime.datetime(2022, 2, 19, 5, 27))</td><td>MSFT</td><td>5179</td></tr><tr><td>Row(start=datetime.datetime(2022, 2, 19, 5, 26, 20), end=datetime.datetime(2022, 2, 19, 5, 26, 50))</td><td>AAPL</td><td>3005</td></tr><tr><td>Row(start=datetime.datetime(2022, 2, 19, 5, 26, 20), end=datetime.datetime(2022, 2, 19, 5, 26, 50))</td><td>GOOG</td><td>3493</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4165:=>           (28 + 1) / 200][Stage 4167:>               (0 + 0) / 1]\r"
     ]
    }
   ],
   "source": [
    "%%sparksql\n",
    "select * from debug6 order by window desc, symbol limit 21\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/02/19 05:29:15 ERROR WriteToDataSourceV2Exec: Data source write support org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite@5fd0f6ba is aborting.\n",
      "22/02/19 05:29:15 ERROR WriteToDataSourceV2Exec: Data source write support org.apache.spark.sql.execution.streaming.sources.MicroBatchWrite@5fd0f6ba aborted.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "debug6.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session Window is similar but used to group data that represents a continuous stream of activity. The time specifies a timeout period or period of inactivity that indicates when a session should end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_window = (df4.select(\"timestamp\", \"symbol\",\"quantity\")\n",
    "        .withWatermark(\"timestamp\", \"10 seconds\") \n",
    "        .groupBy(session_window(\"timestamp\", \"5 minutes\").alias(\"window\"), \"symbol\") \n",
    "        .agg(sum(\"quantity\").alias(\"sum\"))\n",
    "        )\n",
    "print(session_window)\n",
    "\n",
    "debug7 = write_memory(session_window, 'debug7')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's join the streaming aggregation with a static reference table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4648:==========> (180 + 1) / 200][Stage 4651:>               (0 + 0) / 1]\r"
     ]
    }
   ],
   "source": [
    "x = sc.parallelize([('AAPL', 'Apple'), ('MSFT', 'Microsoft'), ('GOOG','Google')])\n",
    "stocks = spark.createDataFrame(x, 'symbol:string, name:string')\n",
    "stocks.createOrReplaceTempView('stocks')\n",
    "fixed_window.createOrReplaceTempView('trades')\n",
    "\n",
    "joined_aggregate = spark.sql(\"\"\"\n",
    "SELECT t.*, s.name\n",
    "FROM trades as t\n",
    "JOIN stocks as s on t.symbol = s.symbol\n",
    "\"\"\")\n",
    "\n",
    "debug8 = write_memory(joined_aggregate, 'debug8')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table><tr style=\"border-bottom: 1px solid\"><td style=\"font-weight: bold\">window</td><td style=\"font-weight: bold\">symbol</td><td style=\"font-weight: bold\">sum</td><td style=\"font-weight: bold\">name</td></tr><tr><td>Row(start=datetime.datetime(2022, 2, 19, 5, 44, 10), end=datetime.datetime(2022, 2, 19, 5, 44, 20))</td><td>AAPL</td><td>2051</td><td>Apple</td></tr><tr><td>Row(start=datetime.datetime(2022, 2, 19, 5, 44, 10), end=datetime.datetime(2022, 2, 19, 5, 44, 20))</td><td>GOOG</td><td>1442</td><td>Google</td></tr><tr><td>Row(start=datetime.datetime(2022, 2, 19, 5, 44, 10), end=datetime.datetime(2022, 2, 19, 5, 44, 20))</td><td>MSFT</td><td>2047</td><td>Microsoft</td></tr><tr><td>Row(start=datetime.datetime(2022, 2, 19, 5, 44), end=datetime.datetime(2022, 2, 19, 5, 44, 10))</td><td>AAPL</td><td>1490</td><td>Apple</td></tr><tr><td>Row(start=datetime.datetime(2022, 2, 19, 5, 44), end=datetime.datetime(2022, 2, 19, 5, 44, 10))</td><td>GOOG</td><td>554</td><td>Google</td></tr><tr><td>Row(start=datetime.datetime(2022, 2, 19, 5, 44), end=datetime.datetime(2022, 2, 19, 5, 44, 10))</td><td>MSFT</td><td>454</td><td>Microsoft</td></tr><tr><td>Row(start=datetime.datetime(2022, 2, 19, 5, 43, 50), end=datetime.datetime(2022, 2, 19, 5, 44))</td><td>AAPL</td><td>1776</td><td>Apple</td></tr><tr><td>Row(start=datetime.datetime(2022, 2, 19, 5, 43, 50), end=datetime.datetime(2022, 2, 19, 5, 44))</td><td>GOOG</td><td>1533</td><td>Google</td></tr><tr><td>Row(start=datetime.datetime(2022, 2, 19, 5, 43, 50), end=datetime.datetime(2022, 2, 19, 5, 44))</td><td>MSFT</td><td>2030</td><td>Microsoft</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4793:=========>  (154 + 1) / 200][Stage 4796:>               (0 + 0) / 1]\r"
     ]
    }
   ],
   "source": [
    "%%sparksql\n",
    "select * from debug8 order by window desc, symbol limit 9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/02/19 05:45:07 ERROR TorrentBroadcast: Store broadcast broadcast_4933 fail, remove all pieces of the broadcast\n"
     ]
    }
   ],
   "source": [
    "debug8.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
