{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "931beffe-4cf4-48e4-a28d-8a2dfaed62c0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1. <font color='blue' face=\"Fixedsys, monospace\" size=\"+2\">ReadFromText</font> allows you to read a text file into a <font color='green' size=\"+2\">PCollection</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19db17f3-ea3a-4418-b12e-5b859d1e9856",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <img src=\"images/python.png\" width=40 height=40 /><font color='cadetblue' size=\"+2\">Python</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab382f85-16e9-4544-a56b-06c6d4edc7b6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### It's a good idea to start naming the steps for debugging and monitoring later. Names must be unique in the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53066990-43fc-42c0-9545-ddf723145fce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 'EASTERN')\n",
      "(2, 'WESTERN')\n",
      "(3, 'NORTHERN')\n",
      "(4, 'SOUTHERN')\n"
     ]
    }
   ],
   "source": [
    "! rm /tmp/outputs*\n",
    "\n",
    "import apache_beam as beam\n",
    "from apache_beam.io import ReadFromText, WriteToText\n",
    "\n",
    "regionsfilename = '/class/datasets/northwind/CSV/regions/regions.csv'\n",
    "with beam.Pipeline() as p:\n",
    "    regions = (\n",
    "        p | 'Read' >> ReadFromText(regionsfilename)\n",
    "          | 'Parse' >> beam.Map(lambda x : x.split(','))\n",
    "          | 'Transform' >> beam.Map(lambda x : (int(x[0]), x[1].upper()))\n",
    "          | 'Write' >> WriteToText('/tmp/outputs')\n",
    "#          | 'Print' >> beam.Map(print)\n",
    "    )\n",
    "    #p.run() # implicit in Python when using with block\n",
    "\n",
    "! cat /tmp/outputs*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee88479c-60c3-48c3-b8c1-3e8113f0d39a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Read from CSV and use <font color='green' size=\"+2\">ParDo</font>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94887a8a-f945-414f-bfaa-2e72fa1aa746",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam.io import ReadFromText, WriteToText\n",
    "\n",
    "class RegionParseTuple(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        regionid, regionname = element.split(',')\n",
    "        #return [(int(regionid), regionname)] # ParDo's need to return a list\n",
    "        yield (int(regionid), regionname) # Can also use yield instead of returning a list\n",
    "#        yield (int(regionid), regionname.upper()) # Include a transformation instead of doing it as a separate step\n",
    "\n",
    "regionsfilename = 'datasets/northwind/CSV/regions/regions.csv'\n",
    "with beam.Pipeline() as p:\n",
    "    regions = (\n",
    "        p | 'Read' >> ReadFromText(regionsfilename)\n",
    "          | 'Parse' >> beam.ParDo(RegionParseTuple())\n",
    "          #| 'Write' >> WriteToText('regions.out')\n",
    "          | 'Print' >> beam.Map(print)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f3fd79-61c1-4d9f-acf1-5ee7af3ff832",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <img src=\"images/java.png\" width=40 height=40 /><font color='indigo' size=\"+2\">Java</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d05cc4-e81e-44d7-adad-d38ee19848c7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Read from CSV and use <font color='blue' face=\"Fixedsys, monospace\" size=\"+2\">Map</font> with <font color='green' >lambda</font>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28492eea-c43a-43ab-aa7f-ceca3f199a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%java\n",
    "package samples.quickstart;\n",
    "\n",
    "import org.apache.beam.sdk.Pipeline;\n",
    "import org.apache.beam.sdk.values.PCollection;\n",
    "import org.apache.beam.sdk.values.TypeDescriptors;\n",
    "import org.apache.beam.sdk.transforms.MapElements;\n",
    "import org.apache.beam.sdk.io.TextIO;\n",
    "\n",
    "public class ReadRegions1 {\n",
    "    public static void main(String[] args) {\n",
    "        Pipeline p = Pipeline.create();\n",
    "\n",
    "        String regionsInputFileName = \"datasets/northwind/CSV/regions/regions.csv\";\n",
    "        String outputsPrefix = \"/tmp/outputs\";\n",
    "\n",
    "        PCollection<String> regions = p\n",
    "            .apply(\"Read\", TextIO.read().from(regionsInputFileName))\n",
    "            .apply(\"Parse\", MapElements.into(TypeDescriptors.strings()).via((String element) -> element.toUpperCase()));\n",
    "        \n",
    "        regions.apply(TextIO.write().to(outputsPrefix));\n",
    "        p.run().waitUntilFinish();\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fbd1da-0fed-43c9-8ab1-e1b73cc9534f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### <font color='green' size=\"+2\">ParDo</font> Example using anonymous class inline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14327a97-f8cd-489b-87eb-82ad962f701e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%java\n",
    "package samples.quickstart;\n",
    "\n",
    "import org.apache.beam.sdk.Pipeline;\n",
    "import org.apache.beam.sdk.values.PCollection;\n",
    "import org.apache.beam.sdk.values.TypeDescriptors;\n",
    "import org.apache.beam.sdk.io.TextIO;\n",
    "import org.apache.beam.sdk.transforms.ParDo;\n",
    "import org.apache.beam.sdk.transforms.DoFn;\n",
    "import org.apache.beam.sdk.transforms.DoFn.ProcessContext;\n",
    "\n",
    "public class ReadRegions2 {\n",
    "    public static void main(String[] args) {\n",
    "        Pipeline p = Pipeline.create();\n",
    "\n",
    "        String regionsInputFileName = \"datasets/northwind/CSV/regions/regions.csv\";\n",
    "        String outputsPrefix = \"/tmp/outputs\";\n",
    "\n",
    "\n",
    "        PCollection<String> regions = p\n",
    "            .apply(\"Read\", TextIO.read().from(regionsInputFileName))\n",
    "            .apply(\"Parse\", ParDo.of(new DoFn<String, String>() {\n",
    "                @ProcessElement\n",
    "                public void process(ProcessContext c) {\n",
    "                    String element = c.element();\n",
    "                    // String[] elements = element.split(\",\");\n",
    "                    c.output(element + \"*\");\n",
    "                }\n",
    "            }));\n",
    "        \n",
    "        regions.apply(TextIO.write().to(outputsPrefix));\n",
    "        p.run().waitUntilFinish();\n",
    "    }\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffc4515-7a9b-40f9-99cf-c456065c3e11",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### <font color='green' size=\"+2\">ParDo</font> using a defined class instead of an anonynous class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8543a5-9d2e-4a6e-81f4-5ae59aedcf89",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%java\n",
    "package samples.quickstart;\n",
    "\n",
    "import org.apache.beam.sdk.Pipeline;\n",
    "import org.apache.beam.sdk.values.PCollection;\n",
    "import org.apache.beam.sdk.values.TypeDescriptors;\n",
    "import org.apache.beam.sdk.io.TextIO;\n",
    "import org.apache.beam.sdk.transforms.ParDo;\n",
    "import org.apache.beam.sdk.transforms.DoFn;\n",
    "import org.apache.beam.sdk.transforms.DoFn.ProcessContext;\n",
    "\n",
    "public class ReadRegions3 {\n",
    "    public static void main(String[] args) {\n",
    "        Pipeline p = Pipeline.create();\n",
    "\n",
    "        String regionsInputFileName = \"datasets/northwind/CSV/regions/regions.csv\";\n",
    "        String outputsPrefix = \"/tmp/outputs\";\n",
    "\n",
    "\n",
    "        PCollection<String> regions = p\n",
    "            .apply(\"Read\", TextIO.read().from(regionsInputFileName))\n",
    "            .apply(\"Parse\", ParDo.of(new AddStar()));\n",
    "        \n",
    "        regions.apply(TextIO.write().to(outputsPrefix));\n",
    "        p.run().waitUntilFinish();\n",
    "    }\n",
    "    \n",
    "    static class AddStar extends DoFn<String, String> {\n",
    "        @ProcessElement\n",
    "        public void process(@Element String line, OutputReceiver<String> out) {\n",
    "            out.output(line + \"*\");\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5ce401-ba9d-4fd8-a21c-670049b2a716",
   "metadata": {},
   "source": [
    "# __ __ __ __ __ __ __ __ __ __ __ __"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71cae856-d38b-4ce9-a4c1-511284e1f9bc",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2. Parse into a model class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f4b34a-2b4a-4f8f-aa9c-af369f7ef9bb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <img src=\"images/python.png\" width=40 height=40 /><font color='cadetblue' size=\"+2\">Python</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebebb6b4-3c55-4b79-a0d8-4d3c017162c7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Create a model based on <font color='blue' face=\"Fixedsys, monospace\" size=\"+2\">typing.NamedTuple</font> so you can use properties instead of keys for <font color='green' size=\"+2\">dict</font> or position for <font color='green' size=\"+2\">tuple</font> and use the <font color='blue' face=\"Fixedsys, monospace\" size=\"+2\">Filter</font> <font color='green' size=\"+2\">PTransform</font> with <font color='blue' face=\"Fixedsys, monospace\" size=\"+2\">lambda</font>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0dd7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam.io import ReadFromText, WriteToText\n",
    "import typing\n",
    "\n",
    "class Territory(typing.NamedTuple):\n",
    "    territoryid: int\n",
    "    territoryname: str\n",
    "    regionid: int\n",
    "beam.coders.registry.register_coder(Territory, beam.coders.RowCoder)\n",
    "        \n",
    "class TerritoryParseClass(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        territoryid, territoryname, regionid = element.split(',')\n",
    "        yield Territory(int(territoryid), territoryname, int(regionid))\n",
    "\n",
    "territoriesfilename = 'datasets/northwind/CSV/territories/territories.csv'\n",
    "with beam.Pipeline() as p:\n",
    "    regions = (\n",
    "        p | 'Read' >> ReadFromText(territoriesfilename)\n",
    "          | 'Parse' >> beam.ParDo(TerritoryParseClass())\n",
    "          | 'Filter 1' >> beam.Filter(lambda x : x.regionid % 2 == 0)\n",
    "          | 'Filter 2' >> beam.Filter(lambda x : x.territoryname.startswith('S'))\n",
    "          | 'Print' >> beam.Map(print)\n",
    "#          | 'Write' >> WriteToText('regions.out')\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5230451d-cfb3-450f-82b6-e5dd24e6e2e4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Use <font color='blue' face=\"Fixedsys, monospace\" size=\"+2\">Filter</font> with a UDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07089b39-7ccf-42cf-830b-3189ab269566",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam.io import ReadFromText, WriteToText\n",
    "import typing\n",
    "\n",
    "class Territory(typing.NamedTuple):\n",
    "    territoryid: int\n",
    "    territoryname: str\n",
    "    regionid: int\n",
    "beam.coders.registry.register_coder(Territory, beam.coders.RowCoder)\n",
    "        \n",
    "class TerritoryParseClass(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        territoryid, territoryname, regionid = element.split(',')\n",
    "        yield Territory(int(territoryid), territoryname, int(regionid))\n",
    "\n",
    "def startsWithS(element):\n",
    "    return element.territoryname.startswith('S')\n",
    "\n",
    "territoriesfilename = 'datasets/northwind/CSV/territories/territories.csv'\n",
    "with beam.Pipeline() as p:\n",
    "    regions = (\n",
    "        p | 'Read' >> ReadFromText(territoriesfilename)\n",
    "          | 'Parse' >> beam.ParDo(TerritoryParseClass())\n",
    "          | 'Filter' >> beam.Filter(startsWithS)\n",
    "          | 'Print' >> beam.Map(print)\n",
    "#          | 'Write' >> WriteToText('regions.out')\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ac3ac5-d61b-496d-bf95-2801b014b28e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Use a <font color='green' size=\"+2\">ParDo</font> class to accomplish filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54691011-9bfd-49d9-8992-ff1333c65824",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam.io import ReadFromText, WriteToText\n",
    "import typing\n",
    "\n",
    "class Territory(typing.NamedTuple):\n",
    "    territoryid: int\n",
    "    territoryname: str\n",
    "    regionid: int\n",
    "beam.coders.registry.register_coder(Territory, beam.coders.RowCoder)\n",
    "        \n",
    "class TerritoryParseClass(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        territoryid, territoryname, regionid = element.split(',')\n",
    "        yield Territory(int(territoryid), territoryname, int(regionid))\n",
    "\n",
    "class StartsWithSFilter(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        if element.territoryname.startswith('S'):\n",
    "            yield element\n",
    "            \n",
    "territoriesfilename = 'datasets/northwind/CSV/territories/territories.csv'\n",
    "with beam.Pipeline() as p:\n",
    "    regions = (\n",
    "        p | 'Read' >> ReadFromText(territoriesfilename)\n",
    "          | 'Parse' >> beam.ParDo(TerritoryParseClass())\n",
    "          | 'Filter' >> beam.ParDo(StartsWithSFilter())\n",
    "          | 'Print' >> beam.Map(print)\n",
    "#          | 'Write' >> WriteToText('regions.out')\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39807dbe-b99a-4767-be21-16b38ff287f8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Put the parsing and filtering all into one <font color='green' size=\"+2\">ParDo</font>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2091357-5550-42fb-8810-3e5fc8d2cb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam.io import ReadFromText, WriteToText\n",
    "import typing\n",
    "\n",
    "class Territory(typing.NamedTuple):\n",
    "    territoryid: int\n",
    "    territoryname: str\n",
    "    regionid: int\n",
    "beam.coders.registry.register_coder(Territory, beam.coders.RowCoder)\n",
    "        \n",
    "class TerritoryParseClass(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        territoryid, territoryname, regionid = element.split(',')\n",
    "        if territoryname.startswith('S'):\n",
    "            yield Territory(int(territoryid), territoryname, int(regionid))\n",
    "\n",
    "territoriesfilename = 'datasets/northwind/CSV/territories/territories.csv'\n",
    "with beam.Pipeline() as p:\n",
    "    regions = (\n",
    "        p | 'Read' >> ReadFromText(territoriesfilename)\n",
    "          | 'Parse' >> beam.ParDo(TerritoryParseClass())\n",
    "          | 'Print' >> beam.Map(print)\n",
    "#          | 'Write' >> WriteToText('regions.out')\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7777973-1216-4e12-995f-5c5a8247b7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with beam.Pipeline() as p:\n",
    "  # records = (p | 'Read' >> beam.io.ReadFromAvro('gs://joey-shared-bucket/datasets/northwind/AVRO/categories/categories.avro')\n",
    "  records = (p | 'Read' >> beam.io.ReadFromText('gs://joey-shared-bucket/datasets/northwind/CSV/categories/categories.csv')\n",
    "             | beam.Map(print))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56bb0592-e4b0-4204-b4f0-775e75d8f02c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <img src=\"images/java.png\" width=40 height=40 /><font color='indigo' size=\"+2\">Java</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e66252-cdcd-411c-866e-44857a6b4379",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Parse a CSV into a class and filter it using a <font color='green' size=\"+2\">Pardo</font>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e1e99c-eece-43f4-9b8e-2ba9df28d888",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%java\n",
    "package samples.quickstart;\n",
    "\n",
    "import org.apache.beam.sdk.Pipeline;\n",
    "import org.apache.beam.sdk.values.PCollection;\n",
    "import org.apache.beam.sdk.values.TypeDescriptors;\n",
    "import org.apache.beam.sdk.io.TextIO;\n",
    "import org.apache.beam.sdk.transforms.ParDo;\n",
    "import org.apache.beam.sdk.transforms.DoFn;\n",
    "import org.apache.beam.sdk.transforms.Filter;\n",
    "import org.apache.beam.sdk.transforms.DoFn.ProcessContext;\n",
    "import org.apache.beam.sdk.coders.DefaultCoder;\n",
    "import org.apache.beam.sdk.coders.AvroCoder;\n",
    "import org.apache.beam.sdk.transforms.SerializableFunction;\n",
    "\n",
    "import org.slf4j.Logger;\n",
    "import org.slf4j.LoggerFactory;\n",
    "\n",
    "public class ReadTerritories {\n",
    "    public static void main(String[] args) {\n",
    "        Pipeline p = Pipeline.create();\n",
    "\n",
    "        String territoriesInputFileName = \"datasets/northwind/CSV/territories/territories.csv\";\n",
    "        String outputsPrefix = \"/tmp/outputs\";\n",
    "\n",
    "        PCollection<Territory> territories = p\n",
    "            .apply(\"Read\", TextIO.read().from(territoriesInputFileName))\n",
    "            .apply(\"Parse\", ParDo.of(new ParseTerritories()))\n",
    "            .apply(\"Filter\", ParDo.of(new FilterTerritories()))\n",
    "        ;                   \n",
    "        \n",
    "        territories.apply(TextIO.<Territory>writeCustomType().to(outputsPrefix).withFormatFunction(new SerializeTerritory()));\n",
    "\n",
    "        p.run().waitUntilFinish();\n",
    "    }\n",
    "    \n",
    "    @DefaultCoder(AvroCoder.class)\n",
    "    static class Territory {\n",
    "        Long territoryID;\n",
    "        String territoryName;\n",
    "        Long regionID;\n",
    "        \n",
    "        Territory() {}\n",
    "        \n",
    "        Territory(long territoryID, String territoryName, long regionID) {\n",
    "            this.territoryID = territoryID;\n",
    "            this.territoryName = territoryName;\n",
    "            this.regionID = regionID;\n",
    "        }\n",
    "        \n",
    "        @Override\n",
    "        public String toString() {\n",
    "            return String.format(\"(territoryID = %d, territoryName = %s, regionID = %d)\", territoryID, territoryName, regionID);\n",
    "        }\n",
    "\n",
    "    }\n",
    "    \n",
    "    static class SerializeTerritory implements SerializableFunction<Territory, String> {\n",
    "        @Override\n",
    "        public String apply(Territory input) {\n",
    "          return input.toString();\n",
    "        }\n",
    "    }\n",
    "\n",
    "    static class ParseTerritories extends DoFn<String, Territory> {\n",
    "        private static final Logger LOG = LoggerFactory.getLogger(ParseTerritories.class);\n",
    "\n",
    "        @ProcessElement\n",
    "        public void process(ProcessContext c) {\n",
    "            String[] columns = c.element().split(\",\");\n",
    "            try {\n",
    "                Long territoryID = Long.parseLong(columns[0].trim());\n",
    "                String territoryName = columns[1].trim();\n",
    "                Long regionID = Long.parseLong(columns[2].trim());\n",
    "                c.output(new Territory(territoryID, territoryName, regionID));\n",
    "            } catch (ArrayIndexOutOfBoundsException | NumberFormatException e) {\n",
    "                LOG.info(\"ParseTerritories: parse error on '\" + c.element() + \"': \" + e.getMessage());\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    static class FilterTerritories extends DoFn<Territory, Territory> {\n",
    "        private static final Logger LOG = LoggerFactory.getLogger(FilterTerritories.class);\n",
    "\n",
    "        @ProcessElement\n",
    "        public void process(@Element Territory t, OutputReceiver<Territory> o) {\n",
    "            if (t.territoryID % 2 == 0 && t.territoryName.startsWith(\"S\")) {\n",
    "                o.output(t);\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7378c125-c36c-4ac0-9443-35f54539258a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Parse a CSV into a class and filter it using and anonymous class to create the condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7292cee7-d525-44b6-a9db-36b5f3acf839",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%java\n",
    "package samples.quickstart;\n",
    "\n",
    "import org.apache.beam.sdk.Pipeline;\n",
    "import org.apache.beam.sdk.values.PCollection;\n",
    "import org.apache.beam.sdk.values.TypeDescriptors;\n",
    "import org.apache.beam.sdk.io.TextIO;\n",
    "import org.apache.beam.sdk.transforms.ParDo;\n",
    "import org.apache.beam.sdk.transforms.DoFn;\n",
    "import org.apache.beam.sdk.transforms.Filter;\n",
    "import org.apache.beam.sdk.transforms.DoFn.ProcessContext;\n",
    "import org.apache.beam.sdk.coders.DefaultCoder;\n",
    "import org.apache.beam.sdk.coders.AvroCoder;\n",
    "import org.apache.beam.sdk.transforms.SerializableFunction;\n",
    "\n",
    "import org.slf4j.Logger;\n",
    "import org.slf4j.LoggerFactory;\n",
    "\n",
    "public class ReadTerritories {\n",
    "    public static void main(String[] args) {\n",
    "        Pipeline p = Pipeline.create();\n",
    "\n",
    "        String territoriesInputFileName = \"datasets/northwind/CSV/territories/territories.csv\";\n",
    "        String outputsPrefix = \"/tmp/outputs\";\n",
    "\n",
    "        PCollection<Territory> territories = p\n",
    "            .apply(\"Read\", TextIO.read().from(territoriesInputFileName))\n",
    "            .apply(\"Parse\", ParDo.of(new ParseTerritories()))\n",
    "            .apply(\"Filter\", Filter.by(new SerializableFunction<Territory, Boolean>() {\n",
    "                @Override\n",
    "                public Boolean apply(Territory t) {\n",
    "                    return t.territoryID % 2 == 0 && t.territoryName.startsWith(\"S\");\n",
    "                }\n",
    "            }))\n",
    "        ;                   \n",
    "        \n",
    "        territories.apply(TextIO.<Territory>writeCustomType().to(outputsPrefix).withFormatFunction(new SerializeTerritory()));\n",
    "        p.run().waitUntilFinish();\n",
    "    }\n",
    "    \n",
    "    @DefaultCoder(AvroCoder.class)\n",
    "    static class Territory {\n",
    "        Long territoryID;\n",
    "        String territoryName;\n",
    "        Long regionID;\n",
    "        \n",
    "        Territory() {}\n",
    "        \n",
    "        Territory(long territoryID, String territoryName, long regionID) {\n",
    "            this.territoryID = territoryID;\n",
    "            this.territoryName = territoryName;\n",
    "            this.regionID = regionID;\n",
    "        }\n",
    "        \n",
    "        @Override\n",
    "        public String toString() {\n",
    "            return String.format(\"(territoryID = %d, territoryName = %s, regionID = %d)\", territoryID, territoryName, regionID);\n",
    "        }\n",
    "\n",
    "    }\n",
    "    \n",
    "    static class SerializeTerritory implements SerializableFunction<Territory, String> {\n",
    "        @Override\n",
    "        public String apply(Territory input) {\n",
    "          return input.toString();\n",
    "        }\n",
    "    }\n",
    "\n",
    "    static class ParseTerritories extends DoFn<String, Territory> {\n",
    "        private static final Logger LOG = LoggerFactory.getLogger(ParseTerritories.class);\n",
    "\n",
    "        @ProcessElement\n",
    "        public void process(ProcessContext c) {\n",
    "            String[] columns = c.element().split(\",\");\n",
    "            try {\n",
    "                Long territoryID = Long.parseLong(columns[0].trim());\n",
    "                String territoryName = columns[1].trim();\n",
    "                Long regionID = Long.parseLong(columns[2].trim());\n",
    "                c.output(new Territory(territoryID, territoryName, regionID));\n",
    "            } catch (ArrayIndexOutOfBoundsException | NumberFormatException e) {\n",
    "                LOG.info(\"ParseTerritories: parse error on '\" + c.element() + \"': \" + e.getMessage());\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    static class FilterTerritories extends DoFn<Territory, Territory> {\n",
    "        private static final Logger LOG = LoggerFactory.getLogger(FilterTerritories.class);\n",
    "\n",
    "        @ProcessElement\n",
    "        public void process(@Element Territory t, OutputReceiver<Territory> o) {\n",
    "            if (t.territoryID % 2 == 0 && t.territoryName.startsWith(\"S\")) {\n",
    "                o.output(t);\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c2fc88-b027-42bf-9136-c02d6583a04c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Parse a CSV into a class and filter it in one step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9339e563-880b-4fae-9cc6-3e8ba50d85d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%java\n",
    "package samples.quickstart;\n",
    "\n",
    "import org.apache.beam.sdk.Pipeline;\n",
    "import org.apache.beam.sdk.values.PCollection;\n",
    "import org.apache.beam.sdk.values.TypeDescriptors;\n",
    "import org.apache.beam.sdk.io.TextIO;\n",
    "import org.apache.beam.sdk.transforms.ParDo;\n",
    "import org.apache.beam.sdk.transforms.DoFn;\n",
    "import org.apache.beam.sdk.transforms.Filter;\n",
    "import org.apache.beam.sdk.transforms.DoFn.ProcessContext;\n",
    "import org.apache.beam.sdk.coders.DefaultCoder;\n",
    "import org.apache.beam.sdk.coders.AvroCoder;\n",
    "import org.apache.beam.sdk.transforms.SerializableFunction;\n",
    "\n",
    "import org.slf4j.Logger;\n",
    "import org.slf4j.LoggerFactory;\n",
    "\n",
    "public class ReadTerritories {\n",
    "    public static void main(String[] args) {\n",
    "        Pipeline p = Pipeline.create();\n",
    "\n",
    "        String territoriesInputFileName = \"datasets/northwind/CSV/territories/territories.csv\";\n",
    "        String outputsPrefix = \"/tmp/outputs\";\n",
    "\n",
    "        PCollection<Territory> territories = p\n",
    "            .apply(\"Read\", TextIO.read().from(territoriesInputFileName))\n",
    "            .apply(\"Parse\", ParDo.of(new ParseTerritories()))\n",
    "        ;                   \n",
    "        \n",
    "        territories.apply(TextIO.<Territory>writeCustomType().to(outputsPrefix).withFormatFunction(new SerializeTerritory()));\n",
    "        p.run().waitUntilFinish();\n",
    "    }\n",
    "    \n",
    "    @DefaultCoder(AvroCoder.class)\n",
    "    static class Territory {\n",
    "        Long territoryID;\n",
    "        String territoryName;\n",
    "        Long regionID;\n",
    "        \n",
    "        Territory() {}\n",
    "        \n",
    "        Territory(long territoryID, String territoryName, long regionID) {\n",
    "            this.territoryID = territoryID;\n",
    "            this.territoryName = territoryName;\n",
    "            this.regionID = regionID;\n",
    "        }\n",
    "        \n",
    "        @Override\n",
    "        public String toString() {\n",
    "            return String.format(\"(territoryID = %d, territoryName = %s, regionID = %d)\", territoryID, territoryName, regionID);\n",
    "        }\n",
    "\n",
    "    }\n",
    "    \n",
    "    static class SerializeTerritory implements SerializableFunction<Territory, String> {\n",
    "        @Override\n",
    "        public String apply(Territory input) {\n",
    "          return input.toString();\n",
    "        }\n",
    "    }\n",
    "\n",
    "    static class ParseTerritories extends DoFn<String, Territory> {\n",
    "        private static final Logger LOG = LoggerFactory.getLogger(ParseTerritories.class);\n",
    "\n",
    "        @ProcessElement\n",
    "        public void process(ProcessContext c) {\n",
    "            String[] columns = c.element().split(\",\");\n",
    "            try {\n",
    "                Long territoryID = Long.parseLong(columns[0].trim());\n",
    "                String territoryName = columns[1].trim();\n",
    "                Long regionID = Long.parseLong(columns[2].trim());\n",
    "                if (territoryName.startsWith(\"S\")) {\n",
    "                    c.output(new Territory(territoryID, territoryName, regionID));\n",
    "                }\n",
    "            } catch (ArrayIndexOutOfBoundsException | NumberFormatException e) {\n",
    "                LOG.info(\"ParseTerritories: parse error on '\" + c.element() + \"': \" + e.getMessage());\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    static class FilterTerritories extends DoFn<Territory, Territory> {\n",
    "        private static final Logger LOG = LoggerFactory.getLogger(FilterTerritories.class);\n",
    "\n",
    "        @ProcessElement\n",
    "        public void process(@Element Territory t, OutputReceiver<Territory> o) {\n",
    "            if (t.territoryID % 2 == 0 && t.territoryName.startsWith(\"S\")) {\n",
    "                o.output(t);\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362f1f94-6b7b-40d4-8284-77bcb9de04d1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### There are special methods like <font color='blue' face=\"Fixedsys, monospace\" size=\"+2\">whereFieldName</font> but they don't do anything differently than just using a regular <font color='green' size=\"+2\">ParDo</font>. This code doesn't actually run, but shows what it would look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d7320c-e979-4bde-a318-1cc4d1064268",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%java verbose\n",
    "package samples.quickstart;\n",
    "\n",
    "import org.apache.beam.sdk.Pipeline;\n",
    "import org.apache.beam.sdk.values.PCollection;\n",
    "import org.apache.beam.sdk.values.TypeDescriptors;\n",
    "import org.apache.beam.sdk.io.TextIO;\n",
    "import org.apache.beam.sdk.transforms.ParDo;\n",
    "import org.apache.beam.sdk.transforms.DoFn;\n",
    "//import org.apache.beam.sdk.transforms.Filter;\n",
    "import org.apache.beam.sdk.schemas.transforms.Filter;\n",
    "import org.apache.beam.sdk.transforms.DoFn.ProcessContext;\n",
    "import org.apache.beam.sdk.coders.DefaultCoder;\n",
    "import org.apache.beam.sdk.coders.AvroCoder;\n",
    "import org.apache.beam.sdk.transforms.SerializableFunction;\n",
    "import org.apache.beam.sdk.values.Row;\n",
    "\n",
    "import org.slf4j.Logger;\n",
    "import org.slf4j.LoggerFactory;\n",
    "\n",
    "public class ReadTerritories {\n",
    "    public static void main(String[] args) {\n",
    "        Pipeline p = Pipeline.create();\n",
    "\n",
    "        String territoriesInputFileName = \"datasets/northwind/CSV/territories/territories.csv\";\n",
    "        String outputsPrefix = \"/tmp/outputs\";\n",
    "\n",
    "        PCollection<Territory> territories = p\n",
    "            .apply(\"Read\", TextIO.read().from(territoriesInputFileName))\n",
    "            .apply(\"Parse\", ParDo.of(new ParseTerritories()))\n",
    "            .apply(\"Filter\", Filter.<Territory>create().whereFieldName(\"regionID\", (Long regionID) -> regionID == 1))\n",
    "        ;                   \n",
    "        \n",
    "        territories.apply(TextIO.<Territory>writeCustomType().to(outputsPrefix).withFormatFunction(new SerializeTerritory()));\n",
    "        p.run().waitUntilFinish();\n",
    "    }\n",
    "    \n",
    "    @DefaultCoder(AvroCoder.class)\n",
    "    static class Territory {\n",
    "        Long territoryID;\n",
    "        String territoryName;\n",
    "        Long regionID;\n",
    "        \n",
    "        Territory() {}\n",
    "        \n",
    "        Territory(long territoryID, String territoryName, long regionID) {\n",
    "            this.territoryID = territoryID;\n",
    "            this.territoryName = territoryName;\n",
    "            this.regionID = regionID;\n",
    "        }\n",
    "        \n",
    "        @Override\n",
    "        public String toString() {\n",
    "            return String.format(\"(territoryID = %d, territoryName = %s, regionID = %d)\", territoryID, territoryName, regionID);\n",
    "        }\n",
    "\n",
    "    }\n",
    "    \n",
    "    static class SerializeTerritory implements SerializableFunction<Territory, String> {\n",
    "        @Override\n",
    "        public String apply(Territory input) {\n",
    "          return input.toString();\n",
    "        }\n",
    "    }\n",
    "\n",
    "    static class ParseTerritories extends DoFn<String, Territory> {\n",
    "        private static final Logger LOG = LoggerFactory.getLogger(ParseTerritories.class);\n",
    "\n",
    "        @ProcessElement\n",
    "        public void process(ProcessContext c) {\n",
    "            String[] columns = c.element().split(\",\");\n",
    "            try {\n",
    "                Long territoryID = Long.parseLong(columns[0].trim());\n",
    "                String territoryName = columns[1].trim();\n",
    "                Long regionID = Long.parseLong(columns[2].trim());\n",
    "                c.output(new Territory(territoryID, territoryName, regionID));\n",
    "            } catch (ArrayIndexOutOfBoundsException | NumberFormatException e) {\n",
    "                LOG.info(\"ParseTerritories: parse error on '\" + c.element() + \"': \" + e.getMessage());\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505552a2-9b45-48b2-a0b1-5165573d33bf",
   "metadata": {},
   "source": [
    "# __ __ __ __ __ __ __ __ __ __ __ __"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e5c5a3-327e-4279-af35-bcf57e7ab9f1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 3. Create multiple outputs from a single read."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10bc9883-ba3b-4b67-ad1c-05b281ef9474",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <img src=\"images/python.png\" width=40 height=40 /><font color='cadetblue' size=\"+2\">Python</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93edcd4b-aac7-412a-a728-d772fd625f56",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Send the same data down multiple paths, such as to group it on two different keys with one read from the source. Also show how to read AVRO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfb9728",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam import pvalue\n",
    "from apache_beam.io import ReadFromText, WriteToText\n",
    "import typing\n",
    "\n",
    "class Territory(typing.NamedTuple):\n",
    "    territoryid: int\n",
    "    territoryname: str\n",
    "    regionid: int\n",
    "beam.coders.registry.register_coder(Territory, beam.coders.RowCoder)\n",
    "        \n",
    "class TerritoryParseClass(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        yield Territory(int(element['territoryid']), element['territorydescription'], int(element['regionid']))\n",
    "\n",
    "territoriesfilename = 'datasets/northwind/AVRO/territories/territories.avro'\n",
    "with beam.Pipeline() as p:\n",
    "    territories = (p | 'Read' >> beam.io.ReadFromAvro(territoriesfilename)\n",
    "                     | 'Parse' >> beam.ParDo(TerritoryParseClass())\n",
    "                  )\n",
    "\n",
    "    # Branch 1\n",
    "    (territories \n",
    "         | 'Lowercase' >> beam.Map(lambda x : (x.territoryid, x.territoryname.lower(), x.regionid))\n",
    "         | 'Write Lower' >> WriteToText('/tmp/territories_lower.out')\n",
    "    )\n",
    "    \n",
    "    # Branch 2\n",
    "    (territories \n",
    "         | 'Uppercase' >> beam.Map(lambda x : (x.territoryid, x.territoryname.upper(), x.regionid))\n",
    "         | 'Write Upper' >> WriteToText('/tmp/territories_upper.out')\n",
    "    )\n",
    "\n",
    "! echo \"Lower\" && cat /tmp/territories_lower.out* && echo \"Upper\" && cat /tmp/territories_upper.out*\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b7374d-4d8b-4923-81b6-4c32e9e13c99",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Branching uses <font color='blue' face=\"Fixedsys, monospace\" size=\"+2\">TaggedOutput</font> in the <font color='green' size=\"+2\">ParDo</font> to split data into two different paths with different data on each. Also show how to read Parquet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7649fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam import pvalue\n",
    "from apache_beam.io import ReadFromText, WriteToText\n",
    "import typing\n",
    "\n",
    "class Territory(typing.NamedTuple):\n",
    "    territoryid: int\n",
    "    territoryname: str\n",
    "    regionid: int\n",
    "beam.coders.registry.register_coder(Territory, beam.coders.RowCoder)\n",
    "        \n",
    "class OddEvenTerritoryParseClass(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        territoryid, territoryname, regionid = int(element['territoryid']), element['territoryname'], int(element['regionid'])\n",
    "        if int(regionid) % 2 == 0:\n",
    "            yield pvalue.TaggedOutput('Even', Territory(int(territoryid), territoryname, int(regionid)))\n",
    "        else:\n",
    "            yield pvalue.TaggedOutput('Odd', Territory(int(territoryid), territoryname, int(regionid)))\n",
    "\n",
    "territoriesfilename = 'datasets/northwind/PARQUET/territories/territories.parquet'\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "    territories = p | 'Read' >> beam.io.ReadFromParquet(territoriesfilename) \n",
    "    # territories would return a tuple of the two tagged outputs\n",
    "    # unpack the two outputs to two separate variables to process differently\n",
    "    evens, odds = territories | 'Parse' >> beam.ParDo(OddEvenTerritoryParseClass()).with_outputs(\"Even\", \"Odd\")\n",
    "    \n",
    "    evens | 'Write Even' >> WriteToText('/tmp/territories_even.out')\n",
    "    \n",
    "    odds | 'Write Odd' >> WriteToText('/tmp/territories_odd.out')\n",
    "\n",
    "! echo \"Evens\" && cat /tmp/territories_even.out* && echo \"Odds\" && cat /tmp/territories_odd.out*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5a3ae2-0add-4eaf-8515-d9420834fa34",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <img src=\"images/java.png\" width=40 height=40 /><font color='indigo' size=\"+2\">Java</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491ce4a7-eacc-41dd-b51b-1746546fcbcd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Send the same output down two different paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f43ccc6-3890-4349-8482-c7c2498e9258",
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm /tmp/territories*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9847873-48ba-4b10-9e17-9bfe3004ed33",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%java nooutput\n",
    "package samples.quickstart;\n",
    "\n",
    "import org.apache.beam.sdk.Pipeline;\n",
    "import org.apache.beam.sdk.values.PCollection;\n",
    "import org.apache.beam.sdk.values.TypeDescriptors;\n",
    "import org.apache.beam.sdk.io.TextIO;\n",
    "import org.apache.beam.sdk.transforms.ParDo;\n",
    "import org.apache.beam.sdk.transforms.DoFn;\n",
    "import org.apache.beam.sdk.transforms.Filter;\n",
    "import org.apache.beam.sdk.transforms.DoFn.ProcessContext;\n",
    "import org.apache.beam.sdk.coders.DefaultCoder;\n",
    "import org.apache.beam.sdk.coders.AvroCoder;\n",
    "import org.apache.beam.sdk.transforms.SerializableFunction;\n",
    "import org.apache.beam.sdk.values.TupleTag;\n",
    "import org.apache.beam.sdk.values.PCollectionTuple;\n",
    "import org.apache.beam.sdk.values.TupleTagList;\n",
    "\n",
    "import org.slf4j.Logger;\n",
    "import org.slf4j.LoggerFactory;\n",
    "\n",
    "public class ReadTerritories {\n",
    "\n",
    "    public static void main(String[] args) {\n",
    "        Pipeline p = Pipeline.create();\n",
    "\n",
    "        String territoriesInputFileName = \"datasets/northwind/CSV/territories/territories.csv\";\n",
    "        String outputsPrefix = \"/tmp/outputs\";\n",
    "\n",
    "        PCollection<Territory> territories = p\n",
    "            .apply(\"Read\", TextIO.read().from(territoriesInputFileName))\n",
    "            .apply(\"Parse Territory\", ParDo.of(new ParseTerritories()))\n",
    "        ;                   \n",
    "        \n",
    "            \n",
    "        territories\n",
    "            .apply(\"Upper\", ParDo.of(new DoFn<Territory, Territory>() {\n",
    "                @ProcessElement\n",
    "                public void process(ProcessContext c) {\n",
    "                    Territory t = c.element();\n",
    "                    c.output(new Territory(t.territoryID, t.territoryName.toUpperCase(), t.regionID));\n",
    "                }\n",
    "            }))\n",
    "             .apply(TextIO.<Territory>writeCustomType().to(\"/tmp/territories_upper\").withFormatFunction(new SerializeTerritory()));\n",
    "\n",
    "        territories\n",
    "            .apply(\"Lower\", ParDo.of(new DoFn<Territory, Territory>() {\n",
    "                @ProcessElement\n",
    "                public void process(ProcessContext c) {\n",
    "                    Territory t = c.element();\n",
    "                    c.output(new Territory(t.territoryID, t.territoryName.toLowerCase(), t.regionID));\n",
    "                }\n",
    "            }))\n",
    "             .apply(TextIO.<Territory>writeCustomType().to(\"/tmp/territories_lower\").withFormatFunction(new SerializeTerritory()));\n",
    "\n",
    "        \n",
    "        p.run().waitUntilFinish();\n",
    "    }\n",
    "    \n",
    "    @DefaultCoder(AvroCoder.class)\n",
    "    static class Territory {\n",
    "        Long territoryID;\n",
    "        String territoryName;\n",
    "        Long regionID;\n",
    "        \n",
    "        Territory() {}\n",
    "        \n",
    "        Territory(long territoryID, String territoryName, long regionID) {\n",
    "            this.territoryID = territoryID;\n",
    "            this.territoryName = territoryName;\n",
    "            this.regionID = regionID;\n",
    "        }\n",
    "        \n",
    "        @Override\n",
    "        public String toString() {\n",
    "            return String.format(\"(territoryID = %d, territoryName = %s, regionID = %d)\", territoryID, territoryName, regionID);\n",
    "        }\n",
    "\n",
    "    }\n",
    "    \n",
    "    static class SerializeTerritory implements SerializableFunction<Territory, String> {\n",
    "        @Override\n",
    "        public String apply(Territory input) {\n",
    "          return input.toString();\n",
    "        }\n",
    "    }\n",
    "\n",
    "    static class ParseTerritories extends DoFn<String, Territory> {\n",
    "        private static final Logger LOG = LoggerFactory.getLogger(ParseTerritories.class);\n",
    "\n",
    "        @ProcessElement\n",
    "        public void process(ProcessContext c) {\n",
    "            String[] columns = c.element().split(\",\");\n",
    "            try {\n",
    "                Long territoryID = Long.parseLong(columns[0].trim());\n",
    "                String territoryName = columns[1].trim();\n",
    "                Long regionID = Long.parseLong(columns[2].trim());\n",
    "                c.output(new Territory(territoryID, territoryName, regionID));\n",
    "            } catch (ArrayIndexOutOfBoundsException | NumberFormatException e) {\n",
    "                LOG.info(\"ParseTerritoriesOddEvenSplit: parse error on '\" + c.element() + \"': \" + e.getMessage());\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae170dd2-fdd2-46fc-b741-5e8423269d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "! echo \"Upper\" && cat /tmp/territories_upper* && echo \"Lower\" && cat /tmp/territories_lower*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992c0f98-4b4b-4253-8045-4bb46bebfa7e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Branching uses <font color='blue' face=\"Fixedsys, monospace\" size=\"+2\">TupleTag</font> to split the output into two separate paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25158a5b-169d-4672-9683-38732dbca1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm /tmp/territories*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72dde632-b934-4bf6-9a58-b551ab264385",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%java nooutput\n",
    "package samples.quickstart;\n",
    "\n",
    "import org.apache.beam.sdk.Pipeline;\n",
    "import org.apache.beam.sdk.values.PCollection;\n",
    "import org.apache.beam.sdk.values.TypeDescriptors;\n",
    "import org.apache.beam.sdk.io.TextIO;\n",
    "import org.apache.beam.sdk.transforms.ParDo;\n",
    "import org.apache.beam.sdk.transforms.DoFn;\n",
    "import org.apache.beam.sdk.transforms.Filter;\n",
    "import org.apache.beam.sdk.transforms.DoFn.ProcessContext;\n",
    "import org.apache.beam.sdk.coders.DefaultCoder;\n",
    "import org.apache.beam.sdk.coders.AvroCoder;\n",
    "import org.apache.beam.sdk.transforms.SerializableFunction;\n",
    "import org.apache.beam.sdk.values.TupleTag;\n",
    "import org.apache.beam.sdk.values.PCollectionTuple;\n",
    "import org.apache.beam.sdk.values.TupleTagList;\n",
    "\n",
    "import org.slf4j.Logger;\n",
    "import org.slf4j.LoggerFactory;\n",
    "\n",
    "public class ReadTerritories {\n",
    "\n",
    "    final static TupleTag<Territory> evenTag = new TupleTag<Territory>() {};\n",
    "    final static TupleTag<Territory> oddTag = new TupleTag<Territory>() {};\n",
    "\n",
    "    public static void main(String[] args) {\n",
    "        Pipeline p = Pipeline.create();\n",
    "\n",
    "        String territoriesInputFileName = \"territories.csv\";\n",
    "        String outputsPrefix = \"/tmp/outputs\";\n",
    "\n",
    "        PCollectionTuple territories = p\n",
    "            .apply(\"Read\", TextIO.read().from(territoriesInputFileName))\n",
    "            .apply(\"OddEvenSplit\", ParDo.of(new ParseTerritoriesOddEvenSplit()).withOutputTags(evenTag, TupleTagList.of(oddTag)))\n",
    "        ;                   \n",
    "        \n",
    "        PCollection<Territory> evenTerritories = territories.get(evenTag);\n",
    "        evenTerritories.apply(TextIO.<Territory>writeCustomType().to(outputsPrefix + \"_even\").withFormatFunction(new SerializeTerritory()));\n",
    "\n",
    "        PCollection<Territory> oddTerritories = territories.get(oddTag);\n",
    "        oddTerritories.apply(TextIO.<Territory>writeCustomType().to(outputsPrefix + \"_odd\").withFormatFunction(new SerializeTerritory()));\n",
    "        p.run().waitUntilFinish();\n",
    "    }\n",
    "    \n",
    "    @DefaultCoder(AvroCoder.class)\n",
    "    static class Territory {\n",
    "        Long territoryID;\n",
    "        String territoryName;\n",
    "        Long regionID;\n",
    "        \n",
    "        Territory() {}\n",
    "        \n",
    "        Territory(long territoryID, String territoryName, long regionID) {\n",
    "            this.territoryID = territoryID;\n",
    "            this.territoryName = territoryName;\n",
    "            this.regionID = regionID;\n",
    "        }\n",
    "        \n",
    "        @Override\n",
    "        public String toString() {\n",
    "            return String.format(\"(territoryID = %d, territoryName = %s, regionID = %d)\", territoryID, territoryName, regionID);\n",
    "        }\n",
    "\n",
    "    }\n",
    "    \n",
    "    static class SerializeTerritory implements SerializableFunction<Territory, String> {\n",
    "        @Override\n",
    "        public String apply(Territory input) {\n",
    "          return input.toString();\n",
    "        }\n",
    "    }\n",
    "\n",
    "    static class ParseTerritoriesOddEvenSplit extends DoFn<String, Territory> {\n",
    "        private static final Logger LOG = LoggerFactory.getLogger(ParseTerritoriesOddEvenSplit.class);\n",
    "\n",
    "        @ProcessElement\n",
    "        public void process(ProcessContext c) {\n",
    "\n",
    "\n",
    "            String[] columns = c.element().split(\",\");\n",
    "            try {\n",
    "                Long territoryID = Long.parseLong(columns[0].trim());\n",
    "                String territoryName = columns[1].trim();\n",
    "                Long regionID = Long.parseLong(columns[2].trim());\n",
    "                if (regionID % 2 == 0) {\n",
    "                    c.output(evenTag, new Territory(territoryID, territoryName, regionID));\n",
    "                } else {\n",
    "                    c.output(oddTag, new Territory(territoryID, territoryName, regionID));\n",
    "                }\n",
    "            } catch (ArrayIndexOutOfBoundsException | NumberFormatException e) {\n",
    "                LOG.info(\"ParseTerritoriesOddEvenSplit: parse error on '\" + c.element() + \"': \" + e.getMessage());\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6d6ae9-a392-4d7f-9222-017e3f2a8d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "! echo \"Odd\" && cat /tmp/outputs_odd* && echo \"Even\" && cat /tmp/outputs_even*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6574254-039b-49ee-b81d-c12aec4d740d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# __ __ __ __ __ __ __ __ __ __ __ __"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec19b5d-5bcf-4edb-ad94-124e7489604e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 4. Group and Join"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c69d69-d541-4d25-aae0-6a165c2f6531",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <img src=\"images/python.png\" width=40 height=40 /><font color='cadetblue' size=\"+2\">Python</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3040dd12-e0d2-4eeb-ba6f-ec490542e822",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### <font color='blue' face=\"Fixedsys, monospace\" size=\"+2\">WithKeys</font> will reshape your data first, then <font color='blue' face=\"Fixedsys, monospace\" size=\"+2\">GroupByKey</font> will cluster the elements as a list under each unique key. The data must be in a <font color='green' size=\"+2\">KV</font> tuple pair first. Also not how to read a JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf6f82f-8a76-49b2-9fc6-d5c8c9651d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam.io import ReadFromText\n",
    "import json\n",
    "import typing\n",
    "\n",
    "class Territory(typing.NamedTuple):\n",
    "    territoryid: int\n",
    "    territoryname: str\n",
    "    regionid: int\n",
    "beam.coders.registry.register_coder(Territory, beam.coders.RowCoder)\n",
    "        \n",
    "class TerritoryParseClass(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        territoryid, territoryname, regionid = int(element['territoryid']), element['territoryname'], int(element['regionid'])\n",
    "        yield Territory(int(territoryid), territoryname, int(regionid))\n",
    "\n",
    "territoriesfilename = 'datasets/northwind/JSON/territories/territories.json'\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "    territories = (\n",
    "                  p | 'Read Territories' >> ReadFromText(territoriesfilename)\n",
    "                    | 'From json' >> beam.Map(json.loads)\n",
    "                    | 'Parse Territories' >> beam.ParDo(TerritoryParseClass())\n",
    "                    | 'Territories With Keys' >> beam.util.WithKeys(lambda x : x.regionid)\n",
    "#                    | 'Group Territories' >> beam.GroupByKey() \n",
    "                    | 'Print Territories' >> beam.Map(print)\n",
    "                  )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4986d67f-7a28-4323-8943-cbab751ee1a6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### <font color='blue' face=\"Fixedsys, monospace\" size=\"+2\">Combine</font> is equivalent to a SQL <font color='blue' face=\"Fixedsys, monospace\" size=\"+2\">GROUP BY</font> query.\n",
    "### <font color='blue' face=\"Fixedsys, monospace\" size=\"+2\">SELECT key, sum(value) as total FROM source GROUP BY key.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c53775",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "    data = (\n",
    "        p | 'Create' >> beam.Create([('a', 10), ('a', 20), ('b', 30), ('b', 40), ('c', 50), ('a', 60)])\n",
    "          | 'Combine' >> beam.CombinePerKey(sum)\n",
    "          | 'Print' >> beam.Map(print)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e542b61e-9a43-4c33-b05b-b18ea5d0ac39",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Custom <font color='blue' face=\"Fixedsys, monospace\" size=\"+2\">CombineFn.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e60d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "\n",
    "class CustomCombine(beam.CombineFn):\n",
    "    \"\"\"\n",
    "    This custom combiner will calculate the max of the first element, sum of the second element and a count of total elements\n",
    "    The final step will also return the average of the second element.\n",
    "    \"\"\"\n",
    "    def create_accumulator(self):\n",
    "        # method defining how to create an empty accumulator\n",
    "        return dict()\n",
    "\n",
    "    def add_input(self, accumulator, input):\n",
    "        # get the input and split it up for easier manipulation\n",
    "        k, v = input\n",
    "        # get the values from the accumulator for the input key or initialize it if it's the first time we see this key\n",
    "        m, s, c = accumulator.get(k, (0, 0, 0))\n",
    "\n",
    "        # take the max for the first element of the tuple and sum the second element and count for the third\n",
    "        accumulator[k] = (v[0] if v[0] > m else m, s + v[1], c + 1)\n",
    "        return accumulator\n",
    "\n",
    "    def merge_accumulators(self, accumulators):\n",
    "        # merge the accumulators from the various workers once they have finished accumulating locally\n",
    "        merged = dict()\n",
    "        for accum in accumulators:\n",
    "          for k, v in accum.items():\n",
    "            m, s, c = merged.get(k, (0, 0, 0))\n",
    "            merged[k] = (v[0] if v[0] > m else m, s + v[1], c + v[2])\n",
    "        return merged\n",
    "\n",
    "    def extract_output(self, accumulator):\n",
    "        # called when all the works accumulators have been merge to render the final output\n",
    "        # return the max, the sum, the count and the average for the key\n",
    "        return {k : (v[0], v[1], v[2], v[1]/v[2]) for k, v in accumulator.items()}\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "    data = (\n",
    "        p | 'Create' >> beam.Create([('a', (1, 10)), ('a', (2, 20)), \n",
    "                                     ('b', (3, 30)), ('c', (5, 50)), \n",
    "                                     ('b', (4, 40)), ('a', (6, 60))])\n",
    "          | 'Combine' >> beam.CombineGlobally(CustomCombine())\n",
    "          | 'Print' >> beam.Map(print)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97109e52-fbf6-42eb-8266-fec461471367",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Create a nested repeating output.\n",
    "### First, create a dataset. Here is Python code for the equivalent bq command of <font color='blue' face=\"Fixedsys, monospace\" size=\"+2\">bq mk dataflow</font>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cfbe66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# same as doing bq mk dataflow\n",
    "\n",
    "from google.cloud import bigquery\n",
    "\n",
    "# Construct a BigQuery client object.\n",
    "client = bigquery.Client()\n",
    "\n",
    "# TODO(developer): Set dataset_id to the ID of the dataset to create.\n",
    "PROJECT_ID = 'qwiklabs-gcp-04-b1b7cded1c4b'\n",
    "dataset_id = f\"{PROJECT_ID}.dataflow\" #.format(client.project)\n",
    "\n",
    "# TODO(developer): Specify the geographic location where the dataset should reside.\n",
    "dataset.location = \"US\"\n",
    "\n",
    "# # Construct a full Dataset object to send to the API.\n",
    "# dataset = bigquery.Dataset(dataset_id)\n",
    "\n",
    "\n",
    "try:\n",
    "    client.get_dataset(dataset_id)  # Make an API request.\n",
    "    print(\"Dataset {} already exists\".format(dataset_id))\n",
    "except:\n",
    "    print(\"Dataset {} is not found\".format(dataset_id))\n",
    "    dataset = bigquery.Dataset(dataset_id)\n",
    "    dataset = client.create_dataset(dataset, timeout=30)  # Make an API request.\n",
    "    print(\"Created dataset {}.{}\".format(client.project, dataset.dataset_id))\n",
    "    \n",
    "    \n",
    "\n",
    "schema = [\n",
    "    bigquery.SchemaField(\"regionid\", \"INTEGER\", mode=\"REQUIRED\"),\n",
    "    bigquery.SchemaField(\"regionname\", \"STRING\", mode=\"REQUIRED\"),\n",
    "    bigquery.SchemaField(\"territories\", \"RECORD\", mode=\"REPEATED\", \n",
    "            fields=[\n",
    "                    bigquery.SchemaField(\"territoryid\", \"STRING\", mode=\"REQUIRED\"),\n",
    "                    bigquery.SchemaField(\"territoryname\", \"STRING\", mode=\"REQUIRED\")\n",
    "                   ]\n",
    "                        )\n",
    "]\n",
    "\n",
    "# create table dataflow.region_territory\n",
    "# (regionid NUMERIC\n",
    "# ,regionname STRING\n",
    "# ,territories ARRAY<STRUCT<territoryid NUMERIC, territoryname STRING>>)\n",
    "\n",
    "table_id = f\"{PROJECT_ID}.dataflow.region_territory\"\n",
    "\n",
    "try:\n",
    "    table = client.get_table(table_id)  # Make an API request.\n",
    "    print(\"Table {} already exists.\".format(table_id))\n",
    "    print(table)\n",
    "except:\n",
    "    table = bigquery.Table(table_id, schema=schema)\n",
    "    table = client.create_table(table)  # Make an API request.\n",
    "    print(\"Table {} created.\".format(table_id))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f289745-bf0e-473f-81f0-943bc1232ba0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### The code here is tricky: \n",
    "### First, parse the two tables into <font color='green' size=\"+2\">tuples</font>, <font color='black' face=\"Fixedsys, monospace\" size=\"+1\">(regionid, regionname)</font> & <font color='black' face=\"Fixedsys, monospace\" size=\"+1\">(regionid, {'territoryid':territoryid, 'territoryname':territoryname})</font>\n",
    "### <font color='blue' face=\"Fixedsys, monospace\" size=\"+2\">CoGroupByKey</font> yields a shape like <font color='black' face=\"Fixedsys, monospace\" size=\"+1\">(regionid, {'regions':['regionname'], 'territories':[{}])</font> so we need to reshape it to <font color='green' size=\"+2\">dicts</font> to write it to BQ.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce154df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam.io import ReadFromText\n",
    "\n",
    "class RegionParseTuple(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        regionid, regionname = element.split(',')\n",
    "        yield (int(regionid), regionname) # Can also use yield instead of returning a list\n",
    "\n",
    "class TerritoryParseTuple(beam.DoFn):\n",
    "    # split territory into KV pair of (regionid, (territoryid, territoryname))\n",
    "    def process(self, element):\n",
    "        territoryid, territoryname, regionid = element.split(',')\n",
    "        yield(int(regionid), {'territoryid': int(territoryid), 'territoryname':territoryname})\n",
    "\n",
    "class SortTerritories(beam.DoFn):\n",
    "    #{'regionid': 1, 'regionname': 'Eastern', 'territories': [{'territoryid': 1730, 'territoryname': 'Bedford'}, \n",
    "    def process(self, element):\n",
    "        territories = element['territories']\n",
    "        element['territories'] = sorted(territories, key = lambda x : x['territoryid'])\n",
    "        yield element\n",
    "\n",
    "regionsfilename = 'datasets/northwind/CSV/regions/regions.csv'\n",
    "territoriesfilename = 'datasets/northwind/CSV/territories/territories.csv'\n",
    "\n",
    "#PROJECT_ID = 'qwiklabs-gcp-04-4cf93802c378'\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "    regions = (\n",
    "              p | 'Read Regions' >> ReadFromText(regionsfilename)\n",
    "                | 'Parse Regions' >> beam.ParDo(RegionParseTuple())\n",
    "#                | 'Print Regions' >> beam.Map(print)\n",
    "              )\n",
    "        \n",
    "    territories = (\n",
    "                  p | 'Read Territories' >> ReadFromText('territories.csv')\n",
    "                    | 'Parse Territories' >> beam.ParDo(TerritoryParseTuple())\n",
    "#                    | 'Print Territories' >> beam.Map(print)\n",
    "                  )\n",
    "    nested = ( \n",
    "        {'regions':regions, 'territories':territories} \n",
    "              | 'Nest territories into regions' >> beam.CoGroupByKey()\n",
    "              | 'Reshape to dict' >> beam.Map(lambda x : {'regionid': x[0], 'regionname': x[1]['regions'][0], \n",
    "                                                        'territories': x[1]['territories']})\n",
    "              | 'Sort by territoryid' >> beam.ParDo(SortTerritories())\n",
    "#              | 'Print' >> beam.Map(print)\n",
    "    )\n",
    "    nested | 'Write nested region_territory to BQ' >> beam.io.WriteToBigQuery('region_territory', dataset = 'dataflow'\n",
    "                                                                             , project = PROJECT_ID\n",
    "                                                                             , method = 'STREAMING_INSERTS'\n",
    "                                                                             )\n",
    "#    nested | 'Print' >> beam.Map(print)\n",
    "             \n",
    "#help(beam.io.WriteToBigQuery)    \n",
    "#(1, {'regions': ['Eastern'], 'territories': [{'territoryid': 1730, 'territoryname': 'Bedford'}, {'territoryid': 1581, 'territoryname': 'Westboro'}, {'territoryid': 1833, 'territoryname': 'Georgetow'}, {'territoryid': 2116, 'territoryname': 'Bosto\n",
    "#{'regionid': 1, 'regionname':'Eastern', 'territories' : [{'territoryid':1, 'territoryname':'name1'}, {}, {}]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ae0097-3880-4cbc-9010-078fb1aa10df",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Query the table to show it was populated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2411903b-871d-487c-8324-089f330cb701",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "\n",
    "# Construct a BigQuery client object.\n",
    "client = bigquery.Client()\n",
    "\n",
    "table_id = f\"{PROJECT_ID}.dataflow.region_territory\"\n",
    "\n",
    "query_job = client.query(f\"\"\"SELECT * FROM {table_id}\"\"\")\n",
    "\n",
    "results = query_job.result()  # Waits for job to complete.\n",
    "display(list(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168bc155-d9d3-419e-8bd8-6b5a41718bdf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Helper functions to make a generic transform to nest children."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd6c54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "\n",
    "class NestJoin(beam.PTransform):\n",
    "    '''\n",
    "    This PTransform will take a dictionary to the left of the | which will be the collection of the two\n",
    "    PCollections you want to join together. Both must be a dictionary. You will then pass in the name of each\n",
    "    PCollection and the key to join them on.\n",
    "    It will automatically reshape the two dicts into tuples of (key, dict) where it removes the key from each dict\n",
    "    It then CoGroups them and reshapes the tuple into a dict ready for insertion to a BQ table\n",
    "    '''\n",
    "    def __init__(self, parent_pipeline_name, parent_key, child_pipeline_name, child_key, sort = lambda x : x):\n",
    "        self.parent_pipeline_name = parent_pipeline_name\n",
    "        self.parent_key = parent_key\n",
    "        self.child_pipeline_name = child_pipeline_name\n",
    "        self.child_key = child_key\n",
    "        self.sort = sort\n",
    "\n",
    "    def expand(self, pcols):\n",
    "        def reshapeToKV(item, key):\n",
    "            # pipeline object should be a dictionary\n",
    "            item1 = item.copy()\n",
    "            del item1[key]\n",
    "            return (item[key], item1)\n",
    "\n",
    "        def reshapeCoGroupToDict(item):\n",
    "            ret = {self.parent_key : item[0]}\n",
    "            ret.update(item[1][self.parent_pipeline_name][0])\n",
    "            ret[self.child_pipeline_name] = item[1][self.child_pipeline_name]\n",
    "            return ret\n",
    "\n",
    "        return (\n",
    "                {\n",
    "                self.parent_pipeline_name : pcols[self.parent_pipeline_name] | f'Convert {self.parent_pipeline_name} to KV' \n",
    "                    >> beam.Map(reshapeToKV, self.parent_key)\n",
    "                ,self.child_pipeline_name : pcols[self.child_pipeline_name] | f'Convert {self.child_pipeline_name} to KV'\n",
    "                    >> beam.Map(reshapeToKV, self.child_key)\n",
    "                } | f'CoGroupByKey {self.child_pipeline_name} into {self.parent_pipeline_name}'\n",
    "                    >> beam.CoGroupByKey()\n",
    "                  | f'Reshape to dictionary'\n",
    "                    >> beam.Map(reshapeCoGroupToDict)\n",
    "                  | f'Sort the nested data' >> beam.Map(self.sort)\n",
    "            \n",
    "        )\n",
    "\n",
    "class RegionParseDict(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        regionid, regionname = element.split(',')\n",
    "        yield {'regionid':int(regionid), 'regionname':regionname.title()}\n",
    "      \n",
    "class TerritoryParseDict(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        territoryid, territoryname, regionid = element.split(',')\n",
    "        yield {'territoryid':int(territoryid), 'territoryname' : territoryname, 'regionid':int(regionid)}\n",
    "    \n",
    "regionsfilename = 'datasets/northwind/CSV/regions/regions.csv'\n",
    "territoriesfilename = 'datasets/northwind/CSV/territories/territories.csv'\n",
    "\n",
    "def sort_territories(element):\n",
    "    territories = element['territories']\n",
    "    element['territories'] = list(sorted(territories, key = lambda x : x['territoryid']))\n",
    "    return element\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "    regions = (\n",
    "              p | 'Read Regions' >> ReadFromText(regionsfilename)\n",
    "                | 'Parse Regions' >> beam.ParDo(RegionParseDict())\n",
    "                #| 'Print Regions' >> beam.Map(print)\n",
    "              )\n",
    "        \n",
    "    territories = (\n",
    "                  p | 'Read Territories' >> ReadFromText('territories.csv')\n",
    "                    | 'Parse Territories' >> beam.ParDo(TerritoryParseDict())\n",
    "                    #| 'Print Territories' >> beam.Map(print)\n",
    "                  )\n",
    "\n",
    "    nestjoin = {'regions':regions, 'territories':territories} | NestJoin('regions', 'regionid', 'territories', 'regionid', sort = sort_territories)\n",
    "    nestjoin | 'Print Nest Join' >> beam.Map(print)\n",
    "#     nestjoin | 'Write nested region_territory to BQ' >> beam.io.WriteToBigQuery('region_territory', dataset = 'dataflow'\n",
    "#                                                                              , project = PROJECT_ID\n",
    "#                                                                              , method = 'STREAMING_INSERTS'\n",
    "#                                                                              )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8bdeb1-854c-48df-b8e8-d3f0c81e3b46",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Simulate an Outer Join with <font color='blue' face=\"Fixedsys, monospace\" size=\"+2\">CoGroup</font>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0108d942-1b57-461e-83e7-70574e318f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "\n",
    "class LeftJoin(beam.PTransform):\n",
    "    '''\n",
    "    This PTransform will take a dictionary to the left of the | which will be the collection of the two\n",
    "    PCollections you want to join together. Both must be a dictionary. You will then pass in the name of each\n",
    "    PCollection and the key to join them on.\n",
    "    It will automatically reshape the two dicts into tuples of (key, dict) where it removes the key from each dict\n",
    "    It then CoGroups them and reshapes the tuple into a dict ready for insertion to a BQ table\n",
    "    '''\n",
    "    def __init__(self, parent_pipeline_name, parent_key, child_pipeline_name, child_key):\n",
    "        self.parent_pipeline_name = parent_pipeline_name\n",
    "        self.parent_key = parent_key\n",
    "        self.child_pipeline_name = child_pipeline_name\n",
    "        self.child_key = child_key\n",
    "\n",
    "    def expand(self, pcols):\n",
    "        def reshapeToKV(item, key):\n",
    "            # pipeline object should be a dictionary\n",
    "            item1 = item.copy()\n",
    "            del item1[key]\n",
    "            return (item[key], item1)\n",
    "\n",
    "        def reshapeCoGroupToFlatDict(item):\n",
    "            parent = {self.parent_key : item[0]}\n",
    "            parent.update(item[1][self.parent_pipeline_name][0])\n",
    "            ret = []\n",
    "            for row1 in item[1][self.child_pipeline_name]:\n",
    "                row = parent.copy()\n",
    "                row.update(row1)\n",
    "                ret.append(row)\n",
    "            return ret\n",
    "\n",
    "        return (\n",
    "                {\n",
    "                self.parent_pipeline_name : pcols[self.parent_pipeline_name] | f'Convert {self.parent_pipeline_name} to KV' \n",
    "                    >> beam.Map(reshapeToKV, self.parent_key)\n",
    "                ,self.child_pipeline_name : pcols[self.child_pipeline_name] | f'Convert {self.child_pipeline_name} to KV'\n",
    "                    >> beam.Map(reshapeToKV, self.child_key)\n",
    "                } | f'CoGroupByKey {self.child_pipeline_name} into {self.parent_pipeline_name}'\n",
    "                    >> beam.CoGroupByKey()\n",
    "                  | f'Reshape to dictionary'\n",
    "                    >> beam.Map(reshapeCoGroupToFlatDict)\n",
    "        )\n",
    "\n",
    "class RegionParseDict(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        regionid, regionname = element.split(',')\n",
    "        yield {'regionid':int(regionid), 'regionname':regionname.title()}\n",
    "\n",
    "class TerritoryParseDict(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        territoryid, territoryname, regionid = element.split(',')\n",
    "        yield {'territoryid':int(territoryid), 'territoryname' : territoryname, 'regionid':int(regionid)}\n",
    "    \n",
    "regionsfilename = 'datasets/northwind/CSV/regions/regions.csv'\n",
    "territoriesfilename = 'datasets/northwind/CSV/territories/territories.csv'\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "    regions = (\n",
    "              p | 'Read Regions' >> ReadFromText(regionsfilename)\n",
    "                | 'Parse Regions' >> beam.ParDo(RegionParseDict())\n",
    "                #| 'Print Regions' >> beam.Map(print)\n",
    "              )\n",
    "        \n",
    "    territories = (\n",
    "                  p | 'Read Territories' >> ReadFromText('territories.csv')\n",
    "                    | 'Parse Territories' >> beam.ParDo(TerritoryParseDict())\n",
    "                    #| 'Print Territories' >> beam.Map(print)\n",
    "                  )\n",
    "\n",
    "    nestjoin = {'regions':regions, 'territories':territories} | LeftJoin('regions', 'regionid', 'territories', 'regionid')\n",
    "    nestjoin | 'Print Nest Join' >> beam.Map(print)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a313cf2f-c440-452a-bf57-129bf2926f29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f4530d-df0e-4844-aef0-632b63109107",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f12065-b9cd-4a07-aa07-7d20633912ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3c2dbd91-e4b8-4bd1-8c75-89064613bbd8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <img src=\"images/java.png\" width=40 height=40 /><font color='indigo' size=\"+2\">Java</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167426fd-5abf-40b0-a06d-bfabda42f6c3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### For Java, you don't need to group into KV shape first; instead you could use the <font color='blue' face=\"Fixedsys, monospace\" size=\"+2\">Group</font> and <font color='blue' face=\"Fixedsys, monospace\" size=\"+2\">Select</font> methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de985dc-b8b4-4aa0-a152-715056b2dd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%java \n",
    "package samples.quickstart;\n",
    "\n",
    "import org.apache.beam.sdk.Pipeline;\n",
    "import org.apache.beam.sdk.values.PCollection;\n",
    "import org.apache.beam.sdk.values.TypeDescriptors;\n",
    "import org.apache.beam.sdk.io.TextIO;\n",
    "import org.apache.beam.sdk.transforms.ParDo;\n",
    "import org.apache.beam.sdk.transforms.DoFn;\n",
    "import org.apache.beam.sdk.transforms.Filter;\n",
    "import org.apache.beam.sdk.transforms.DoFn.ProcessContext;\n",
    "import org.apache.beam.sdk.coders.DefaultCoder;\n",
    "import org.apache.beam.sdk.coders.AvroCoder;\n",
    "import org.apache.beam.sdk.transforms.SerializableFunction;\n",
    "import org.apache.beam.sdk.schemas.transforms.Group;\n",
    "import org.apache.beam.sdk.schemas.transforms.Select;\n",
    "import org.apache.beam.sdk.transforms.*;\n",
    "import org.apache.beam.sdk.schemas.JavaFieldSchema;\n",
    "import org.apache.beam.sdk.schemas.annotations.DefaultSchema;\n",
    "import org.apache.beam.sdk.values.Row;\n",
    "import org.apache.beam.sdk.schemas.transforms.Convert;\n",
    "\n",
    "import org.slf4j.Logger;\n",
    "import org.slf4j.LoggerFactory;\n",
    "\n",
    "public class GroupTerritories {\n",
    "    public static void main(String[] args) {\n",
    "        Pipeline p = Pipeline.create();\n",
    "\n",
    "        String territoriesInputFileName = \"datasets/northwind/CSV/territories/territories.csv\";\n",
    "        String outputsPrefix = \"/tmp/outputs\";\n",
    "\n",
    "        PCollection<Result> territories = p\n",
    "            .apply(\"Read\", TextIO.read().from(territoriesInputFileName))\n",
    "            .apply(\"Parse\", ParDo.of(new ParseTerritories()))\n",
    "            .apply(\"GroupBy regionID\", Group.<Territory>byFieldNames(\"regionID\")\n",
    "                                            .aggregateField(\"territoryID\", Count.combineFn(), \"cnt\"))\n",
    "            .apply(\"Select\", Select.fieldNames(\"key.regionID\", \"value.cnt\"))\n",
    "            .apply(Convert.fromRows(Result.class))\n",
    "                   \n",
    "        ;                   \n",
    "        \n",
    "        territories.apply(TextIO.<Result>writeCustomType().to(outputsPrefix).withFormatFunction(new SerializeResult()));\n",
    "        p.run().waitUntilFinish();\n",
    "    }\n",
    "    \n",
    "    @DefaultCoder(AvroCoder.class)\n",
    "    @DefaultSchema(JavaFieldSchema.class)\n",
    "    static class Territory {\n",
    "        Long territoryID;\n",
    "        String territoryName;\n",
    "        Long regionID;\n",
    "        \n",
    "        Territory() {}\n",
    "        \n",
    "        Territory(long territoryID, String territoryName, long regionID) {\n",
    "            this.territoryID = territoryID;\n",
    "            this.territoryName = territoryName;\n",
    "            this.regionID = regionID;\n",
    "        }\n",
    "        \n",
    "        @Override\n",
    "        public String toString() {\n",
    "            return String.format(\"(territoryID = %d, territoryName = %s, regionID = %d)\", territoryID, territoryName, regionID);\n",
    "        }\n",
    "\n",
    "    }\n",
    "    \n",
    "    static class SerializeTerritory implements SerializableFunction<Territory, String> {\n",
    "        @Override\n",
    "        public String apply(Territory input) {\n",
    "          return input.toString();\n",
    "        }\n",
    "    }\n",
    "\n",
    "    static class ParseTerritories extends DoFn<String, Territory> {\n",
    "        private static final Logger LOG = LoggerFactory.getLogger(ParseTerritories.class);\n",
    "\n",
    "        @ProcessElement\n",
    "        public void process(ProcessContext c) {\n",
    "            String[] columns = c.element().split(\",\");\n",
    "            try {\n",
    "                Long territoryID = Long.parseLong(columns[0].trim());\n",
    "                String territoryName = columns[1].trim();\n",
    "                Long regionID = Long.parseLong(columns[2].trim());\n",
    "                c.output(new Territory(territoryID, territoryName, regionID));\n",
    "            } catch (ArrayIndexOutOfBoundsException | NumberFormatException e) {\n",
    "                LOG.info(\"ParseTerritories: parse error on '\" + c.element() + \"': \" + e.getMessage());\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    @DefaultCoder(AvroCoder.class)\n",
    "    @DefaultSchema(JavaFieldSchema.class)\n",
    "    static class Result {\n",
    "        Long regionID;\n",
    "        Long cnt;\n",
    "        \n",
    "        Result() {}\n",
    "        \n",
    "        Result(Long regionID, Long cnt) {\n",
    "            this.regionID = regionID;\n",
    "            this.cnt = cnt;\n",
    "        }\n",
    "        \n",
    "        @Override\n",
    "        public String toString() {\n",
    "            return String.format(\"(regionid = %d, cnt = %d)\", regionID, cnt);\n",
    "        }\n",
    "    }\n",
    "    static class SerializeResult implements SerializableFunction<Result, String> {\n",
    "        @Override\n",
    "        public String apply(Result input) {\n",
    "          return input.toString();\n",
    "        }\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be08fffa-0c55-4c8a-8469-a03d87facb3b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### For the <font color='blue' face=\"Fixedsys, monospace\" size=\"+2\">JOIN</font> extension function, you still need to shape the data into a KV pair and then unnest it when done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0030bb71-8ee0-48e6-aa39-284b2c65c042",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%java\n",
    "package samples.quickstart;\n",
    "\n",
    "import org.apache.beam.sdk.Pipeline;\n",
    "import org.apache.beam.sdk.values.PCollection;\n",
    "import org.apache.beam.sdk.values.TypeDescriptors;\n",
    "import org.apache.beam.sdk.io.TextIO;\n",
    "import org.apache.beam.sdk.transforms.ParDo;\n",
    "import org.apache.beam.sdk.transforms.DoFn;\n",
    "import org.apache.beam.sdk.transforms.Filter;\n",
    "import org.apache.beam.sdk.transforms.DoFn.ProcessContext;\n",
    "import org.apache.beam.sdk.coders.DefaultCoder;\n",
    "import org.apache.beam.sdk.coders.AvroCoder;\n",
    "import org.apache.beam.sdk.transforms.SerializableFunction;\n",
    "import org.apache.beam.sdk.schemas.transforms.Group;\n",
    "import org.apache.beam.sdk.schemas.transforms.Select;\n",
    "import org.apache.beam.sdk.transforms.*;\n",
    "import org.apache.beam.sdk.schemas.JavaFieldSchema;\n",
    "import org.apache.beam.sdk.schemas.annotations.DefaultSchema;\n",
    "import org.apache.beam.sdk.values.Row;\n",
    "import org.apache.beam.sdk.schemas.transforms.Convert;\n",
    "import org.apache.beam.sdk.extensions.joinlibrary.Join;\n",
    "import org.apache.beam.sdk.values.KV;\n",
    "import org.apache.beam.sdk.transforms.WithKeys;\n",
    "\n",
    "import org.slf4j.Logger;\n",
    "import org.slf4j.LoggerFactory;\n",
    "\n",
    "public class JoinTerritories {\n",
    "    public static void main(String[] args) {\n",
    "        Pipeline p = Pipeline.create();\n",
    "        \n",
    "        String regionsInputFileName = \"datasets/northwind/CSV/regions/regions.csv\";\n",
    "        String territoriesInputFileName = \"datasets/northwind/CSV/territories/territories.csv\";\n",
    "        String outputsPrefix = \"/tmp/outputs\";\n",
    "\n",
    "        PCollection<KV<Long, Region>> regions = p\n",
    "            .apply(\"Read Regions\", TextIO.read().from(regionsInputFileName))\n",
    "            .apply(\"Parse Regions\", ParDo.of(new CSVToRegion()))\n",
    "            .apply(\"Regions KV\", WithKeys.of(new SerializableFunction<Region, Long>() {\n",
    "                @Override\n",
    "                public Long apply(Region r) {\n",
    "                  return r.regionid;\n",
    "                }}));\n",
    "          ;\n",
    "        \n",
    "        PCollection<KV<Long, Territory>> territories = p\n",
    "            .apply(\"Read Territories\", TextIO.read().from(territoriesInputFileName))\n",
    "            .apply(\"Parse Territories\", ParDo.of(new ParseTerritories()))\n",
    "            .apply(\"Territories KV\", WithKeys.of(new SerializableFunction<Territory, Long>() {\n",
    "                @Override\n",
    "                public Long apply(Territory t) {\n",
    "                  return t.regionid;\n",
    "                }}));\n",
    "          ;\n",
    "        \n",
    "        PCollection<KV<Long, KV<Region, Territory>>> result =\n",
    "            Join.innerJoin(regions, territories);  \n",
    "        \n",
    "        PCollection<Result> result2 = result\n",
    "        \n",
    "            .apply(\"Unnest KV\", ParDo.of(new DoFn<KV<Long, KV<Region, Territory>>, Result>() {\n",
    "                @ProcessElement\n",
    "                public void process(ProcessContext c) {\n",
    "                    KV<Long, KV<Region, Territory>> e = c.element();\n",
    "                    Long regionid = e.getKey();\n",
    "                    KV<Region, Territory> v = e.getValue();\n",
    "                    Region r = v.getKey();\n",
    "                    Territory t = v.getValue(); \n",
    "                    String regionname = r.regionname;\n",
    "                    Long territoryid = t.territoryid;\n",
    "                    String territoryname = t.territoryname;\n",
    "                    //c.output(new Result(1L, \"regionname\", 2L, \"territoryname\"));\n",
    "                    c.output(new Result(regionid, regionname, territoryid, territoryname));\n",
    "                }\n",
    "                \n",
    "            })\n",
    "            );\n",
    "\n",
    "        \n",
    "        result2.apply(TextIO.<Result>writeCustomType().to(outputsPrefix).withFormatFunction(new SerializeResult()));\n",
    "        p.run().waitUntilFinish();\n",
    "    }\n",
    "    \n",
    "    @DefaultCoder(AvroCoder.class)\n",
    "    @DefaultSchema(JavaFieldSchema.class)\n",
    "    static class Region {\n",
    "        Long regionid;\n",
    "        String regionname;\n",
    "        \n",
    "        Region() {}\n",
    "        \n",
    "        Region(Long regionid, String regionname) {\n",
    "            this.regionid = regionid;\n",
    "            this.regionname = regionname;\n",
    "        }\n",
    "        \n",
    "        @Override\n",
    "        public String toString() {\n",
    "            return String.format(\"(regionid = %d, regionname = %s)\", regionid, regionname);\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    static class SerializeRegion implements SerializableFunction<Region, String> {\n",
    "        @Override\n",
    "        public String apply(Region input) {\n",
    "          return input.toString();\n",
    "        }\n",
    "    }\n",
    "\n",
    "    static class CSVToRegion extends DoFn<String, Region> {\n",
    "        private static final Logger LOG = LoggerFactory.getLogger(CSVToRegion.class);\n",
    "\n",
    "        @ProcessElement\n",
    "        public void process(ProcessContext c) {\n",
    "            String[] columns = c.element().split(\",\");\n",
    "            try {\n",
    "                Long regionid = Long.parseLong(columns[0].trim());\n",
    "                String regionname = columns[1].trim();\n",
    "                c.output(new Region(regionid, regionname));\n",
    "            } catch (ArrayIndexOutOfBoundsException | NumberFormatException e) {\n",
    "                LOG.info(\"CSVToRegion: parse error on '\" + c.element() + \"': \" + e.getMessage());\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    @DefaultCoder(AvroCoder.class)\n",
    "    @DefaultSchema(JavaFieldSchema.class)\n",
    "    static class Territory {\n",
    "        Long territoryid;\n",
    "        String territoryname;\n",
    "        Long regionid;\n",
    "        \n",
    "        Territory() {}\n",
    "        \n",
    "        Territory(long territoryid, String territoryname, long regionid) {\n",
    "            this.territoryid = territoryid;\n",
    "            this.territoryname = territoryname;\n",
    "            this.regionid = regionid;\n",
    "        }\n",
    "        \n",
    "        @Override\n",
    "        public String toString() {\n",
    "            return String.format(\"(territoryid = %d, territoryname = %s, regionid = %d)\", territoryid, territoryname, regionid);\n",
    "        }\n",
    "\n",
    "    }\n",
    "    \n",
    "    static class SerializeTerritory implements SerializableFunction<Territory, String> {\n",
    "        @Override\n",
    "        public String apply(Territory input) {\n",
    "          return input.toString();\n",
    "        }\n",
    "    }\n",
    "\n",
    "    static class ParseTerritories extends DoFn<String, Territory> {\n",
    "        private static final Logger LOG = LoggerFactory.getLogger(ParseTerritories.class);\n",
    "\n",
    "        @ProcessElement\n",
    "        public void process(ProcessContext c) {\n",
    "            String[] columns = c.element().split(\",\");\n",
    "            try {\n",
    "                Long territoryid = Long.parseLong(columns[0].trim());\n",
    "                String territoryname = columns[1].trim();\n",
    "                Long regionid = Long.parseLong(columns[2].trim());\n",
    "                c.output(new Territory(territoryid, territoryname, regionid));\n",
    "            } catch (ArrayIndexOutOfBoundsException | NumberFormatException e) {\n",
    "                LOG.info(\"ParseTerritories: parse error on '\" + c.element() + \"': \" + e.getMessage());\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    @DefaultCoder(AvroCoder.class)\n",
    "    @DefaultSchema(JavaFieldSchema.class)\n",
    "    static class Result {\n",
    "        Long regionid;\n",
    "        String regionname;\n",
    "        Long territoryid;\n",
    "        String territoryname;\n",
    "        \n",
    "        Result() {}\n",
    "        \n",
    "        Result(Long regionid, String regionname, Long territoryid, String territoryname) {\n",
    "            this.regionid = regionid;\n",
    "            this.regionname = regionname;\n",
    "            this.territoryid = territoryid;\n",
    "            this.territoryname = territoryname;\n",
    "        }\n",
    "        \n",
    "        @Override\n",
    "        public String toString() {\n",
    "            return String.format(\"(regionid = %d, regionname = %s, territoryid = %d, territoryname = %s)\", regionid, regionname, territoryid, territoryname);\n",
    "        }\n",
    "    }\n",
    "    static class SerializeResult implements SerializableFunction<Result, String> {\n",
    "        @Override\n",
    "        public String apply(Result input) {\n",
    "          return input.toString();\n",
    "        }\n",
    "    }\n",
    "}\n",
    "                   \n",
    "                   \n",
    "// KV{3, KV{(regionid = 3, regionname = Northern), (territoryid = 3801, territoryname = Portsmouth, regionid = 3)}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62fc2a94-7ddf-4a23-8581-e58bac8e7240",
   "metadata": {
    "tags": []
   },
   "source": [
    "# __ __ __ __ __ __ __ __ __ __ __ __"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcaad46b-2a1d-40df-a573-02e5716279c6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 5. BeamSQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c07e256-d80c-4465-a6cf-8930c1825f2a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## <img src=\"images/python.png\" width=40 height=40 /><font color='cadetblue' size=\"+2\">Python</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a986c8bd-d52f-40f8-9638-ef5ccfeb59d5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### SQL Transform uses <font color='green' size=\"+2\">PCOLLECTION</font> as the name of a single source passed into it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391af3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam.io import ReadFromText\n",
    "from apache_beam import coders\n",
    "from apache_beam.transforms.sql import SqlTransform\n",
    "\n",
    "import typing\n",
    "import json\n",
    "\n",
    "class Territory(typing.NamedTuple):\n",
    "    territoryid: int\n",
    "    territoryname: str\n",
    "    regionid: int\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f'territoryid = {self.territoryid} territoryname = {self.territoryname} regionid = {self.regionid}'\n",
    "coders.registry.register_coder(Territory, coders.RowCoder)\n",
    "        \n",
    "@beam.typehints.with_output_types(Territory)\n",
    "class TerritoryParseClass(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        territoryid, territoryname, regionid = element.split(',')\n",
    "        yield Territory(int(territoryid), territoryname.title(), int(regionid))\n",
    "    \n",
    "class RegionCount(typing.NamedTuple):\n",
    "    regionid: int\n",
    "    count: int\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f'regionid = {self.regionid} count = {self.count}'\n",
    "coders.registry.register_coder(RegionCount, coders.RowCoder)\n",
    "        \n",
    "        \n",
    "territoriesfilename = 'territories.csv'\n",
    "with beam.Pipeline() as p:\n",
    "    territories = (\n",
    "                  p | 'Read Territories' >> ReadFromText('territories.csv')\n",
    "#                    | 'Parse Territories' >> beam.ParDo(TerritoryParseClass()).with_output_types(Territory) # if we didn't have with_output_types decorator\n",
    "                    | 'Parse Territories' >> beam.ParDo(TerritoryParseClass())\n",
    "                    | 'SQL Territories' >> SqlTransform(\"\"\"SELECT regionid, count(*) as `count` FROM PCOLLECTION GROUP BY regionid\"\"\")\n",
    "#                    | 'Map Territories for Print' >> beam.Map(lambda x : f'regionid = {x.regionid}  count = {x.count}')\n",
    "#                    | 'Convert to RegionCount Class' >> beam.Map(lambda x : RegionCount(x.regionid, x.count))\n",
    "                    | 'Print SQL' >> beam.Map(print)\n",
    "                    )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10c3a7c-0584-4268-a2f5-fbc04534eacc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### For a SQL query that has more than one source, bundle the sources together in a dictionary, they keys become the table names inside the SQL string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc93081",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam.io import ReadFromText\n",
    "from apache_beam import coders\n",
    "from apache_beam.transforms.sql import SqlTransform\n",
    "\n",
    "import typing\n",
    "import json\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "    parent = (\n",
    "            p | 'Create Parent' >> beam.Create([(1, 'Vowel'), (2, 'Consonant'), (4, 'Unknown')])\n",
    "              | 'Map Parent' >> beam.Map(lambda x : beam.Row(parent_id = x[0], parent_name = x[1]))\n",
    "    )\n",
    "\n",
    "    child = (\n",
    "            p | 'Create Child' >> beam.Create([('Alpha', 1), ('Beta', 2), ('Gamma', 2), ('Delta', 2), ('Epsilon', 1), ('Pi', 3)])\n",
    "              | 'Map Child' >> beam.Map(lambda x : beam.Row(child_name = x[0], parent_id = x[1]))\n",
    "    )\n",
    "    \n",
    "    result = ( {'parent': parent, 'child' : child} \n",
    "         | SqlTransform(\"\"\"\n",
    "             SELECT p.parent_id, p.parent_name, c.child_name \n",
    "             FROM parent as p \n",
    "             INNER JOIN child as c ON p.parent_id = c.parent_id\n",
    "             \"\"\")\n",
    "        | 'Format Output' >> beam.Map(lambda x : f'{x.parent_id}, {x.parent_name}, {x.child_name}')\n",
    "        | 'Print Join' >> beam.Map(print)\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e11a34-1834-4402-b1e3-d00b1bfce2d5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Real example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d426a5c0-80c8-4605-bb8e-371cb5d9d459",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam import pvalue\n",
    "from apache_beam.io import ReadFromText, WriteToText\n",
    "import typing\n",
    "\n",
    "class Region(typing.NamedTuple):\n",
    "    regionid: int\n",
    "    regionname: str\n",
    "beam.coders.registry.register_coder(Region, beam.coders.RowCoder)\n",
    "        \n",
    "class RegionParseClass(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        yield Region(int(element['regionid']), element['regiondescription'])\n",
    "\n",
    "class Territory(typing.NamedTuple):\n",
    "    territoryid: int\n",
    "    territoryname: str\n",
    "    regionid: int\n",
    "beam.coders.registry.register_coder(Territory, beam.coders.RowCoder)\n",
    "        \n",
    "class TerritoryParseClass(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        yield Territory(int(element['territoryid']), element['territorydescription'], int(element['regionid']))\n",
    "\n",
    "class Result(typing.NamedTuple):\n",
    "    regionid: int\n",
    "    regionname: str\n",
    "    cnt: int\n",
    "beam.coders.registry.register_coder(Result, beam.coders.RowCoder)\n",
    "               \n",
    "regionsfilename = 'datasets/northwind/AVRO/regions/regions.avro'\n",
    "territoriesfilename = 'datasets/northwind/AVRO/territories/territories.avro'\n",
    "with beam.Pipeline() as p:\n",
    "    regions = (p | 'Read Regions' >> beam.io.ReadFromAvro(regionsfilename)\n",
    "                     | 'Parse Regions' >> beam.ParDo(RegionParseClass())\n",
    "                  )\n",
    "    territories = (p | 'Read Territories' >> beam.io.ReadFromAvro(territoriesfilename)\n",
    "                     | 'Parse Territories' >> beam.ParDo(TerritoryParseClass())\n",
    "                  )\n",
    "\n",
    "    result = ( {'regions': regions, 'territories' : territories} \n",
    "         | SqlTransform(\"\"\"\n",
    "SELECT r.regionid AS regionid, r.regionname AS regionname, SUM(1) AS cnt \n",
    "FROM regions AS r \n",
    "JOIN territories AS t on t.regionid = r.regionid \n",
    "GROUP BY r.regionid, r.regionname\n",
    "\"\"\")\n",
    "        | 'Convert to Result Class' >> beam.Map(lambda x : Result(x.regionid, x.regionname, x.cnt))\n",
    "#        | 'Format Output' >> beam.Map(lambda x : f'{x.regionid}, {x.regionname}, {x.cnt}')\n",
    "        | 'Print Join' >> beam.Map(print)\n",
    "             )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe17701-cec9-4a53-b8ea-66f8bf5ad2ae",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <img src=\"images/java.png\" width=40 height=40 /><font color='indigo' size=\"+2\">Java</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e65dd25-b71a-470f-8634-a9719bf580fa",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Beam SQL using Pojo with a simple query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5950d747-c1dc-4390-85f5-5191cfcb4afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%java verbose\n",
    "package samples.quickstart;\n",
    "\n",
    "import org.apache.beam.sdk.Pipeline;\n",
    "import org.apache.beam.sdk.values.PCollection;\n",
    "import org.apache.beam.sdk.values.TypeDescriptors;\n",
    "import org.apache.beam.sdk.io.TextIO;\n",
    "import org.apache.beam.sdk.transforms.ParDo;\n",
    "import org.apache.beam.sdk.transforms.DoFn;\n",
    "import org.apache.beam.sdk.transforms.Filter;\n",
    "import org.apache.beam.sdk.transforms.DoFn.ProcessContext;\n",
    "import org.apache.beam.sdk.coders.DefaultCoder;\n",
    "import org.apache.beam.sdk.coders.AvroCoder;\n",
    "import org.apache.beam.sdk.transforms.SerializableFunction;\n",
    "import org.apache.beam.sdk.schemas.Schema;\n",
    "import org.apache.beam.sdk.schemas.Schema.FieldType;\n",
    "import org.apache.beam.sdk.values.Row;\n",
    "import org.apache.beam.sdk.extensions.sql.SqlTransform;\n",
    "import org.apache.beam.sdk.transforms.MapElements;\n",
    "import org.apache.beam.sdk.transforms.SimpleFunction;\n",
    "import org.apache.beam.sdk.schemas.AutoValueSchema;\n",
    "import org.apache.beam.sdk.schemas.annotations.DefaultSchema;\n",
    "import org.apache.beam.sdk.schemas.JavaFieldSchema;\n",
    "import org.apache.beam.sdk.schemas.annotations.SchemaCreate;\n",
    "import com.google.auto.value.AutoValue;\n",
    "import org.apache.beam.sdk.schemas.transforms.Convert;\n",
    "import com.google.gson.Gson;\n",
    "\n",
    "import org.slf4j.Logger;\n",
    "import org.slf4j.LoggerFactory;\n",
    "\n",
    "\n",
    "public class ReadTerritories {\n",
    "    public static void main(String[] args) {\n",
    "        System.getProperties().put(\"org.apache.commons.logging.simplelog.defaultlog\",\"fatal\");\n",
    "\n",
    "        Pipeline p = Pipeline.create();\n",
    "        p.getSchemaRegistry().registerPOJO(Territory.class);\n",
    " \n",
    "        String territoriesInputFileName = \"datasets/northwind/JSON/territories/territories.json\";\n",
    "        String outputsPrefix = \"/tmp/outputs\";\n",
    "\n",
    "        PCollection<Territory> result = p\n",
    "            .apply(\"Read\", TextIO.read().from(territoriesInputFileName))\n",
    "            .apply(\"Parse\", ParDo.of(new JsonToTerritory()))\n",
    "            .apply(SqlTransform.query(\"SELECT territoryid, upper(territoryname) as territoryname, regionid FROM PCOLLECTION WHERE regionid = 1\"))\n",
    "            .apply(Convert.fromRows(Territory.class))\n",
    "        ;\n",
    "\n",
    "        /*\n",
    "        result.apply(MapElements.via(\n",
    "            new SimpleFunction<Territory, Territory>() {\n",
    "              @Override\n",
    "              public Territory apply(Territory t) {\n",
    "                System.out.println(\"** \" + t);\n",
    "                return t;\n",
    "              }\n",
    "            })); \n",
    "        */\n",
    "        \n",
    "        result.apply(TextIO.<Territory>writeCustomType().to(outputsPrefix).withFormatFunction(new SerializeTerritory()));\n",
    "        \n",
    "        p.run().waitUntilFinish();\n",
    "    }\n",
    "    \n",
    "\n",
    "    @DefaultSchema(JavaFieldSchema.class)\n",
    "    static class Territory {\n",
    "        Long territoryid;\n",
    "        String territoryname;\n",
    "        Long regionid;\n",
    "        \n",
    "        Territory() {}\n",
    "        \n",
    "        Territory(long territoryid, String territoryname, long regionid) {\n",
    "            this.territoryid = territoryid;\n",
    "            this.territoryname = territoryname;\n",
    "            this.regionid = regionid;\n",
    "        }\n",
    "        \n",
    "        @Override\n",
    "        public String toString() {\n",
    "            return String.format(\"(territoryID = %d, territoryName = %s, regionID = %d)\", territoryid, territoryname, regionid);\n",
    "        }\n",
    "\n",
    "    }\n",
    "    \n",
    "    static class SerializeTerritory implements SerializableFunction<Territory, String> {\n",
    "        @Override\n",
    "        public String apply(Territory input) {\n",
    "          return input.toString();\n",
    "        }\n",
    "    }\n",
    "\n",
    "    static class JsonToTerritory extends DoFn<String, Territory> {\n",
    "        @ProcessElement\n",
    "        public void process(@Element String json, OutputReceiver<Territory> r) throws Exception {\n",
    "            Gson gson = new Gson();\n",
    "            Territory t = gson.fromJson(json, Territory.class);\n",
    "            r.output(t);\n",
    "        }\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa61dbd-c2bf-4010-a657-35e4732dc3ed",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Beam SQL using multiple sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfee942f-062c-4711-bee5-e8f7ec6709c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%java\n",
    "package samples.quickstart;\n",
    "\n",
    "import org.apache.beam.sdk.Pipeline;\n",
    "import org.apache.beam.sdk.values.PCollection;\n",
    "import org.apache.beam.sdk.values.TypeDescriptors;\n",
    "import org.apache.beam.sdk.io.TextIO;\n",
    "import org.apache.beam.sdk.transforms.ParDo;\n",
    "import org.apache.beam.sdk.transforms.DoFn;\n",
    "import org.apache.beam.sdk.transforms.Filter;\n",
    "import org.apache.beam.sdk.transforms.DoFn.ProcessContext;\n",
    "import org.apache.beam.sdk.coders.DefaultCoder;\n",
    "import org.apache.beam.sdk.coders.AvroCoder;\n",
    "import org.apache.beam.sdk.transforms.SerializableFunction;\n",
    "import org.apache.beam.sdk.schemas.Schema;\n",
    "import org.apache.beam.sdk.schemas.Schema.FieldType;\n",
    "import org.apache.beam.sdk.values.Row;\n",
    "import org.apache.beam.sdk.extensions.sql.SqlTransform;\n",
    "import org.apache.beam.sdk.transforms.MapElements;\n",
    "import org.apache.beam.sdk.transforms.SimpleFunction;\n",
    "import org.apache.beam.sdk.schemas.AutoValueSchema;\n",
    "import org.apache.beam.sdk.schemas.annotations.DefaultSchema;\n",
    "import org.apache.beam.sdk.schemas.JavaFieldSchema;\n",
    "import org.apache.beam.sdk.schemas.annotations.SchemaCreate;\n",
    "import com.google.auto.value.AutoValue;\n",
    "import org.apache.beam.sdk.schemas.transforms.Convert;\n",
    "import com.google.gson.Gson;\n",
    "import org.apache.beam.sdk.values.PCollectionTuple;\n",
    "import org.apache.beam.sdk.values.TupleTag;\n",
    "import java.io.Serializable;\n",
    "\n",
    "import org.slf4j.Logger;\n",
    "import org.slf4j.LoggerFactory;\n",
    "\n",
    "\n",
    "public class ReadTerritories {\n",
    "    public static void main(String[] args) {\n",
    "        System.getProperties().put(\"org.apache.commons.logging.simplelog.defaultlog\",\"fatal\");\n",
    "\n",
    "        Pipeline p = Pipeline.create();\n",
    "        p.getSchemaRegistry().registerPOJO(Region.class);\n",
    "        p.getSchemaRegistry().registerPOJO(Territory.class);\n",
    "        p.getSchemaRegistry().registerPOJO(Result.class);\n",
    " \n",
    "        String regionsInputFileName = \"datasets/northwind/CSV/regions/regions.csv\";\n",
    "        String territoriesInputFileName = \"datasets/northwind/JSON/territories/territories.json\";\n",
    "        String outputsPrefix = \"/tmp/outputs\";\n",
    "\n",
    "        PCollection<Region> regions = p\n",
    "            .apply(\"Read Regions\", TextIO.read().from(regionsInputFileName))\n",
    "            .apply(\"Parse Regions\", ParDo.of(new CSVToRegion()));\n",
    "\n",
    "        PCollection<Territory> territories = p\n",
    "            .apply(\"Read Territories\", TextIO.read().from(territoriesInputFileName))\n",
    "            .apply(\"Parse Territories\", ParDo.of(new JsonToTerritory()));\n",
    "        \n",
    "         PCollectionTuple joinSources = PCollectionTuple\n",
    "                                        .of(new TupleTag<>(\"regions\"), regions)\n",
    "                                        .and(new TupleTag<>(\"territories\"), territories);                                          \n",
    "                                                    \n",
    "\n",
    "\n",
    "        PCollection<Result> result = joinSources\n",
    "            .apply(SqlTransform.query(\"SELECT r.regionid AS regionid, r.regionname AS regionname, SUM(1) AS cnt FROM regions AS r JOIN territories AS t on t.regionid = r.regionid group by r.regionid, r.regionname\"))\n",
    "            .apply(Convert.fromRows(Result.class))\n",
    "        ;\n",
    "\n",
    "        result.apply(TextIO.<Result>writeCustomType().to(outputsPrefix).withFormatFunction(new SerializeResult()));\n",
    "        \n",
    "        p.run().waitUntilFinish();\n",
    "    }\n",
    "    \n",
    "    @DefaultCoder(AvroCoder.class)\n",
    "    @DefaultSchema(JavaFieldSchema.class)\n",
    "    static class Region {\n",
    "        Long regionid;\n",
    "        String regionname;\n",
    "        \n",
    "        Region() {}\n",
    "        \n",
    "        Region(Long regionid, String regionname) {\n",
    "            this.regionid = regionid;\n",
    "            this.regionname = regionname;\n",
    "        }\n",
    "        \n",
    "        @Override\n",
    "        public String toString() {\n",
    "            return String.format(\"(regionid = %d, regionname = %s)\", regionid, regionname);\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    static class SerializeRegion implements SerializableFunction<Region, String> {\n",
    "        @Override\n",
    "        public String apply(Region input) {\n",
    "          return input.toString();\n",
    "        }\n",
    "    }\n",
    "\n",
    "    static class CSVToRegion extends DoFn<String, Region> {\n",
    "        private static final Logger LOG = LoggerFactory.getLogger(CSVToRegion.class);\n",
    "\n",
    "        @ProcessElement\n",
    "        public void process(ProcessContext c) {\n",
    "            String[] columns = c.element().split(\",\");\n",
    "            try {\n",
    "                Long regionid = Long.parseLong(columns[0].trim());\n",
    "                String regionname = columns[1].trim();\n",
    "                c.output(new Region(regionid, regionname));\n",
    "            } catch (ArrayIndexOutOfBoundsException | NumberFormatException e) {\n",
    "                LOG.info(\"CSVToRegion: parse error on '\" + c.element() + \"': \" + e.getMessage());\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    @DefaultCoder(AvroCoder.class)\n",
    "    @DefaultSchema(JavaFieldSchema.class)\n",
    "    static class Territory {\n",
    "        Long territoryid;\n",
    "        String territoryname;\n",
    "        Long regionid;\n",
    "        \n",
    "        Territory() {}\n",
    "        \n",
    "        Territory(Long territoryid, String territoryname, Long regionid) {\n",
    "            this.territoryid = territoryid;\n",
    "            this.territoryname = territoryname;\n",
    "            this.regionid = regionid;\n",
    "        }\n",
    "        \n",
    "        @Override\n",
    "        public String toString() {\n",
    "            return String.format(\"(territoryid = %d, territoryname = %s, regionID = %d)\", territoryid, territoryname, regionid);\n",
    "        }\n",
    "        /*\n",
    "        @Override\n",
    "        public boolean equals (Object o) {\n",
    "            if (o == this)\n",
    "                return true;\n",
    "            return false;\n",
    "         }\n",
    "        */\n",
    "    }\n",
    "    \n",
    "    static class SerializeTerritory implements SerializableFunction<Territory, String> {\n",
    "        @Override\n",
    "        public String apply(Territory input) {\n",
    "          return input.toString();\n",
    "        }\n",
    "    }\n",
    "\n",
    "    static class JsonToTerritory extends DoFn<String, Territory> {\n",
    "        @ProcessElement\n",
    "        public void process(@Element String json, OutputReceiver<Territory> r) throws Exception {\n",
    "            Gson gson = new Gson();\n",
    "            Territory t = gson.fromJson(json, Territory.class);\n",
    "            r.output(t);\n",
    "        }\n",
    "    }\n",
    "     \n",
    "    @DefaultCoder(AvroCoder.class)\n",
    "    @DefaultSchema(JavaFieldSchema.class)\n",
    "    static class Result {\n",
    "        Long regionid;\n",
    "        String regionname;\n",
    "        int cnt;\n",
    "        \n",
    "        Result() {}\n",
    "        \n",
    "        Result(Long regionid, String regionname, int cnt) {\n",
    "            this.regionid = regionid;\n",
    "            this.regionname = regionname;\n",
    "            this.cnt = cnt;\n",
    "        }\n",
    "        \n",
    "        @Override\n",
    "        public String toString() {\n",
    "            return String.format(\"(regionid = %d, regionname = %s, cnt = %d)\", regionid, regionname, cnt);\n",
    "        }\n",
    "        /*\n",
    "        @Override\n",
    "        public boolean equals (Object o) {\n",
    "            if (o == this)\n",
    "                return true;\n",
    "            return false;\n",
    "         }\n",
    "        */\n",
    "    }\n",
    "    \n",
    "    static class SerializeResult implements SerializableFunction<Result, String> {\n",
    "        @Override\n",
    "        public String apply(Result input) {\n",
    "          return input.toString();\n",
    "        }\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640c41a3-cb72-403d-8b41-7309459b22ee",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Example from Beam documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f836c9-1a79-41b0-be02-d3d7fb5a2ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%java verbose\n",
    "package samples.quickstart;\n",
    "\n",
    "import org.apache.beam.sdk.Pipeline;\n",
    "import org.apache.beam.sdk.extensions.sql.SqlTransform;\n",
    "import org.apache.beam.sdk.options.PipelineOptions;\n",
    "import org.apache.beam.sdk.options.PipelineOptionsFactory;\n",
    "import org.apache.beam.sdk.schemas.Schema;\n",
    "import org.apache.beam.sdk.transforms.Create;\n",
    "import org.apache.beam.sdk.transforms.MapElements;\n",
    "import org.apache.beam.sdk.transforms.SimpleFunction;\n",
    "import org.apache.beam.sdk.values.PBegin;\n",
    "import org.apache.beam.sdk.values.PCollection;\n",
    "import org.apache.beam.sdk.values.PCollectionTuple;\n",
    "import org.apache.beam.sdk.values.Row;\n",
    "import org.apache.beam.sdk.values.TupleTag;\n",
    "\n",
    "/**\n",
    " * This is a quick example, which uses Beam SQL DSL to create a data pipeline.\n",
    " *\n",
    " * <p>Run the example from the Beam source root with\n",
    " *\n",
    " * <pre>\n",
    " *   ./gradlew :sdks:java:extensions:sql:runBasicExample\n",
    " * </pre>\n",
    " *\n",
    " * <p>The above command executes the example locally using direct runner. Running the pipeline in\n",
    " * other runners require additional setup and are out of scope of the SQL examples. Please consult\n",
    " * Beam documentation on how to run pipelines.\n",
    " */\n",
    "class BeamSqlExample {\n",
    "\n",
    "  public static void main(String[] args) {\n",
    "    PipelineOptions options = PipelineOptionsFactory.fromArgs(args).create();\n",
    "    Pipeline p = Pipeline.create(options);\n",
    "\n",
    "    // define the input row format\n",
    "    Schema type =\n",
    "        Schema.builder().addInt32Field(\"c1\").addStringField(\"c2\").addDoubleField(\"c3\").build();\n",
    "\n",
    "    Row row1 = Row.withSchema(type).addValues(1, \"row\", 1.0).build();\n",
    "    Row row2 = Row.withSchema(type).addValues(2, \"row\", 2.0).build();\n",
    "    Row row3 = Row.withSchema(type).addValues(3, \"row\", 3.0).build();\n",
    "\n",
    "    // create a source PCollection with Create.of();\n",
    "    PCollection<Row> inputTable =\n",
    "        PBegin.in(p).apply(Create.of(row1, row2, row3).withRowSchema(type));\n",
    "\n",
    "    // Case 1. run a simple SQL query over input PCollection with BeamSql.simpleQuery;\n",
    "    PCollection<Row> outputStream =\n",
    "        inputTable.apply(SqlTransform.query(\"select c1, c2, c3 from PCOLLECTION where c1 > 1\"));\n",
    "\n",
    "    // print the output record of case 1;\n",
    "    outputStream\n",
    "        .apply(\n",
    "            \"log_result\",\n",
    "            MapElements.via(\n",
    "                new SimpleFunction<Row, Row>() {\n",
    "                  @Override\n",
    "                  public Row apply(Row input) {\n",
    "                    // expect output:\n",
    "                    //  PCOLLECTION: [3, row, 3.0]\n",
    "                    //  PCOLLECTION: [2, row, 2.0]\n",
    "                    System.out.println(\"PCOLLECTION: \" + input.getValues());\n",
    "                    return input;\n",
    "                  }\n",
    "                }))\n",
    "        .setRowSchema(type);\n",
    "\n",
    "    // Case 2. run the query with SqlTransform.query over result PCollection of case 1.\n",
    "    PCollection<Row> outputStream2 =\n",
    "        PCollectionTuple.of(new TupleTag<>(\"CASE1_RESULT\"), outputStream)\n",
    "            .apply(SqlTransform.query(\"select c2, sum(c3) from CASE1_RESULT group by c2\"));\n",
    "\n",
    "    // print the output record of case 2;\n",
    "    outputStream2\n",
    "        .apply(\n",
    "            \"log_result\",\n",
    "            MapElements.via(\n",
    "                new SimpleFunction<Row, Row>() {\n",
    "                  @Override\n",
    "                  public Row apply(Row input) {\n",
    "                    // expect output:\n",
    "                    //  CASE1_RESULT: [row, 5.0]\n",
    "                    System.out.println(\"CASE1_RESULT: \" + input.getValues());\n",
    "                    return input;\n",
    "                  }\n",
    "                }))\n",
    "        .setRowSchema(\n",
    "            Schema.builder().addStringField(\"stringField\").addDoubleField(\"doubleField\").build());\n",
    "\n",
    "    p.run().waitUntilFinish();\n",
    "  }\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844a012b-77f4-4e56-be03-eb8ec0f50a1e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Beam SQL using Pojo into a Result Pojo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b385018-4d9a-4085-ab75-88d9021b7017",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%java \n",
    "package samples.quickstart;\n",
    "\n",
    "import org.apache.beam.sdk.Pipeline;\n",
    "import org.apache.beam.sdk.values.PCollection;\n",
    "import org.apache.beam.sdk.values.TypeDescriptors;\n",
    "import org.apache.beam.sdk.io.TextIO;\n",
    "import org.apache.beam.sdk.transforms.ParDo;\n",
    "import org.apache.beam.sdk.transforms.DoFn;\n",
    "import org.apache.beam.sdk.transforms.Filter;\n",
    "import org.apache.beam.sdk.transforms.DoFn.ProcessContext;\n",
    "import org.apache.beam.sdk.coders.DefaultCoder;\n",
    "import org.apache.beam.sdk.coders.AvroCoder;\n",
    "import org.apache.beam.sdk.transforms.SerializableFunction;\n",
    "import org.apache.beam.sdk.schemas.Schema;\n",
    "import org.apache.beam.sdk.schemas.Schema.FieldType;\n",
    "import org.apache.beam.sdk.values.Row;\n",
    "import org.apache.beam.sdk.extensions.sql.SqlTransform;\n",
    "import org.apache.beam.sdk.transforms.MapElements;\n",
    "import org.apache.beam.sdk.transforms.SimpleFunction;\n",
    "import org.apache.beam.sdk.schemas.AutoValueSchema;\n",
    "import org.apache.beam.sdk.schemas.annotations.DefaultSchema;\n",
    "import org.apache.beam.sdk.schemas.JavaFieldSchema;\n",
    "import org.apache.beam.sdk.schemas.annotations.SchemaCreate;\n",
    "import com.google.auto.value.AutoValue;\n",
    "import org.apache.beam.sdk.schemas.transforms.Convert;\n",
    "import com.google.gson.Gson;\n",
    "\n",
    "import org.slf4j.Logger;\n",
    "import org.slf4j.LoggerFactory;\n",
    "\n",
    "\n",
    "public class ReadTerritories {\n",
    "    public static void main(String[] args) {\n",
    "        System.getProperties().put(\"org.apache.commons.logging.simplelog.defaultlog\",\"fatal\");\n",
    "\n",
    "        Pipeline p = Pipeline.create();\n",
    "        p.getSchemaRegistry().registerPOJO(Result.class);\n",
    " \n",
    "        String territoriesInputFileName = \"datasets/northwind/JSON/territories/territories.json\";\n",
    "        String outputsPrefix = \"/tmp/outputs\";\n",
    "\n",
    "        // Define the schema to hold the results.\n",
    "        Schema resultSchema = Schema.of(\n",
    "            Schema.Field.of(\"regionid\", Schema.FieldType.INT64), \n",
    "            Schema.Field.of(\"cnt\", Schema.FieldType.INT64));\n",
    "\n",
    "        PCollection<Result> result = p\n",
    "            .apply(\"Read\", TextIO.read().from(territoriesInputFileName))\n",
    "            .apply(\"Parse\", ParDo.of(new JsonToTerritory()))\n",
    "            .apply(SqlTransform.query(\"SELECT regionid, COUNT(*) as cnt FROM PCOLLECTION GROUP BY regionid\"))\n",
    "            .apply(Convert.fromRows(Result.class))\n",
    "        ;\n",
    "        \n",
    "        result.apply(TextIO.<Result>writeCustomType().to(outputsPrefix).withFormatFunction(new SerializeResult()));\n",
    "        \n",
    "        p.run().waitUntilFinish();\n",
    "    }\n",
    "    \n",
    "\n",
    "    @DefaultSchema(JavaFieldSchema.class)\n",
    "    static class Territory {\n",
    "        Long territoryid;\n",
    "        String territoryname;\n",
    "        Long regionid;\n",
    "        \n",
    "        Territory() {}\n",
    "        \n",
    "        Territory(long territoryid, String territoryname, long regionid) {\n",
    "            this.territoryid = territoryid;\n",
    "            this.territoryname = territoryname;\n",
    "            this.regionid = regionid;\n",
    "        }\n",
    "        \n",
    "        @Override\n",
    "        public String toString() {\n",
    "            return String.format(\"(territoryID = %d, territoryName = %s, regionID = %d)\", territoryid, territoryname, regionid);\n",
    "        }\n",
    "\n",
    "    }\n",
    "    \n",
    "    static class SerializeTerritory implements SerializableFunction<Territory, String> {\n",
    "        @Override\n",
    "        public String apply(Territory input) {\n",
    "          return input.toString();\n",
    "        }\n",
    "    }\n",
    "\n",
    "    static class JsonToTerritory extends DoFn<String, Territory> {\n",
    "        @ProcessElement\n",
    "        public void process(@Element String json, OutputReceiver<Territory> r) throws Exception {\n",
    "            Gson gson = new Gson();\n",
    "            Territory t = gson.fromJson(json, Territory.class);\n",
    "            r.output(t);\n",
    "        }\n",
    "    }\n",
    "     \n",
    "    @DefaultSchema(JavaFieldSchema.class)\n",
    "    static class Result {\n",
    "        Long regionid;\n",
    "        Long cnt;\n",
    "        \n",
    "        Result() {}\n",
    "        \n",
    "        Result(Long regionid, Long cnt) {\n",
    "            this.regionid = regionid;\n",
    "            this.cnt = cnt;\n",
    "        }\n",
    "        \n",
    "        @Override\n",
    "        public String toString() {\n",
    "            return String.format(\"(regionid = %d, cnt = %d)\", regionid, cnt);\n",
    "        }\n",
    "        @Override\n",
    "        public boolean equals (Object o) {\n",
    "            if (o == this)\n",
    "                return true;\n",
    "            return false;\n",
    "         }\n",
    "    }\n",
    "    \n",
    "    static class SerializeResult implements SerializableFunction<Result, String> {\n",
    "        @Override\n",
    "        public String apply(Result input) {\n",
    "          return input.toString();\n",
    "        }\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5490fb-33e0-4949-9f1a-45c5d4254aad",
   "metadata": {
    "tags": []
   },
   "source": [
    "### BeamSQL Java working wrong way with schemas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb24f9f2-4b5f-47a8-9718-8b553c2814fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%java verbose nooutput\n",
    "package samples.quickstart;\n",
    "\n",
    "import org.apache.beam.sdk.Pipeline;\n",
    "import org.apache.beam.sdk.values.PCollection;\n",
    "import org.apache.beam.sdk.values.TypeDescriptors;\n",
    "import org.apache.beam.sdk.io.TextIO;\n",
    "import org.apache.beam.sdk.transforms.ParDo;\n",
    "import org.apache.beam.sdk.transforms.DoFn;\n",
    "import org.apache.beam.sdk.transforms.Filter;\n",
    "import org.apache.beam.sdk.transforms.DoFn.ProcessContext;\n",
    "import org.apache.beam.sdk.coders.DefaultCoder;\n",
    "import org.apache.beam.sdk.coders.AvroCoder;\n",
    "import org.apache.beam.sdk.transforms.SerializableFunction;\n",
    "import org.apache.beam.sdk.schemas.Schema;\n",
    "import org.apache.beam.sdk.schemas.Schema.FieldType;\n",
    "import org.apache.beam.sdk.values.Row;\n",
    "import org.apache.beam.sdk.extensions.sql.SqlTransform;\n",
    "import org.apache.beam.sdk.transforms.MapElements;\n",
    "import org.apache.beam.sdk.transforms.SimpleFunction;\n",
    "import org.apache.beam.sdk.schemas.AutoValueSchema;\n",
    "import org.apache.beam.sdk.schemas.annotations.DefaultSchema;\n",
    "import org.apache.beam.sdk.schemas.annotations.SchemaCreate;\n",
    "import com.google.auto.value.AutoValue;\n",
    "import org.apache.beam.sdk.schemas.transforms.Convert;\n",
    "\n",
    "import org.slf4j.Logger;\n",
    "import org.slf4j.LoggerFactory;\n",
    "\n",
    "public class ReadTerritories {\n",
    "    public static void main(String[] args) {\n",
    "        Pipeline p = Pipeline.create();\n",
    "\n",
    "        String territoriesInputFileName = \"datasets/northwind/CSV/territories/territories.csv\";\n",
    "        String outputsPrefix = \"/tmp/outputs\";\n",
    "\n",
    "        PCollection<Territory> territories = p\n",
    "            .apply(\"Read\", TextIO.read().from(territoriesInputFileName))\n",
    "            .apply(\"Parse\", ParDo.of(new ParseTerritories()))\n",
    "        ;                   \n",
    "        \n",
    "        // Define the schema for the records.\n",
    "        Schema territorySchema = Schema\n",
    "          .builder()\n",
    "          .addInt64Field(\"territoryID\")\n",
    "          .addStringField(\"territoryName\")\n",
    "          .addInt64Field(\"regionID\")\n",
    "          .build();\n",
    "        // Define the schema to hold the results.\n",
    "        \n",
    "        Schema resultSchema = Schema.of(\n",
    "            Schema.Field.of(\"regionID\", Schema.FieldType.INT64), \n",
    "            Schema.Field.of(\"cnt\", Schema.FieldType.INT64));\n",
    "        \n",
    "        // Convert them to Rows with the same schema as defined above via a DoFn.\n",
    "        PCollection<Row> territories2 = territories\n",
    "          .apply(\n",
    "          ParDo.of(new DoFn<Territory, Row>() {\n",
    "            @ProcessElement\n",
    "            public void process(ProcessContext c) {\n",
    "              // Get the current POJO instance\n",
    "              Territory t = c.element();\n",
    "\n",
    "              // Create a Row with the appSchema schema\n",
    "              // and values from the current POJO\n",
    "              Row territoryRow =\n",
    "                    Row\n",
    "                      .withSchema(territorySchema)\n",
    "                      .addValues(\n",
    "                        t.territoryID,\n",
    "                        t.territoryName,\n",
    "                        t.regionID)\n",
    "                      .build();\n",
    "\n",
    "              // Output the Row representing the current POJO\n",
    "              c.output(territoryRow);\n",
    "            }\n",
    "          })).setRowSchema(territorySchema);\n",
    "        \n",
    "          PCollection<Row> territories3 = territories2.apply(Convert.toRows()).apply(\n",
    "             SqlTransform.query(\"SELECT regionID, COUNT(*) as cnt from PCOLLECTION GROUP BY regionID\")).setRowSchema(resultSchema);\n",
    "        \n",
    "          territories3.apply(\n",
    "              \"Print\", MapElements.via(new SimpleFunction<Row, Row>() {\n",
    "                  @Override\n",
    "                  public Row apply(Row input) {\n",
    "                      System.out.println(\"SQL Result: \" + input.getValues());\n",
    "                      return input;\n",
    "                  }\n",
    "              }\n",
    "          )).setRowSchema(resultSchema);\n",
    "//        territories3.apply(TextIO.<Row>writeCustomType().to(outputsPrefix).withFormatFunction(new SerializeTerritory()));\n",
    "        p.run().waitUntilFinish();\n",
    "    }\n",
    "\n",
    "/*    \n",
    "    @schemultSchema(AutoValueSchema.class)\n",
    "    @AutoValue\n",
    "    public static abstract class Territory {\n",
    "      public abstract Long getTerritoryID();\n",
    "      public abstract String getTerritoryName();\n",
    "      public abstract Long getRegionID();\n",
    "\n",
    "      @SchemaCreate\n",
    "      public static Territory create(Long territoryID, String territoryName, Long regionID) {\n",
    "        return new AutoValue_TerritoryClass(territoryID, territoryName, regionID);\n",
    "      }\n",
    "*/    \n",
    "    \n",
    "    @DefaultCoder(AvroCoder.class)\n",
    "    static class Territory {\n",
    "        Long territoryID;\n",
    "        String territoryName;\n",
    "        Long regionID;\n",
    "        \n",
    "        Territory() {}\n",
    "        \n",
    "        Territory(long territoryID, String territoryName, long regionID) {\n",
    "            this.territoryID = territoryID;\n",
    "            this.territoryName = territoryName;\n",
    "            this.regionID = regionID;\n",
    "        }\n",
    "        \n",
    "        @Override\n",
    "        public String toString() {\n",
    "            return String.format(\"(territoryID = %d, territoryName = %s, regionID = %d)\", territoryID, territoryName, regionID);\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    static class SerializeTerritory implements SerializableFunction<Territory, String> {\n",
    "        @Override\n",
    "        public String apply(Territory input) {\n",
    "          return input.toString();\n",
    "        }\n",
    "    }\n",
    "\n",
    "    static class ParseTerritories extends DoFn<String, Territory> {\n",
    "        private static final Logger LOG = LoggerFactory.getLogger(ParseTerritories.class);\n",
    "\n",
    "        @ProcessElement\n",
    "        public void process(ProcessContext c) {\n",
    "            String[] columns = c.element().split(\",\");\n",
    "            try {\n",
    "                Long territoryID = Long.parseLong(columns[0].trim());\n",
    "                String territoryName = columns[1].trim();\n",
    "                Long regionID = Long.parseLong(columns[2].trim());\n",
    "                c.output(new Territory(territoryID, territoryName, regionID));\n",
    "            } catch (ArrayIndexOutOfBoundsException | NumberFormatException e) {\n",
    "                LOG.info(\"ParseTerritories: parse error on '\" + c.element() + \"': \" + e.getMessage());\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "\n",
    "    \n",
    "     \n",
    "/*    \n",
    "    \n",
    "    \n",
    "    @DefaultCoder(AvroCoder.class)\n",
    "    static class Region {\n",
    "        Long regionID;\n",
    "        Long cnt regionName;\n",
    "        \n",
    "        Region() {}\n",
    "        \n",
    "        Region(long regionID, long cnt) {\n",
    "            this.regionID = regionID;\n",
    "            this.cnt = cnt;\n",
    "        }\n",
    "        \n",
    "        @Override\n",
    "        public String toString() {\n",
    "            return String.format(\"(regionID = %d, cnt = %d)\", regionID, cnt);\n",
    "        }\n",
    "\n",
    "    }\n",
    "    \n",
    "    static class SerializeTerritory implements SerializableFunction<Region, String> {\n",
    "        @Override\n",
    "        public String apply(Region input) {\n",
    "          return input.toString();\n",
    "        }\n",
    "    }\n",
    "\n",
    "    \n",
    "    \n",
    "private class Transform extends PTransform<pcollectionlist<row>, PCollection<row>> {\n",
    " \n",
    "    @Override\n",
    "    public PCollection<row> expand(PCollectionList<row> pinput) {\n",
    "      checkArgument(\n",
    "          pinput.size() == 1,\n",
    "          \"Wrong number of inputs for %s: %s\",\n",
    "          BeamUncollectRel.class.getSimpleName(),\n",
    "          pinput);\n",
    "      PCollection<row> upstream = pinput.get(0);\n",
    " \n",
    "      // Each row of the input contains a single array of things to be emitted; Calcite knows\n",
    "      // what the row looks like\n",
    "      Schema outputSchema = CalciteUtils.toSchema(getRowType());\n",
    " \n",
    "      PCollection<row> uncollected =\n",
    "          upstream.apply(ParDo.of(new UncollectDoFn(outputSchema))).setRowSchema(outputSchema);\n",
    " \n",
    "      return uncollected;\n",
    "    }\n",
    "  }    \n",
    "    static class ParseRegions extends DoFn<Row, Region> {\n",
    "        private static final Logger LOG = LoggerFactory.getLogger(ParseTerritories.class);\n",
    "\n",
    "        @ProcessElement\n",
    "        public void process(ProcessContext c) {\n",
    "            \n",
    "            String[] columns = c.element().split(\",\");\n",
    "            try {\n",
    "                Long territoryID = Long.parseLong(columns[0].trim());\n",
    "                String territoryName = columns[1].trim();\n",
    "                Long regionID = Long.parseLong(columns[2].trim());\n",
    "                c.output(new Territory(territoryID, territoryName, regionID));\n",
    "            } catch (ArrayIndexOutOfBoundsException | NumberFormatException e) {\n",
    "                LOG.info(\"ParseTerritories: parse error on '\" + c.element() + \"': \" + e.getMessage());\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    \n",
    "    \n",
    "*/\n",
    "    \n",
    "    \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66766ce8-77db-471d-8938-f6bfada4c0ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fdd2ffb3-9158-4ae6-9c8e-999f9a0ab3b8",
   "metadata": {},
   "source": [
    "# __ __ __ __ __ __ __ __ __ __ __ __"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f139a1-db8c-41b5-9881-1a5eb2570f5d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# 6. <font color='green' size=\"+2\">DoFn</font> Lifecycle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0cb066-030c-4846-a446-eeada7ddd4af",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <img src=\"images/python.png\" width=40 height=40 /><font color='cadetblue' size=\"+2\">Python</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8143846d-38ce-438a-bd4d-85e0838f716f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### <font color='green' size=\"+2\">DoFn</font> Lifecycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3951161-ad01-4a5b-aee3-9e3b220883d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam.pvalue import AsSingleton, AsDict\n",
    "from apache_beam.io import ReadFromText\n",
    "\n",
    "class TerritoryParseTuple(beam.DoFn):\n",
    "    # split territory into KV pair of (regionid, (territoryid, territoryname))\n",
    "    def process(self, element):\n",
    "        territoryid, territoryname, regionid = element.split(',')\n",
    "        yield(int(territoryid), territoryname, int(regionid))\n",
    "        \n",
    "                \n",
    "class LookupRegion(beam.DoFn):\n",
    "    def setup(self):\n",
    "        self.lookup = {1:'North', 2:'South', 3:'East', 4:'West'}\n",
    "        print('setup')\n",
    "        \n",
    "    def start_bundle(self):\n",
    "        print('start bundle')\n",
    "        \n",
    "    def process(self, element, uppercase = 0):\n",
    "        #lookuptable = {1:'North', 2:'South', 3:'East', 4:'West'}\n",
    "        territoryid, territoryname, regionid = element\n",
    "        region = self.lookup.get(regionid, 'No Region')\n",
    "        if uppercase == 1:\n",
    "            region = region.upper()\n",
    "        yield(territoryid, territoryname, regionid, region)\n",
    "        \n",
    "    def finish_bundle(self):\n",
    "        print('finish bundle')\n",
    "\n",
    "    def teardown(self):\n",
    "        print('teardown')\n",
    "        del self.lookup\n",
    "    \n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "    territories =  (\n",
    "        p | 'Read Territories' >> ReadFromText('territories.csv')\n",
    "          | 'Parse Territories' >> beam.ParDo(TerritoryParseTuple())\n",
    "    )\n",
    "    \n",
    "    lookup = (\n",
    "        territories\n",
    "        | beam.ParDo(LookupRegion(), uppercase = 1 ) \n",
    "        | 'Print Loopup' >> beam.Map(print)\n",
    "    )\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13c622d-8a76-41da-afb2-60258b499577",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "89ac07ff-2799-4372-8e75-9c35d849a6d6",
   "metadata": {},
   "source": [
    "# __ __ __ __ __ __ __ __ __ __ __ __"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e93fc8-c37b-4f8c-b56e-8e114161ef10",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# 7. Side Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58cfa540-e59d-44b2-a561-84d5142cf63e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## <img src=\"images/python.png\" width=40 height=40 /><font color='cadetblue' size=\"+2\">Python</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8562667-e840-48c5-a9e3-db3432ae2d4a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Side inputs are about passing extra parameters to a function where the parameters are calculated in the pipeline itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28641185",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam.pvalue import AsSingleton, AsDict\n",
    "from apache_beam.io import ReadFromText\n",
    "from apache_beam.transforms.combiners import Sample\n",
    "\n",
    "class TerritoryParseTuple(beam.DoFn):\n",
    "    # split territory into KV pair of (regionid, (territoryid, territoryname))\n",
    "    def process(self, element, uppercase = '0'):\n",
    "        # It's a bit weird here but what is passed in is a single element array of a string\n",
    "        #print('***', uppercase)\n",
    "        territoryid, territoryname, regionid = element.split(',')\n",
    "        yield(int(territoryid), territoryname if uppercase[0] == '0' else territoryname.upper(), int(regionid))\n",
    "\n",
    "        \n",
    "with beam.Pipeline() as p:\n",
    "    sideinput = (\n",
    "        p | 'Read sideinput.txt' >> ReadFromText('sideinput.txt')\n",
    "          | Sample.FixedSizeGlobally(1)\n",
    "    )\n",
    "    \n",
    "    territories =  (\n",
    "        p | 'Read Territories' >> ReadFromText('territories.csv')\n",
    "          | 'Parse Territories' >> beam.ParDo(TerritoryParseTuple(), uppercase = [\"0\"]) # This is not a side input but just passing a fixed parameter\n",
    "#          | 'Parse Territories' >> beam.ParDo(TerritoryParseTuple(), uppercase = sideinput)  # fails because sideinput is a PCollection not an integer\n",
    "#          | 'Parse Territories' >> beam.ParDo(TerritoryParseTuple(), uppercase = beam.pvalue.AsSingleton(sideinput))  # When the parameter is calculated in the pipeline itself, that makes it a side input\n",
    "          | 'Print Loopup' >> beam.Map(print)\n",
    "    )\n",
    "\n",
    "#    maxregion | 'Print Min' >> beam.Map(print)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9668295-b173-48d2-8868-cc72fbaa6927",
   "metadata": {},
   "source": [
    "### Side input that is a lookup list.\n",
    "### More realistic example where the entire lookup table is read in the pipeline then distributed to each worker as a side input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20edebef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "from apache_beam.pvalue import AsList\n",
    "from apache_beam.io import ReadFromText\n",
    "\n",
    "class RegionParseDict(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        regionid, regionname = element.split(',')\n",
    "        yield {'regionid': int(regionid), 'regionname': regionname.title()}\n",
    "\n",
    "class TerritoryParseTuple(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        territoryid, territoryname, regionid = element.split(',')\n",
    "        yield(int(territoryid), territoryname, int(regionid))\n",
    "        \n",
    "                \n",
    "class LookupRegion(beam.DoFn):\n",
    "    def process(self, element, lookuptable = [{'regionid':1, 'regionname':'North'}, {'regionid':2, 'regionname':'South'}]):\n",
    "        # {1:'North', 2:'South'}\n",
    "        territoryid, territoryname, regionid = element\n",
    "        # Becase the regions PCollection is a different shape, use the following comprehension to make it easier to do a lookup\n",
    "        lookup = {e['regionid'] : e['regionname'] for e in lookuptable } # {1:'North', 2:'South'}\n",
    "        yield(territoryid, territoryname, regionid, lookup.get(regionid, 'No Region'))\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "    regions = (\n",
    "        p | 'Read Regions' >> ReadFromText('regions.csv')\n",
    "          | 'Parse Regions' >> beam.ParDo(RegionParseDict())\n",
    "#          | 'Print Regions' >> beam.Map(print)\n",
    "    )\n",
    "\n",
    "    territories =  (\n",
    "        p | 'Read Territories' >> ReadFromText('territories.csv')\n",
    "          | 'Parse Territories' >> beam.ParDo(TerritoryParseTuple())\n",
    "#          | 'Print Territories' >> beam.Map(print)\n",
    "    )\n",
    "    \n",
    "    lookup = (\n",
    "        territories\n",
    "        | beam.ParDo(LookupRegion(), lookuptable = beam.pvalue.AsList(regions))\n",
    "        | 'Print Loopup' >> beam.Map(print)\n",
    "    )\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9be7e3-8313-44d5-a323-e361227209ab",
   "metadata": {},
   "source": [
    "# __ __ __ __ __ __ __ __ __ __ __ __"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214bd2fa-a669-4725-b831-b415c86b93d6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 8. Streaming Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3af94d-94f6-4e50-98ca-62bd4cbfc968",
   "metadata": {},
   "source": [
    "## Streaming sources in Beam are not too different than they are in Spark. Running a pipeline with streaming sources is tricky to do because the direct runner does not support running them. Instead, you need to submit jobs to a runner like Flink or Spark or Google DataFlow. For the rest of the code examples, we won't be able to run them here because the environment is not set up to allow it. However, we can use a bounded data source to simulate what we would see."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62c90d3-9590-4087-bcfd-7b3c02303854",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <img src=\"images/python.png\" width=40 height=40 /><font color='cadetblue' size=\"+2\">Python</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a806643-e93b-47f0-aa66-5004df77976b",
   "metadata": {},
   "source": [
    "### We need the correct packages installed. This is already done on this machine, so these are for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e54856-d983-4bfe-9ed1-0740e9f6e5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install apache-beam\n",
    "! pip install apache-beam[gcp]\n",
    "! pip install apache-beam[interactive]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d893a2d4-33fc-4b4d-b3c2-a9b04c318b93",
   "metadata": {
    "tags": []
   },
   "source": [
    "## A basic example of reading from a Kafka streaming source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5d3f0f-4649-438d-859c-7385571efa4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "import apache_beam.transforms.window as window\n",
    "from apache_beam.io.external.kafka import ReadFromKafka, WriteToKafka\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "import apache_beam.transforms.window as window\n",
    "\n",
    "brokers = 'localhost:9092'\n",
    "\n",
    "kafka_config = {\n",
    "                  'bootstrap.servers': brokers\n",
    "                }\n",
    "\n",
    "pipeline_options = PipelineOptions(streaming = True)\n",
    "kafka_topic = 'stocks-json'\n",
    "\n",
    "'''\n",
    "These are a sample of some of the types of options we would have to send\n",
    "     runner = \"DirectRunner\"\n",
    "     runner = \"SparkRunner\"\n",
    "     runner = \"FlinkRunner\"\n",
    "     , flink_master=\"localhost:8081\"\n",
    "     , environment_type=\"LOOPBACK\"\n",
    "     , streaming=\"true\"\n",
    "     , checkpointing_interval=1000\n",
    "     , environment_type = \"DOCKER\"\n",
    "'''\n",
    "\n",
    "with beam.Pipeline(options = pipeline_options) as p:\n",
    "    k = ( p\n",
    "          | 'Read from Kafka' >> ReadFromKafka(consumer_config = kafka_config, topics=[kafka_topic]) \n",
    "          | 'Window of 10 seconds' >> beam.WindowInto(window.FixedWindows(10))\n",
    "          | 'Print' >> beam.Map(print)\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd805ef0-3a8b-4f71-80c0-d36a3c5ec035",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "import apache_beam.transforms.window as window\n",
    "from apache_beam.io.external.kafka import ReadFromKafka, WriteToKafka\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "\n",
    "kafka_config = {\n",
    "                  'bootstrap.servers': brokers\n",
    "                }\n",
    "\n",
    "pipeline_options = PipelineOptions(streaming = True)\n",
    "kafka_tope = 'stocks-json'\n",
    "\n",
    "def convert_kafka_record_to_dictionary(record):\n",
    "    # the records have 'value' attribute when --with_metadata is given\n",
    "    if hasattr(record, 'value'):\n",
    "      stock_bytes = record.value\n",
    "    elif isinstance(record, tuple):\n",
    "      stock_bytes = record[1]\n",
    "    else:\n",
    "      raise RuntimeError('unknown record type: %s' % type(record))\n",
    "    # Converting bytes record from Kafka to a dictionary.\n",
    "    import ast\n",
    "    stock = ast.literal_eval(stock_bytes.decode(\"UTF-8\"))\n",
    "    output = {\n",
    "        key: stock[key]\n",
    "        for key in ['timestamp', 'symbol', 'price']\n",
    "    }\n",
    "    if hasattr(record, 'timestamp'):\n",
    "      # timestamp is read from Kafka metadata\n",
    "      output['timestamp'] = record.timestamp\n",
    "    print (record, output)\n",
    "    return output\n",
    "\n",
    "\n",
    "with beam.Pipeline(options = pipeline_options) as p:\n",
    "    k = ( p\n",
    "          | 'Read from Kafka' >> ReadFromKafka(consumer_config = kafka_config, topics=[kafka_topic]) \n",
    "          | 'Convert Message' >> beam.Map(convert_kafka_record_to_dictionary)\n",
    "          | 'Window' >> beam.WindowInto(FixedWindows(10))\n",
    "#          | beam.Map(lambda x : ('x', 1))\n",
    "          | beam.CombinePerKey(sum)          \n",
    "          | 'Print' >> beam.Map(print)\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4943add-c45a-4053-971d-c247d8b6ecd1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Some examples of what we might do once we start reading the messages are similar to what we'd do in Spark. We'd have to convert the message into a usable structed object shape, then do whatever filtering, aggregating, windowing, etc. we need, and then write the final data out to some sink or destination, like another message in Kafka or PubSub, or write it to a database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64124ccf-f4d2-4130-bdb5-53a3548b6295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test code to convert a sample message into a structured object\n",
    "from typing import NamedTuple\n",
    "import uuid\n",
    "\n",
    "\n",
    "class Trade(NamedTuple):\n",
    "    key: str\n",
    "    timestamp: int\n",
    "    symbol: str\n",
    "    event_time: float\n",
    "    price: float\n",
    "    quantity: int\n",
    "    \n",
    "    \n",
    "def convert_dict_to_stock(record: dict) -> Trade:\n",
    "    return Trade(**record)\n",
    "\n",
    "sample_message = { \"key\": \"6a826a01-4b23-4713-8a3e-f8f2d720ba07\"\n",
    "    , \"timestamp\": 1645290182886\n",
    "    , \"event_time\": \"2022-02-19 17:00:42\"\n",
    "    , \"symbol\": \"MSFT\"\n",
    "    , \"price\": 154.49\n",
    "    , \"quantity\": 164}\n",
    "\n",
    "print(convert_dict_to_stock(sample_message))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5d3a10-ce0b-46e6-ad9d-8fc083244545",
   "metadata": {},
   "source": [
    "## Let's read data from a file simulating like it's from a streaming source, so we can see how to convert it and aggregate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2057b3c4-bfd2-4ee3-88d3-a085a23344ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "import json\n",
    "from apache_beam.io import ReadFromText, WriteToText\n",
    "#from apache_beam.combiners import Sample\n",
    "\n",
    "class Trade(NamedTuple):\n",
    "    key: str\n",
    "    timestamp: int\n",
    "    symbol: str\n",
    "    event_time: float\n",
    "    price: float\n",
    "    quantity: int\n",
    "    \n",
    "def convert_dict_to_stock(record):\n",
    "    x = json.loads(record)\n",
    "    msg = x['value']\n",
    "    msg['key'] = x['key']\n",
    "    msg['timestamp'] = x['timestamp']\n",
    "    return Trade(**msg)\n",
    "\n",
    "stocks_filename = 'trades.txt'\n",
    "with beam.Pipeline() as p:\n",
    "    k = ( p\n",
    "          | 'Read from Kafka' >> ReadFromText(stocks_filename)\n",
    "         # | 'Convert message to a dictionary' >> beam.Map(convert_kafka_record_to_dictionary)\n",
    "          | 'Convert to Trade Object' >> beam.Map(convert_dict_to_stock)\n",
    "          | 'MSFT trades' >> beam.Filter(lambda t : t.symbol == 'MSFT')\n",
    "          | 'Limit 10' >> beam.combiners.Sample.FixedSizeGlobally(10)\n",
    "          | 'Print' >> beam.Map(print)\n",
    "        )\n",
    "\n",
    "# Trade(key='de76c3b3-7b4c-4fcb-a4ff-ab4cca374f9d', timestamp=1645290182886, symbol='AAPL', event_time='2022-02-19 17:37:13', price=266.92, quantity=143)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375f292c-265e-49ee-8af3-b6b6cca1b331",
   "metadata": {},
   "source": [
    "## This is a really simple aggregate that sums up the total number of trades for each stock for the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60acef2-b3f5-4096-a7d7-f0446ec03bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "import json\n",
    "from apache_beam.io import ReadFromText, WriteToText\n",
    "\n",
    "class Trade(NamedTuple):\n",
    "    key: str\n",
    "    timestamp: int\n",
    "    symbol: str\n",
    "    event_time: float\n",
    "    price: float\n",
    "    quantity: int\n",
    "    \n",
    "def convert_dict_to_stock(record):\n",
    "    x = json.loads(record)\n",
    "    msg = x['value']\n",
    "    msg['key'] = x['key']\n",
    "    msg['timestamp'] = x['timestamp']\n",
    "    return Trade(**msg)\n",
    "\n",
    "stocks_filename = 'trades.txt'\n",
    "with beam.Pipeline() as p:\n",
    "    k = ( p\n",
    "          | 'Read from Kafka' >> ReadFromText(stocks_filename)\n",
    "          | 'Convert to Trade Object' >> beam.Map(convert_dict_to_stock)\n",
    "          | 'Aggregate 1' >> beam.GroupBy('symbol').aggregate_field('quantity', sum, 'total_quantity')\n",
    "          | 'Print' >> beam.Map(print)\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128e0a15-4d71-41ce-acb1-e502baf7b150",
   "metadata": {},
   "source": [
    "## Now let's introduce windowing to do micro-aggregations on the data for a fixed window. In this case, we will do 1 day (60 seconds * 60 minutes * 24 hours)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c9aae2-e73b-4ea6-8055-9e93e0774a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from apache_beam import window\n",
    "\n",
    "import apache_beam as beam\n",
    "import json\n",
    "from apache_beam.io import ReadFromText, WriteToText\n",
    "from apache_beam.transforms.window import FixedWindows\n",
    "\n",
    "class Trade(NamedTuple):\n",
    "    key: str\n",
    "    timestamp: int\n",
    "    symbol: str\n",
    "    event_time: float\n",
    "    price: float\n",
    "    quantity: int\n",
    "    \n",
    "def convert_dict_to_stock(record):\n",
    "    x = json.loads(record)\n",
    "    msg = x['value']\n",
    "    msg['key'] = x['key']\n",
    "    msg['timestamp'] = x['timestamp']\n",
    "    return Trade(**msg)\n",
    "\n",
    "def add_timestamp(element):\n",
    "    ## This adds an element to the PCollection to be used for determining which window an item falls into\n",
    "    unix_timestamp = element.timestamp\n",
    "    event_time = element.event_time\n",
    "    return beam.window.TimestampedValue(element, unix_timestamp)\n",
    "\n",
    "stocks_filename = 'trades.txt'\n",
    "with beam.Pipeline() as p:\n",
    "    k = ( p\n",
    "          | 'Read from Kafka' >> ReadFromText(stocks_filename)\n",
    "          | 'Convert to Trade Object' >> beam.Map(convert_dict_to_stock)\n",
    "          | 'Timestamp the Trade' >> beam.Map(add_timestamp)\n",
    "          | 'Window' >> beam.WindowInto(FixedWindows(60 * 60 * 24))\n",
    "          | 'Aggregate 1' >> beam.GroupBy('symbol').aggregate_field('quantity', sum, 'total_quantity')\n",
    "          | 'Print' >> beam.Map(print)\n",
    "        )\n",
    "\n",
    "# fixed_windowed_items = (\n",
    "#     timestamped_items = items | 'timestamp' >> beam.ParDo(AddTimestampDoFn())\n",
    "#     items | 'window' >> beam.WindowInto(window.FixedWindows(60)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef51b63-7551-4ae1-8330-80fdece638e5",
   "metadata": {},
   "source": [
    "## It would be nice to see the time ranges that these windows represent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29817534-9109-4e72-8113-32998af60a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we did the same thing but instead the Trade class has event_time as Python datetime datatype\n",
    "from apache_beam import window\n",
    "from apache_beam.transforms.combiners import Sample\n",
    "\n",
    "import apache_beam as beam\n",
    "import json\n",
    "from apache_beam.io import ReadFromText, WriteToText\n",
    "from apache_beam.transforms.window import FixedWindows\n",
    "from datetime import datetime\n",
    "\n",
    "class Trade(NamedTuple):\n",
    "    key: str\n",
    "    timestamp: float\n",
    "    symbol: str\n",
    "    event_time: datetime\n",
    "    price: float\n",
    "    quantity: int\n",
    "beam.coders.registry.register_coder(Trade, beam.coders.RowCoder)\n",
    "    \n",
    "def convert_dict_to_stock(record):\n",
    "    x = json.loads(record)\n",
    "    msg = x['value']\n",
    "    msg['key'] = x['key']\n",
    "    msg['timestamp'] = x['timestamp']\n",
    "    return Trade(**msg)\n",
    "    \n",
    "@beam.typehints.with_output_types(Trade)\n",
    "class ParseStockMessage(beam.DoFn):\n",
    "    def process(self, record):\n",
    "        x = json.loads(record)\n",
    "        msg = x['value']\n",
    "        msg['event_time'] = datetime.strptime(msg['event_time'], \"%Y-%m-%d %H:%M:%S\")\n",
    "        msg['key'] = x['key']\n",
    "        msg['timestamp'] = x['timestamp']\n",
    "        t = Trade(**msg)\n",
    "        yield t\n",
    "        \n",
    "def add_timestamp(element):\n",
    "    unix_timestamp = element.timestamp\n",
    "    event_time = element.event_time\n",
    "    return beam.window.TimestampedValue(element, unix_timestamp)\n",
    "        \n",
    "class TradeAggregate(NamedTuple):\n",
    "    window_start: datetime\n",
    "    window_end: datetime\n",
    "    symbol: str\n",
    "    total_quantity: int\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f'{self.window_start.strftime(\"%Y-%m-%d %H:%M:%S\")} - {self.window_end.strftime(\"%Y-%m-%d %H:%M:%S\")} {self.symbol} {self.total_quantity}'\n",
    "\n",
    "beam.coders.registry.register_coder(TradeAggregate, beam.coders.RowCoder)\n",
    "\n",
    "@beam.typehints.with_output_types(TradeAggregate)\n",
    "class AddWindowRange(beam.DoFn):\n",
    "    def process(self, element,  window=beam.DoFn.WindowParam):\n",
    "        window_start = window.start.to_utc_datetime()\n",
    "        window_end = window.end.to_utc_datetime()\n",
    "        yield TradeAggregate(window_start, window_end, element.symbol, element.total_quantity)\n",
    "        \n",
    "        \n",
    "stocks_filename = 'trades.txt'\n",
    "with beam.Pipeline() as p:\n",
    "    k = ( p\n",
    "          | 'Read from Kafka' >> ReadFromText(stocks_filename)\n",
    "          | 'Convert to Trade Object' >> beam.Map(convert_dict_to_stock)\n",
    "          | 'Timestamp the Trade' >> beam.Map(add_timestamp)\n",
    "          | 'Window' >> beam.WindowInto(FixedWindows(60 * 60 * 24))\n",
    "          | 'Aggregate 1' >> beam.GroupBy('symbol').aggregate_field('quantity', sum, 'total_quantity')\n",
    "          | 'AddWindowEndTimestamp' >> (beam.ParDo(AddWindowRange()))\n",
    "          | 'Print' >> beam.Map(print)\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c2b0a8-94f9-4838-b5e8-ed0bb4a0fb93",
   "metadata": {},
   "source": [
    "## Try a sliding window this time doing a 24-hour period updated hourly. Here we are also introducing a trick useful for debugging which is to convert a Beam PCollection into a Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc88ad10-2c08-4d34-8e3d-372d3160c1f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <link rel=\"stylesheet\" href=\"https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css\" integrity=\"sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh\" crossorigin=\"anonymous\">\n",
       "            <div id=\"progress_indicator_b45ecfa17a84a3e7f5e8c587fd5fd7bb\" class=\"spinner-border text-info\" role=\"status\">\n",
       "            </div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (typeof window.interactive_beam_jquery == 'undefined') {\n",
       "          var jqueryScript = document.createElement('script');\n",
       "          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n",
       "          jqueryScript.type = 'text/javascript';\n",
       "          jqueryScript.onload = function() {\n",
       "            var datatableScript = document.createElement('script');\n",
       "            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n",
       "            datatableScript.type = 'text/javascript';\n",
       "            datatableScript.onload = function() {\n",
       "              window.interactive_beam_jquery = jQuery.noConflict(true);\n",
       "              window.interactive_beam_jquery(document).ready(function($){\n",
       "                \n",
       "            $(\"#progress_indicator_b45ecfa17a84a3e7f5e8c587fd5fd7bb\").remove();\n",
       "              });\n",
       "            }\n",
       "            document.head.appendChild(datatableScript);\n",
       "          };\n",
       "          document.head.appendChild(jqueryScript);\n",
       "        } else {\n",
       "          window.interactive_beam_jquery(document).ready(function($){\n",
       "            \n",
       "            $(\"#progress_indicator_b45ecfa17a84a3e7f5e8c587fd5fd7bb\").remove();\n",
       "          });\n",
       "        }"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>symbol</th>\n",
       "      <th>total_quantity</th>\n",
       "      <th>window_start</th>\n",
       "      <th>window_end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>5084</td>\n",
       "      <td>2022-02-19 19:00:00</td>\n",
       "      <td>2022-02-18 19:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>12344</td>\n",
       "      <td>2022-02-19 20:00:00</td>\n",
       "      <td>2022-02-18 20:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>18707</td>\n",
       "      <td>2022-02-19 21:00:00</td>\n",
       "      <td>2022-02-18 21:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>25061</td>\n",
       "      <td>2022-02-19 22:00:00</td>\n",
       "      <td>2022-02-18 22:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>30909</td>\n",
       "      <td>2022-02-19 23:00:00</td>\n",
       "      <td>2022-02-18 23:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>37274</td>\n",
       "      <td>2022-02-20 00:00:00</td>\n",
       "      <td>2022-02-19 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>45964</td>\n",
       "      <td>2022-02-20 01:00:00</td>\n",
       "      <td>2022-02-19 01:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>52863</td>\n",
       "      <td>2022-02-20 02:00:00</td>\n",
       "      <td>2022-02-19 02:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>57737</td>\n",
       "      <td>2022-02-20 03:00:00</td>\n",
       "      <td>2022-02-19 03:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>63447</td>\n",
       "      <td>2022-02-20 04:00:00</td>\n",
       "      <td>2022-02-19 04:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>69892</td>\n",
       "      <td>2022-02-20 05:00:00</td>\n",
       "      <td>2022-02-19 05:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>75637</td>\n",
       "      <td>2022-02-20 06:00:00</td>\n",
       "      <td>2022-02-19 06:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>81277</td>\n",
       "      <td>2022-02-20 07:00:00</td>\n",
       "      <td>2022-02-19 07:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>87653</td>\n",
       "      <td>2022-02-20 08:00:00</td>\n",
       "      <td>2022-02-19 08:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>94477</td>\n",
       "      <td>2022-02-20 09:00:00</td>\n",
       "      <td>2022-02-19 09:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>99937</td>\n",
       "      <td>2022-02-20 10:00:00</td>\n",
       "      <td>2022-02-19 10:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>104971</td>\n",
       "      <td>2022-02-20 11:00:00</td>\n",
       "      <td>2022-02-19 11:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>113085</td>\n",
       "      <td>2022-02-20 12:00:00</td>\n",
       "      <td>2022-02-19 12:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>119371</td>\n",
       "      <td>2022-02-20 13:00:00</td>\n",
       "      <td>2022-02-19 13:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>124473</td>\n",
       "      <td>2022-02-20 14:00:00</td>\n",
       "      <td>2022-02-19 14:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>129385</td>\n",
       "      <td>2022-02-20 15:00:00</td>\n",
       "      <td>2022-02-19 15:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>135751</td>\n",
       "      <td>2022-02-20 16:00:00</td>\n",
       "      <td>2022-02-19 16:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>139390</td>\n",
       "      <td>2022-02-20 17:00:00</td>\n",
       "      <td>2022-02-19 17:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>145311</td>\n",
       "      <td>2022-02-20 18:00:00</td>\n",
       "      <td>2022-02-19 18:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>147544</td>\n",
       "      <td>2022-02-20 19:00:00</td>\n",
       "      <td>2022-02-19 19:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>144267</td>\n",
       "      <td>2022-02-20 20:00:00</td>\n",
       "      <td>2022-02-19 20:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>145131</td>\n",
       "      <td>2022-02-20 21:00:00</td>\n",
       "      <td>2022-02-19 21:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>143740</td>\n",
       "      <td>2022-02-20 22:00:00</td>\n",
       "      <td>2022-02-19 22:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>143219</td>\n",
       "      <td>2022-02-20 23:00:00</td>\n",
       "      <td>2022-02-19 23:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>143673</td>\n",
       "      <td>2022-02-21 00:00:00</td>\n",
       "      <td>2022-02-20 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>140295</td>\n",
       "      <td>2022-02-21 01:00:00</td>\n",
       "      <td>2022-02-20 01:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>139347</td>\n",
       "      <td>2022-02-21 02:00:00</td>\n",
       "      <td>2022-02-20 02:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>141550</td>\n",
       "      <td>2022-02-21 03:00:00</td>\n",
       "      <td>2022-02-20 03:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>140773</td>\n",
       "      <td>2022-02-21 04:00:00</td>\n",
       "      <td>2022-02-20 04:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>140715</td>\n",
       "      <td>2022-02-21 05:00:00</td>\n",
       "      <td>2022-02-20 05:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>140238</td>\n",
       "      <td>2022-02-21 06:00:00</td>\n",
       "      <td>2022-02-20 06:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>141156</td>\n",
       "      <td>2022-02-21 07:00:00</td>\n",
       "      <td>2022-02-20 07:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>139966</td>\n",
       "      <td>2022-02-21 08:00:00</td>\n",
       "      <td>2022-02-20 08:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>138756</td>\n",
       "      <td>2022-02-21 09:00:00</td>\n",
       "      <td>2022-02-20 09:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>138756</td>\n",
       "      <td>2022-02-21 10:00:00</td>\n",
       "      <td>2022-02-20 10:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>140324</td>\n",
       "      <td>2022-02-21 11:00:00</td>\n",
       "      <td>2022-02-20 11:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>139966</td>\n",
       "      <td>2022-02-21 12:00:00</td>\n",
       "      <td>2022-02-20 12:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>138402</td>\n",
       "      <td>2022-02-21 13:00:00</td>\n",
       "      <td>2022-02-20 13:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>138438</td>\n",
       "      <td>2022-02-21 14:00:00</td>\n",
       "      <td>2022-02-20 14:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>139531</td>\n",
       "      <td>2022-02-21 15:00:00</td>\n",
       "      <td>2022-02-20 15:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>139131</td>\n",
       "      <td>2022-02-21 16:00:00</td>\n",
       "      <td>2022-02-20 16:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>141402</td>\n",
       "      <td>2022-02-21 17:00:00</td>\n",
       "      <td>2022-02-20 17:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>142681</td>\n",
       "      <td>2022-02-21 18:00:00</td>\n",
       "      <td>2022-02-20 18:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>139631</td>\n",
       "      <td>2022-02-21 19:00:00</td>\n",
       "      <td>2022-02-20 19:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>141245</td>\n",
       "      <td>2022-02-21 20:00:00</td>\n",
       "      <td>2022-02-20 20:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>138895</td>\n",
       "      <td>2022-02-21 21:00:00</td>\n",
       "      <td>2022-02-20 21:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>139180</td>\n",
       "      <td>2022-02-21 22:00:00</td>\n",
       "      <td>2022-02-20 22:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>139242</td>\n",
       "      <td>2022-02-21 23:00:00</td>\n",
       "      <td>2022-02-20 23:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>139311</td>\n",
       "      <td>2022-02-22 00:00:00</td>\n",
       "      <td>2022-02-21 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>139480</td>\n",
       "      <td>2022-02-22 01:00:00</td>\n",
       "      <td>2022-02-21 01:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>139725</td>\n",
       "      <td>2022-02-22 02:00:00</td>\n",
       "      <td>2022-02-21 02:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>139654</td>\n",
       "      <td>2022-02-22 03:00:00</td>\n",
       "      <td>2022-02-21 03:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>140779</td>\n",
       "      <td>2022-02-22 04:00:00</td>\n",
       "      <td>2022-02-21 04:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>141321</td>\n",
       "      <td>2022-02-22 05:00:00</td>\n",
       "      <td>2022-02-21 05:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>142431</td>\n",
       "      <td>2022-02-22 06:00:00</td>\n",
       "      <td>2022-02-21 06:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>142410</td>\n",
       "      <td>2022-02-22 07:00:00</td>\n",
       "      <td>2022-02-21 07:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>143809</td>\n",
       "      <td>2022-02-22 08:00:00</td>\n",
       "      <td>2022-02-21 08:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>144593</td>\n",
       "      <td>2022-02-22 09:00:00</td>\n",
       "      <td>2022-02-21 09:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>146039</td>\n",
       "      <td>2022-02-22 10:00:00</td>\n",
       "      <td>2022-02-21 10:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>145403</td>\n",
       "      <td>2022-02-22 11:00:00</td>\n",
       "      <td>2022-02-21 11:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>143664</td>\n",
       "      <td>2022-02-22 12:00:00</td>\n",
       "      <td>2022-02-21 12:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>143986</td>\n",
       "      <td>2022-02-22 13:00:00</td>\n",
       "      <td>2022-02-21 13:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>144728</td>\n",
       "      <td>2022-02-22 14:00:00</td>\n",
       "      <td>2022-02-21 14:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>143742</td>\n",
       "      <td>2022-02-22 15:00:00</td>\n",
       "      <td>2022-02-21 15:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>144066</td>\n",
       "      <td>2022-02-22 16:00:00</td>\n",
       "      <td>2022-02-21 16:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>143802</td>\n",
       "      <td>2022-02-22 17:00:00</td>\n",
       "      <td>2022-02-21 17:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>142819</td>\n",
       "      <td>2022-02-22 18:00:00</td>\n",
       "      <td>2022-02-21 18:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>144645</td>\n",
       "      <td>2022-02-22 19:00:00</td>\n",
       "      <td>2022-02-21 19:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>145088</td>\n",
       "      <td>2022-02-22 20:00:00</td>\n",
       "      <td>2022-02-21 20:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>144240</td>\n",
       "      <td>2022-02-22 21:00:00</td>\n",
       "      <td>2022-02-21 21:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>144205</td>\n",
       "      <td>2022-02-22 22:00:00</td>\n",
       "      <td>2022-02-21 22:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>144458</td>\n",
       "      <td>2022-02-22 23:00:00</td>\n",
       "      <td>2022-02-21 23:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>143400</td>\n",
       "      <td>2022-02-23 00:00:00</td>\n",
       "      <td>2022-02-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>144569</td>\n",
       "      <td>2022-02-23 01:00:00</td>\n",
       "      <td>2022-02-22 01:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>143739</td>\n",
       "      <td>2022-02-23 02:00:00</td>\n",
       "      <td>2022-02-22 02:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>143729</td>\n",
       "      <td>2022-02-23 03:00:00</td>\n",
       "      <td>2022-02-22 03:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>143207</td>\n",
       "      <td>2022-02-23 04:00:00</td>\n",
       "      <td>2022-02-22 04:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>140133</td>\n",
       "      <td>2022-02-23 05:00:00</td>\n",
       "      <td>2022-02-22 05:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>135187</td>\n",
       "      <td>2022-02-23 06:00:00</td>\n",
       "      <td>2022-02-22 06:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>128650</td>\n",
       "      <td>2022-02-23 07:00:00</td>\n",
       "      <td>2022-02-22 07:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>122065</td>\n",
       "      <td>2022-02-23 08:00:00</td>\n",
       "      <td>2022-02-22 08:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>115667</td>\n",
       "      <td>2022-02-23 09:00:00</td>\n",
       "      <td>2022-02-22 09:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>108761</td>\n",
       "      <td>2022-02-23 10:00:00</td>\n",
       "      <td>2022-02-22 10:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>102795</td>\n",
       "      <td>2022-02-23 11:00:00</td>\n",
       "      <td>2022-02-22 11:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>96778</td>\n",
       "      <td>2022-02-23 12:00:00</td>\n",
       "      <td>2022-02-22 12:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>91734</td>\n",
       "      <td>2022-02-23 13:00:00</td>\n",
       "      <td>2022-02-22 13:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>85854</td>\n",
       "      <td>2022-02-23 14:00:00</td>\n",
       "      <td>2022-02-22 14:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>80835</td>\n",
       "      <td>2022-02-23 15:00:00</td>\n",
       "      <td>2022-02-22 15:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>74545</td>\n",
       "      <td>2022-02-23 16:00:00</td>\n",
       "      <td>2022-02-22 16:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>68899</td>\n",
       "      <td>2022-02-23 17:00:00</td>\n",
       "      <td>2022-02-22 17:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>62682</td>\n",
       "      <td>2022-02-23 18:00:00</td>\n",
       "      <td>2022-02-22 18:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>56589</td>\n",
       "      <td>2022-02-23 19:00:00</td>\n",
       "      <td>2022-02-22 19:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>50549</td>\n",
       "      <td>2022-02-23 20:00:00</td>\n",
       "      <td>2022-02-22 20:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>46520</td>\n",
       "      <td>2022-02-23 21:00:00</td>\n",
       "      <td>2022-02-22 21:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>41307</td>\n",
       "      <td>2022-02-23 22:00:00</td>\n",
       "      <td>2022-02-22 22:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>35665</td>\n",
       "      <td>2022-02-23 23:00:00</td>\n",
       "      <td>2022-02-22 23:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>29835</td>\n",
       "      <td>2022-02-24 00:00:00</td>\n",
       "      <td>2022-02-23 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>23185</td>\n",
       "      <td>2022-02-24 01:00:00</td>\n",
       "      <td>2022-02-23 01:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>17819</td>\n",
       "      <td>2022-02-24 02:00:00</td>\n",
       "      <td>2022-02-23 02:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>10823</td>\n",
       "      <td>2022-02-24 03:00:00</td>\n",
       "      <td>2022-02-23 03:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>5287</td>\n",
       "      <td>2022-02-24 04:00:00</td>\n",
       "      <td>2022-02-23 04:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>1432</td>\n",
       "      <td>2022-02-24 05:00:00</td>\n",
       "      <td>2022-02-23 05:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>6171</td>\n",
       "      <td>2022-02-19 19:00:00</td>\n",
       "      <td>2022-02-18 19:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>10786</td>\n",
       "      <td>2022-02-19 20:00:00</td>\n",
       "      <td>2022-02-18 20:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>16151</td>\n",
       "      <td>2022-02-19 21:00:00</td>\n",
       "      <td>2022-02-18 21:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>20322</td>\n",
       "      <td>2022-02-19 22:00:00</td>\n",
       "      <td>2022-02-18 22:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>25889</td>\n",
       "      <td>2022-02-19 23:00:00</td>\n",
       "      <td>2022-02-18 23:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>33788</td>\n",
       "      <td>2022-02-20 00:00:00</td>\n",
       "      <td>2022-02-19 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>40734</td>\n",
       "      <td>2022-02-20 01:00:00</td>\n",
       "      <td>2022-02-19 01:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>45731</td>\n",
       "      <td>2022-02-20 02:00:00</td>\n",
       "      <td>2022-02-19 02:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>49586</td>\n",
       "      <td>2022-02-20 03:00:00</td>\n",
       "      <td>2022-02-19 03:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>55794</td>\n",
       "      <td>2022-02-20 04:00:00</td>\n",
       "      <td>2022-02-19 04:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>60617</td>\n",
       "      <td>2022-02-20 05:00:00</td>\n",
       "      <td>2022-02-19 05:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>67244</td>\n",
       "      <td>2022-02-20 06:00:00</td>\n",
       "      <td>2022-02-19 06:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>72195</td>\n",
       "      <td>2022-02-20 07:00:00</td>\n",
       "      <td>2022-02-19 07:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>76847</td>\n",
       "      <td>2022-02-20 08:00:00</td>\n",
       "      <td>2022-02-19 08:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>84495</td>\n",
       "      <td>2022-02-20 09:00:00</td>\n",
       "      <td>2022-02-19 09:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>91567</td>\n",
       "      <td>2022-02-20 10:00:00</td>\n",
       "      <td>2022-02-19 10:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>99147</td>\n",
       "      <td>2022-02-20 11:00:00</td>\n",
       "      <td>2022-02-19 11:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>103447</td>\n",
       "      <td>2022-02-20 12:00:00</td>\n",
       "      <td>2022-02-19 12:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>111007</td>\n",
       "      <td>2022-02-20 13:00:00</td>\n",
       "      <td>2022-02-19 13:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>115987</td>\n",
       "      <td>2022-02-20 14:00:00</td>\n",
       "      <td>2022-02-19 14:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>122249</td>\n",
       "      <td>2022-02-20 15:00:00</td>\n",
       "      <td>2022-02-19 15:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>128477</td>\n",
       "      <td>2022-02-20 16:00:00</td>\n",
       "      <td>2022-02-19 16:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>134980</td>\n",
       "      <td>2022-02-20 17:00:00</td>\n",
       "      <td>2022-02-19 17:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>139580</td>\n",
       "      <td>2022-02-20 18:00:00</td>\n",
       "      <td>2022-02-19 18:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>141108</td>\n",
       "      <td>2022-02-20 19:00:00</td>\n",
       "      <td>2022-02-19 19:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>141481</td>\n",
       "      <td>2022-02-20 20:00:00</td>\n",
       "      <td>2022-02-19 20:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>140795</td>\n",
       "      <td>2022-02-20 21:00:00</td>\n",
       "      <td>2022-02-19 21:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>143515</td>\n",
       "      <td>2022-02-20 22:00:00</td>\n",
       "      <td>2022-02-19 22:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>143782</td>\n",
       "      <td>2022-02-20 23:00:00</td>\n",
       "      <td>2022-02-19 23:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>141101</td>\n",
       "      <td>2022-02-21 00:00:00</td>\n",
       "      <td>2022-02-20 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>139958</td>\n",
       "      <td>2022-02-21 01:00:00</td>\n",
       "      <td>2022-02-20 01:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>141518</td>\n",
       "      <td>2022-02-21 02:00:00</td>\n",
       "      <td>2022-02-20 02:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>142505</td>\n",
       "      <td>2022-02-21 03:00:00</td>\n",
       "      <td>2022-02-20 03:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>142602</td>\n",
       "      <td>2022-02-21 04:00:00</td>\n",
       "      <td>2022-02-20 04:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>144176</td>\n",
       "      <td>2022-02-21 05:00:00</td>\n",
       "      <td>2022-02-20 05:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>143169</td>\n",
       "      <td>2022-02-21 06:00:00</td>\n",
       "      <td>2022-02-20 06:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>143668</td>\n",
       "      <td>2022-02-21 07:00:00</td>\n",
       "      <td>2022-02-20 07:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>144717</td>\n",
       "      <td>2022-02-21 08:00:00</td>\n",
       "      <td>2022-02-20 08:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>142765</td>\n",
       "      <td>2022-02-21 09:00:00</td>\n",
       "      <td>2022-02-20 09:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>142024</td>\n",
       "      <td>2022-02-21 10:00:00</td>\n",
       "      <td>2022-02-20 10:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>141358</td>\n",
       "      <td>2022-02-21 11:00:00</td>\n",
       "      <td>2022-02-20 11:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>143203</td>\n",
       "      <td>2022-02-21 12:00:00</td>\n",
       "      <td>2022-02-20 12:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>141534</td>\n",
       "      <td>2022-02-21 13:00:00</td>\n",
       "      <td>2022-02-20 13:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>142422</td>\n",
       "      <td>2022-02-21 14:00:00</td>\n",
       "      <td>2022-02-20 14:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>141804</td>\n",
       "      <td>2022-02-21 15:00:00</td>\n",
       "      <td>2022-02-20 15:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>139496</td>\n",
       "      <td>2022-02-21 16:00:00</td>\n",
       "      <td>2022-02-20 16:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>138703</td>\n",
       "      <td>2022-02-21 17:00:00</td>\n",
       "      <td>2022-02-20 17:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>142090</td>\n",
       "      <td>2022-02-21 18:00:00</td>\n",
       "      <td>2022-02-20 18:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>140277</td>\n",
       "      <td>2022-02-21 19:00:00</td>\n",
       "      <td>2022-02-20 19:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>140332</td>\n",
       "      <td>2022-02-21 20:00:00</td>\n",
       "      <td>2022-02-20 20:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>140435</td>\n",
       "      <td>2022-02-21 21:00:00</td>\n",
       "      <td>2022-02-20 21:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>138622</td>\n",
       "      <td>2022-02-21 22:00:00</td>\n",
       "      <td>2022-02-20 22:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>138299</td>\n",
       "      <td>2022-02-21 23:00:00</td>\n",
       "      <td>2022-02-20 23:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>139073</td>\n",
       "      <td>2022-02-22 00:00:00</td>\n",
       "      <td>2022-02-21 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>140402</td>\n",
       "      <td>2022-02-22 01:00:00</td>\n",
       "      <td>2022-02-21 01:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>139511</td>\n",
       "      <td>2022-02-22 02:00:00</td>\n",
       "      <td>2022-02-21 02:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>140201</td>\n",
       "      <td>2022-02-22 03:00:00</td>\n",
       "      <td>2022-02-21 03:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>138992</td>\n",
       "      <td>2022-02-22 04:00:00</td>\n",
       "      <td>2022-02-21 04:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>138937</td>\n",
       "      <td>2022-02-22 05:00:00</td>\n",
       "      <td>2022-02-21 05:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>139169</td>\n",
       "      <td>2022-02-22 06:00:00</td>\n",
       "      <td>2022-02-21 06:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>137208</td>\n",
       "      <td>2022-02-22 07:00:00</td>\n",
       "      <td>2022-02-21 07:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>138627</td>\n",
       "      <td>2022-02-22 08:00:00</td>\n",
       "      <td>2022-02-21 08:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>139072</td>\n",
       "      <td>2022-02-22 09:00:00</td>\n",
       "      <td>2022-02-21 09:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>139710</td>\n",
       "      <td>2022-02-22 10:00:00</td>\n",
       "      <td>2022-02-21 10:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>138205</td>\n",
       "      <td>2022-02-22 11:00:00</td>\n",
       "      <td>2022-02-21 11:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>137780</td>\n",
       "      <td>2022-02-22 12:00:00</td>\n",
       "      <td>2022-02-21 12:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>137257</td>\n",
       "      <td>2022-02-22 13:00:00</td>\n",
       "      <td>2022-02-21 13:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>137494</td>\n",
       "      <td>2022-02-22 14:00:00</td>\n",
       "      <td>2022-02-21 14:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>138911</td>\n",
       "      <td>2022-02-22 15:00:00</td>\n",
       "      <td>2022-02-21 15:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>142124</td>\n",
       "      <td>2022-02-22 16:00:00</td>\n",
       "      <td>2022-02-21 16:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>143410</td>\n",
       "      <td>2022-02-22 17:00:00</td>\n",
       "      <td>2022-02-21 17:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>139614</td>\n",
       "      <td>2022-02-22 18:00:00</td>\n",
       "      <td>2022-02-21 18:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>139631</td>\n",
       "      <td>2022-02-22 19:00:00</td>\n",
       "      <td>2022-02-21 19:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>140494</td>\n",
       "      <td>2022-02-22 20:00:00</td>\n",
       "      <td>2022-02-21 20:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>142147</td>\n",
       "      <td>2022-02-22 21:00:00</td>\n",
       "      <td>2022-02-21 21:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>141997</td>\n",
       "      <td>2022-02-22 22:00:00</td>\n",
       "      <td>2022-02-21 22:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>142900</td>\n",
       "      <td>2022-02-22 23:00:00</td>\n",
       "      <td>2022-02-21 23:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>141246</td>\n",
       "      <td>2022-02-23 00:00:00</td>\n",
       "      <td>2022-02-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>140744</td>\n",
       "      <td>2022-02-23 01:00:00</td>\n",
       "      <td>2022-02-22 01:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>141719</td>\n",
       "      <td>2022-02-23 02:00:00</td>\n",
       "      <td>2022-02-22 02:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>141292</td>\n",
       "      <td>2022-02-23 03:00:00</td>\n",
       "      <td>2022-02-22 03:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>144324</td>\n",
       "      <td>2022-02-23 04:00:00</td>\n",
       "      <td>2022-02-22 04:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>144624</td>\n",
       "      <td>2022-02-23 05:00:00</td>\n",
       "      <td>2022-02-22 05:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>140536</td>\n",
       "      <td>2022-02-23 06:00:00</td>\n",
       "      <td>2022-02-22 06:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>137047</td>\n",
       "      <td>2022-02-23 07:00:00</td>\n",
       "      <td>2022-02-22 07:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>129927</td>\n",
       "      <td>2022-02-23 08:00:00</td>\n",
       "      <td>2022-02-22 08:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>123786</td>\n",
       "      <td>2022-02-23 09:00:00</td>\n",
       "      <td>2022-02-22 09:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>116817</td>\n",
       "      <td>2022-02-23 10:00:00</td>\n",
       "      <td>2022-02-22 10:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>111408</td>\n",
       "      <td>2022-02-23 11:00:00</td>\n",
       "      <td>2022-02-22 11:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>105688</td>\n",
       "      <td>2022-02-23 12:00:00</td>\n",
       "      <td>2022-02-22 12:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>100320</td>\n",
       "      <td>2022-02-23 13:00:00</td>\n",
       "      <td>2022-02-22 13:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>94215</td>\n",
       "      <td>2022-02-23 14:00:00</td>\n",
       "      <td>2022-02-22 14:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>87154</td>\n",
       "      <td>2022-02-23 15:00:00</td>\n",
       "      <td>2022-02-22 15:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>80021</td>\n",
       "      <td>2022-02-23 16:00:00</td>\n",
       "      <td>2022-02-22 16:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>73025</td>\n",
       "      <td>2022-02-23 17:00:00</td>\n",
       "      <td>2022-02-22 17:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>68834</td>\n",
       "      <td>2022-02-23 18:00:00</td>\n",
       "      <td>2022-02-22 18:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>62931</td>\n",
       "      <td>2022-02-23 19:00:00</td>\n",
       "      <td>2022-02-22 19:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>57025</td>\n",
       "      <td>2022-02-23 20:00:00</td>\n",
       "      <td>2022-02-22 20:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>50590</td>\n",
       "      <td>2022-02-23 21:00:00</td>\n",
       "      <td>2022-02-22 21:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>45662</td>\n",
       "      <td>2022-02-23 22:00:00</td>\n",
       "      <td>2022-02-22 22:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>39248</td>\n",
       "      <td>2022-02-23 23:00:00</td>\n",
       "      <td>2022-02-22 23:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>34910</td>\n",
       "      <td>2022-02-24 00:00:00</td>\n",
       "      <td>2022-02-23 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>28280</td>\n",
       "      <td>2022-02-24 01:00:00</td>\n",
       "      <td>2022-02-23 01:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>21639</td>\n",
       "      <td>2022-02-24 02:00:00</td>\n",
       "      <td>2022-02-23 02:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>16534</td>\n",
       "      <td>2022-02-24 03:00:00</td>\n",
       "      <td>2022-02-23 03:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>8406</td>\n",
       "      <td>2022-02-24 04:00:00</td>\n",
       "      <td>2022-02-23 04:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>GOOG</td>\n",
       "      <td>1764</td>\n",
       "      <td>2022-02-24 05:00:00</td>\n",
       "      <td>2022-02-23 05:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>4453</td>\n",
       "      <td>2022-02-19 19:00:00</td>\n",
       "      <td>2022-02-18 19:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>10079</td>\n",
       "      <td>2022-02-19 20:00:00</td>\n",
       "      <td>2022-02-18 20:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>16260</td>\n",
       "      <td>2022-02-19 21:00:00</td>\n",
       "      <td>2022-02-18 21:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>23727</td>\n",
       "      <td>2022-02-19 22:00:00</td>\n",
       "      <td>2022-02-18 22:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>30254</td>\n",
       "      <td>2022-02-19 23:00:00</td>\n",
       "      <td>2022-02-18 23:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>36734</td>\n",
       "      <td>2022-02-20 00:00:00</td>\n",
       "      <td>2022-02-19 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>41980</td>\n",
       "      <td>2022-02-20 01:00:00</td>\n",
       "      <td>2022-02-19 01:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>47295</td>\n",
       "      <td>2022-02-20 02:00:00</td>\n",
       "      <td>2022-02-19 02:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>53789</td>\n",
       "      <td>2022-02-20 03:00:00</td>\n",
       "      <td>2022-02-19 03:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>60349</td>\n",
       "      <td>2022-02-20 04:00:00</td>\n",
       "      <td>2022-02-19 04:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>64717</td>\n",
       "      <td>2022-02-20 05:00:00</td>\n",
       "      <td>2022-02-19 05:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>72803</td>\n",
       "      <td>2022-02-20 06:00:00</td>\n",
       "      <td>2022-02-19 06:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>80336</td>\n",
       "      <td>2022-02-20 07:00:00</td>\n",
       "      <td>2022-02-19 07:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>88263</td>\n",
       "      <td>2022-02-20 08:00:00</td>\n",
       "      <td>2022-02-19 08:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>94653</td>\n",
       "      <td>2022-02-20 09:00:00</td>\n",
       "      <td>2022-02-19 09:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>100791</td>\n",
       "      <td>2022-02-20 10:00:00</td>\n",
       "      <td>2022-02-19 10:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>106664</td>\n",
       "      <td>2022-02-20 11:00:00</td>\n",
       "      <td>2022-02-19 11:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>111972</td>\n",
       "      <td>2022-02-20 12:00:00</td>\n",
       "      <td>2022-02-19 12:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>119034</td>\n",
       "      <td>2022-02-20 13:00:00</td>\n",
       "      <td>2022-02-19 13:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>123402</td>\n",
       "      <td>2022-02-20 14:00:00</td>\n",
       "      <td>2022-02-19 14:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>129229</td>\n",
       "      <td>2022-02-20 15:00:00</td>\n",
       "      <td>2022-02-19 15:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>136202</td>\n",
       "      <td>2022-02-20 16:00:00</td>\n",
       "      <td>2022-02-19 16:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>140907</td>\n",
       "      <td>2022-02-20 17:00:00</td>\n",
       "      <td>2022-02-19 17:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>147818</td>\n",
       "      <td>2022-02-20 18:00:00</td>\n",
       "      <td>2022-02-19 18:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>149605</td>\n",
       "      <td>2022-02-20 19:00:00</td>\n",
       "      <td>2022-02-19 19:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>149483</td>\n",
       "      <td>2022-02-20 20:00:00</td>\n",
       "      <td>2022-02-19 20:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>148522</td>\n",
       "      <td>2022-02-20 21:00:00</td>\n",
       "      <td>2022-02-19 21:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>148338</td>\n",
       "      <td>2022-02-20 22:00:00</td>\n",
       "      <td>2022-02-19 22:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>148064</td>\n",
       "      <td>2022-02-20 23:00:00</td>\n",
       "      <td>2022-02-19 23:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>146417</td>\n",
       "      <td>2022-02-21 00:00:00</td>\n",
       "      <td>2022-02-20 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>147801</td>\n",
       "      <td>2022-02-21 01:00:00</td>\n",
       "      <td>2022-02-20 01:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>148208</td>\n",
       "      <td>2022-02-21 02:00:00</td>\n",
       "      <td>2022-02-20 02:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>147496</td>\n",
       "      <td>2022-02-21 03:00:00</td>\n",
       "      <td>2022-02-20 03:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>147514</td>\n",
       "      <td>2022-02-21 04:00:00</td>\n",
       "      <td>2022-02-20 04:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>148280</td>\n",
       "      <td>2022-02-21 05:00:00</td>\n",
       "      <td>2022-02-20 05:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>147151</td>\n",
       "      <td>2022-02-21 06:00:00</td>\n",
       "      <td>2022-02-20 06:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>144821</td>\n",
       "      <td>2022-02-21 07:00:00</td>\n",
       "      <td>2022-02-20 07:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>144226</td>\n",
       "      <td>2022-02-21 08:00:00</td>\n",
       "      <td>2022-02-20 08:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>144353</td>\n",
       "      <td>2022-02-21 09:00:00</td>\n",
       "      <td>2022-02-20 09:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>144213</td>\n",
       "      <td>2022-02-21 10:00:00</td>\n",
       "      <td>2022-02-20 10:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>143792</td>\n",
       "      <td>2022-02-21 11:00:00</td>\n",
       "      <td>2022-02-20 11:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>144963</td>\n",
       "      <td>2022-02-21 12:00:00</td>\n",
       "      <td>2022-02-20 12:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>143910</td>\n",
       "      <td>2022-02-21 13:00:00</td>\n",
       "      <td>2022-02-20 13:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>145031</td>\n",
       "      <td>2022-02-21 14:00:00</td>\n",
       "      <td>2022-02-20 14:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>146060</td>\n",
       "      <td>2022-02-21 15:00:00</td>\n",
       "      <td>2022-02-20 15:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>144470</td>\n",
       "      <td>2022-02-21 16:00:00</td>\n",
       "      <td>2022-02-20 16:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>146001</td>\n",
       "      <td>2022-02-21 17:00:00</td>\n",
       "      <td>2022-02-20 17:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>145270</td>\n",
       "      <td>2022-02-21 18:00:00</td>\n",
       "      <td>2022-02-20 18:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>145588</td>\n",
       "      <td>2022-02-21 19:00:00</td>\n",
       "      <td>2022-02-20 19:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>146548</td>\n",
       "      <td>2022-02-21 20:00:00</td>\n",
       "      <td>2022-02-20 20:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>149363</td>\n",
       "      <td>2022-02-21 21:00:00</td>\n",
       "      <td>2022-02-20 21:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>149496</td>\n",
       "      <td>2022-02-21 22:00:00</td>\n",
       "      <td>2022-02-20 22:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>149227</td>\n",
       "      <td>2022-02-21 23:00:00</td>\n",
       "      <td>2022-02-20 23:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>149803</td>\n",
       "      <td>2022-02-22 00:00:00</td>\n",
       "      <td>2022-02-21 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>148586</td>\n",
       "      <td>2022-02-22 01:00:00</td>\n",
       "      <td>2022-02-21 01:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>148363</td>\n",
       "      <td>2022-02-22 02:00:00</td>\n",
       "      <td>2022-02-21 02:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>148776</td>\n",
       "      <td>2022-02-22 03:00:00</td>\n",
       "      <td>2022-02-21 03:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>147901</td>\n",
       "      <td>2022-02-22 04:00:00</td>\n",
       "      <td>2022-02-21 04:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>148100</td>\n",
       "      <td>2022-02-22 05:00:00</td>\n",
       "      <td>2022-02-21 05:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>145399</td>\n",
       "      <td>2022-02-22 06:00:00</td>\n",
       "      <td>2022-02-21 06:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>147365</td>\n",
       "      <td>2022-02-22 07:00:00</td>\n",
       "      <td>2022-02-21 07:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>146449</td>\n",
       "      <td>2022-02-22 08:00:00</td>\n",
       "      <td>2022-02-21 08:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>146354</td>\n",
       "      <td>2022-02-22 09:00:00</td>\n",
       "      <td>2022-02-21 09:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>148072</td>\n",
       "      <td>2022-02-22 10:00:00</td>\n",
       "      <td>2022-02-21 10:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>147915</td>\n",
       "      <td>2022-02-22 11:00:00</td>\n",
       "      <td>2022-02-21 11:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>147714</td>\n",
       "      <td>2022-02-22 12:00:00</td>\n",
       "      <td>2022-02-21 12:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>146672</td>\n",
       "      <td>2022-02-22 13:00:00</td>\n",
       "      <td>2022-02-21 13:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>146592</td>\n",
       "      <td>2022-02-22 14:00:00</td>\n",
       "      <td>2022-02-21 14:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>144455</td>\n",
       "      <td>2022-02-22 15:00:00</td>\n",
       "      <td>2022-02-21 15:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>145961</td>\n",
       "      <td>2022-02-22 16:00:00</td>\n",
       "      <td>2022-02-21 16:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>146253</td>\n",
       "      <td>2022-02-22 17:00:00</td>\n",
       "      <td>2022-02-21 17:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>145879</td>\n",
       "      <td>2022-02-22 18:00:00</td>\n",
       "      <td>2022-02-21 18:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>146811</td>\n",
       "      <td>2022-02-22 19:00:00</td>\n",
       "      <td>2022-02-21 19:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>147633</td>\n",
       "      <td>2022-02-22 20:00:00</td>\n",
       "      <td>2022-02-21 20:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>146821</td>\n",
       "      <td>2022-02-22 21:00:00</td>\n",
       "      <td>2022-02-21 21:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>144789</td>\n",
       "      <td>2022-02-22 22:00:00</td>\n",
       "      <td>2022-02-21 22:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>145560</td>\n",
       "      <td>2022-02-22 23:00:00</td>\n",
       "      <td>2022-02-21 23:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>147090</td>\n",
       "      <td>2022-02-23 00:00:00</td>\n",
       "      <td>2022-02-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>147620</td>\n",
       "      <td>2022-02-23 01:00:00</td>\n",
       "      <td>2022-02-22 01:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>147379</td>\n",
       "      <td>2022-02-23 02:00:00</td>\n",
       "      <td>2022-02-22 02:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>146556</td>\n",
       "      <td>2022-02-23 03:00:00</td>\n",
       "      <td>2022-02-22 03:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>146086</td>\n",
       "      <td>2022-02-23 04:00:00</td>\n",
       "      <td>2022-02-22 04:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>146869</td>\n",
       "      <td>2022-02-23 05:00:00</td>\n",
       "      <td>2022-02-22 05:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>144729</td>\n",
       "      <td>2022-02-23 06:00:00</td>\n",
       "      <td>2022-02-22 06:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>137560</td>\n",
       "      <td>2022-02-23 07:00:00</td>\n",
       "      <td>2022-02-22 07:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>131144</td>\n",
       "      <td>2022-02-23 08:00:00</td>\n",
       "      <td>2022-02-22 08:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>124722</td>\n",
       "      <td>2022-02-23 09:00:00</td>\n",
       "      <td>2022-02-22 09:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>117006</td>\n",
       "      <td>2022-02-23 10:00:00</td>\n",
       "      <td>2022-02-22 10:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>111711</td>\n",
       "      <td>2022-02-23 11:00:00</td>\n",
       "      <td>2022-02-22 11:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>105433</td>\n",
       "      <td>2022-02-23 12:00:00</td>\n",
       "      <td>2022-02-22 12:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>100466</td>\n",
       "      <td>2022-02-23 13:00:00</td>\n",
       "      <td>2022-02-22 13:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>95057</td>\n",
       "      <td>2022-02-23 14:00:00</td>\n",
       "      <td>2022-02-22 14:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>90338</td>\n",
       "      <td>2022-02-23 15:00:00</td>\n",
       "      <td>2022-02-22 15:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>83449</td>\n",
       "      <td>2022-02-23 16:00:00</td>\n",
       "      <td>2022-02-22 16:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>76921</td>\n",
       "      <td>2022-02-23 17:00:00</td>\n",
       "      <td>2022-02-22 17:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>71115</td>\n",
       "      <td>2022-02-23 18:00:00</td>\n",
       "      <td>2022-02-22 18:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>63625</td>\n",
       "      <td>2022-02-23 19:00:00</td>\n",
       "      <td>2022-02-22 19:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>56339</td>\n",
       "      <td>2022-02-23 20:00:00</td>\n",
       "      <td>2022-02-22 20:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>49116</td>\n",
       "      <td>2022-02-23 21:00:00</td>\n",
       "      <td>2022-02-22 21:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>43732</td>\n",
       "      <td>2022-02-23 22:00:00</td>\n",
       "      <td>2022-02-22 22:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>36977</td>\n",
       "      <td>2022-02-23 23:00:00</td>\n",
       "      <td>2022-02-22 23:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>30038</td>\n",
       "      <td>2022-02-24 00:00:00</td>\n",
       "      <td>2022-02-23 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>24095</td>\n",
       "      <td>2022-02-24 01:00:00</td>\n",
       "      <td>2022-02-23 01:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>18837</td>\n",
       "      <td>2022-02-24 02:00:00</td>\n",
       "      <td>2022-02-23 02:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>13465</td>\n",
       "      <td>2022-02-24 03:00:00</td>\n",
       "      <td>2022-02-23 03:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>8232</td>\n",
       "      <td>2022-02-24 04:00:00</td>\n",
       "      <td>2022-02-23 04:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>MSFT</td>\n",
       "      <td>2116</td>\n",
       "      <td>2022-02-24 05:00:00</td>\n",
       "      <td>2022-02-23 05:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    symbol  total_quantity        window_start          window_end\n",
       "23    AAPL            5084 2022-02-19 19:00:00 2022-02-18 19:00:00\n",
       "22    AAPL           12344 2022-02-19 20:00:00 2022-02-18 20:00:00\n",
       "21    AAPL           18707 2022-02-19 21:00:00 2022-02-18 21:00:00\n",
       "20    AAPL           25061 2022-02-19 22:00:00 2022-02-18 22:00:00\n",
       "19    AAPL           30909 2022-02-19 23:00:00 2022-02-18 23:00:00\n",
       "18    AAPL           37274 2022-02-20 00:00:00 2022-02-19 00:00:00\n",
       "17    AAPL           45964 2022-02-20 01:00:00 2022-02-19 01:00:00\n",
       "16    AAPL           52863 2022-02-20 02:00:00 2022-02-19 02:00:00\n",
       "15    AAPL           57737 2022-02-20 03:00:00 2022-02-19 03:00:00\n",
       "14    AAPL           63447 2022-02-20 04:00:00 2022-02-19 04:00:00\n",
       "13    AAPL           69892 2022-02-20 05:00:00 2022-02-19 05:00:00\n",
       "12    AAPL           75637 2022-02-20 06:00:00 2022-02-19 06:00:00\n",
       "11    AAPL           81277 2022-02-20 07:00:00 2022-02-19 07:00:00\n",
       "10    AAPL           87653 2022-02-20 08:00:00 2022-02-19 08:00:00\n",
       "9     AAPL           94477 2022-02-20 09:00:00 2022-02-19 09:00:00\n",
       "8     AAPL           99937 2022-02-20 10:00:00 2022-02-19 10:00:00\n",
       "7     AAPL          104971 2022-02-20 11:00:00 2022-02-19 11:00:00\n",
       "6     AAPL          113085 2022-02-20 12:00:00 2022-02-19 12:00:00\n",
       "5     AAPL          119371 2022-02-20 13:00:00 2022-02-19 13:00:00\n",
       "4     AAPL          124473 2022-02-20 14:00:00 2022-02-19 14:00:00\n",
       "3     AAPL          129385 2022-02-20 15:00:00 2022-02-19 15:00:00\n",
       "2     AAPL          135751 2022-02-20 16:00:00 2022-02-19 16:00:00\n",
       "1     AAPL          139390 2022-02-20 17:00:00 2022-02-19 17:00:00\n",
       "0     AAPL          145311 2022-02-20 18:00:00 2022-02-19 18:00:00\n",
       "24    AAPL          147544 2022-02-20 19:00:00 2022-02-19 19:00:00\n",
       "25    AAPL          144267 2022-02-20 20:00:00 2022-02-19 20:00:00\n",
       "26    AAPL          145131 2022-02-20 21:00:00 2022-02-19 21:00:00\n",
       "27    AAPL          143740 2022-02-20 22:00:00 2022-02-19 22:00:00\n",
       "28    AAPL          143219 2022-02-20 23:00:00 2022-02-19 23:00:00\n",
       "29    AAPL          143673 2022-02-21 00:00:00 2022-02-20 00:00:00\n",
       "30    AAPL          140295 2022-02-21 01:00:00 2022-02-20 01:00:00\n",
       "31    AAPL          139347 2022-02-21 02:00:00 2022-02-20 02:00:00\n",
       "32    AAPL          141550 2022-02-21 03:00:00 2022-02-20 03:00:00\n",
       "33    AAPL          140773 2022-02-21 04:00:00 2022-02-20 04:00:00\n",
       "34    AAPL          140715 2022-02-21 05:00:00 2022-02-20 05:00:00\n",
       "35    AAPL          140238 2022-02-21 06:00:00 2022-02-20 06:00:00\n",
       "36    AAPL          141156 2022-02-21 07:00:00 2022-02-20 07:00:00\n",
       "37    AAPL          139966 2022-02-21 08:00:00 2022-02-20 08:00:00\n",
       "38    AAPL          138756 2022-02-21 09:00:00 2022-02-20 09:00:00\n",
       "39    AAPL          138756 2022-02-21 10:00:00 2022-02-20 10:00:00\n",
       "40    AAPL          140324 2022-02-21 11:00:00 2022-02-20 11:00:00\n",
       "41    AAPL          139966 2022-02-21 12:00:00 2022-02-20 12:00:00\n",
       "42    AAPL          138402 2022-02-21 13:00:00 2022-02-20 13:00:00\n",
       "43    AAPL          138438 2022-02-21 14:00:00 2022-02-20 14:00:00\n",
       "44    AAPL          139531 2022-02-21 15:00:00 2022-02-20 15:00:00\n",
       "45    AAPL          139131 2022-02-21 16:00:00 2022-02-20 16:00:00\n",
       "46    AAPL          141402 2022-02-21 17:00:00 2022-02-20 17:00:00\n",
       "47    AAPL          142681 2022-02-21 18:00:00 2022-02-20 18:00:00\n",
       "48    AAPL          139631 2022-02-21 19:00:00 2022-02-20 19:00:00\n",
       "49    AAPL          141245 2022-02-21 20:00:00 2022-02-20 20:00:00\n",
       "50    AAPL          138895 2022-02-21 21:00:00 2022-02-20 21:00:00\n",
       "51    AAPL          139180 2022-02-21 22:00:00 2022-02-20 22:00:00\n",
       "52    AAPL          139242 2022-02-21 23:00:00 2022-02-20 23:00:00\n",
       "53    AAPL          139311 2022-02-22 00:00:00 2022-02-21 00:00:00\n",
       "54    AAPL          139480 2022-02-22 01:00:00 2022-02-21 01:00:00\n",
       "55    AAPL          139725 2022-02-22 02:00:00 2022-02-21 02:00:00\n",
       "56    AAPL          139654 2022-02-22 03:00:00 2022-02-21 03:00:00\n",
       "57    AAPL          140779 2022-02-22 04:00:00 2022-02-21 04:00:00\n",
       "58    AAPL          141321 2022-02-22 05:00:00 2022-02-21 05:00:00\n",
       "59    AAPL          142431 2022-02-22 06:00:00 2022-02-21 06:00:00\n",
       "60    AAPL          142410 2022-02-22 07:00:00 2022-02-21 07:00:00\n",
       "61    AAPL          143809 2022-02-22 08:00:00 2022-02-21 08:00:00\n",
       "62    AAPL          144593 2022-02-22 09:00:00 2022-02-21 09:00:00\n",
       "63    AAPL          146039 2022-02-22 10:00:00 2022-02-21 10:00:00\n",
       "64    AAPL          145403 2022-02-22 11:00:00 2022-02-21 11:00:00\n",
       "65    AAPL          143664 2022-02-22 12:00:00 2022-02-21 12:00:00\n",
       "66    AAPL          143986 2022-02-22 13:00:00 2022-02-21 13:00:00\n",
       "67    AAPL          144728 2022-02-22 14:00:00 2022-02-21 14:00:00\n",
       "68    AAPL          143742 2022-02-22 15:00:00 2022-02-21 15:00:00\n",
       "69    AAPL          144066 2022-02-22 16:00:00 2022-02-21 16:00:00\n",
       "70    AAPL          143802 2022-02-22 17:00:00 2022-02-21 17:00:00\n",
       "71    AAPL          142819 2022-02-22 18:00:00 2022-02-21 18:00:00\n",
       "72    AAPL          144645 2022-02-22 19:00:00 2022-02-21 19:00:00\n",
       "73    AAPL          145088 2022-02-22 20:00:00 2022-02-21 20:00:00\n",
       "74    AAPL          144240 2022-02-22 21:00:00 2022-02-21 21:00:00\n",
       "75    AAPL          144205 2022-02-22 22:00:00 2022-02-21 22:00:00\n",
       "76    AAPL          144458 2022-02-22 23:00:00 2022-02-21 23:00:00\n",
       "77    AAPL          143400 2022-02-23 00:00:00 2022-02-22 00:00:00\n",
       "78    AAPL          144569 2022-02-23 01:00:00 2022-02-22 01:00:00\n",
       "79    AAPL          143739 2022-02-23 02:00:00 2022-02-22 02:00:00\n",
       "80    AAPL          143729 2022-02-23 03:00:00 2022-02-22 03:00:00\n",
       "81    AAPL          143207 2022-02-23 04:00:00 2022-02-22 04:00:00\n",
       "82    AAPL          140133 2022-02-23 05:00:00 2022-02-22 05:00:00\n",
       "83    AAPL          135187 2022-02-23 06:00:00 2022-02-22 06:00:00\n",
       "84    AAPL          128650 2022-02-23 07:00:00 2022-02-22 07:00:00\n",
       "85    AAPL          122065 2022-02-23 08:00:00 2022-02-22 08:00:00\n",
       "86    AAPL          115667 2022-02-23 09:00:00 2022-02-22 09:00:00\n",
       "87    AAPL          108761 2022-02-23 10:00:00 2022-02-22 10:00:00\n",
       "88    AAPL          102795 2022-02-23 11:00:00 2022-02-22 11:00:00\n",
       "89    AAPL           96778 2022-02-23 12:00:00 2022-02-22 12:00:00\n",
       "90    AAPL           91734 2022-02-23 13:00:00 2022-02-22 13:00:00\n",
       "91    AAPL           85854 2022-02-23 14:00:00 2022-02-22 14:00:00\n",
       "92    AAPL           80835 2022-02-23 15:00:00 2022-02-22 15:00:00\n",
       "93    AAPL           74545 2022-02-23 16:00:00 2022-02-22 16:00:00\n",
       "94    AAPL           68899 2022-02-23 17:00:00 2022-02-22 17:00:00\n",
       "95    AAPL           62682 2022-02-23 18:00:00 2022-02-22 18:00:00\n",
       "96    AAPL           56589 2022-02-23 19:00:00 2022-02-22 19:00:00\n",
       "97    AAPL           50549 2022-02-23 20:00:00 2022-02-22 20:00:00\n",
       "98    AAPL           46520 2022-02-23 21:00:00 2022-02-22 21:00:00\n",
       "99    AAPL           41307 2022-02-23 22:00:00 2022-02-22 22:00:00\n",
       "100   AAPL           35665 2022-02-23 23:00:00 2022-02-22 23:00:00\n",
       "101   AAPL           29835 2022-02-24 00:00:00 2022-02-23 00:00:00\n",
       "102   AAPL           23185 2022-02-24 01:00:00 2022-02-23 01:00:00\n",
       "103   AAPL           17819 2022-02-24 02:00:00 2022-02-23 02:00:00\n",
       "104   AAPL           10823 2022-02-24 03:00:00 2022-02-23 03:00:00\n",
       "105   AAPL            5287 2022-02-24 04:00:00 2022-02-23 04:00:00\n",
       "106   AAPL            1432 2022-02-24 05:00:00 2022-02-23 05:00:00\n",
       "130   GOOG            6171 2022-02-19 19:00:00 2022-02-18 19:00:00\n",
       "129   GOOG           10786 2022-02-19 20:00:00 2022-02-18 20:00:00\n",
       "128   GOOG           16151 2022-02-19 21:00:00 2022-02-18 21:00:00\n",
       "127   GOOG           20322 2022-02-19 22:00:00 2022-02-18 22:00:00\n",
       "126   GOOG           25889 2022-02-19 23:00:00 2022-02-18 23:00:00\n",
       "125   GOOG           33788 2022-02-20 00:00:00 2022-02-19 00:00:00\n",
       "124   GOOG           40734 2022-02-20 01:00:00 2022-02-19 01:00:00\n",
       "123   GOOG           45731 2022-02-20 02:00:00 2022-02-19 02:00:00\n",
       "122   GOOG           49586 2022-02-20 03:00:00 2022-02-19 03:00:00\n",
       "121   GOOG           55794 2022-02-20 04:00:00 2022-02-19 04:00:00\n",
       "120   GOOG           60617 2022-02-20 05:00:00 2022-02-19 05:00:00\n",
       "119   GOOG           67244 2022-02-20 06:00:00 2022-02-19 06:00:00\n",
       "118   GOOG           72195 2022-02-20 07:00:00 2022-02-19 07:00:00\n",
       "117   GOOG           76847 2022-02-20 08:00:00 2022-02-19 08:00:00\n",
       "116   GOOG           84495 2022-02-20 09:00:00 2022-02-19 09:00:00\n",
       "115   GOOG           91567 2022-02-20 10:00:00 2022-02-19 10:00:00\n",
       "114   GOOG           99147 2022-02-20 11:00:00 2022-02-19 11:00:00\n",
       "113   GOOG          103447 2022-02-20 12:00:00 2022-02-19 12:00:00\n",
       "112   GOOG          111007 2022-02-20 13:00:00 2022-02-19 13:00:00\n",
       "111   GOOG          115987 2022-02-20 14:00:00 2022-02-19 14:00:00\n",
       "110   GOOG          122249 2022-02-20 15:00:00 2022-02-19 15:00:00\n",
       "109   GOOG          128477 2022-02-20 16:00:00 2022-02-19 16:00:00\n",
       "108   GOOG          134980 2022-02-20 17:00:00 2022-02-19 17:00:00\n",
       "107   GOOG          139580 2022-02-20 18:00:00 2022-02-19 18:00:00\n",
       "131   GOOG          141108 2022-02-20 19:00:00 2022-02-19 19:00:00\n",
       "132   GOOG          141481 2022-02-20 20:00:00 2022-02-19 20:00:00\n",
       "133   GOOG          140795 2022-02-20 21:00:00 2022-02-19 21:00:00\n",
       "134   GOOG          143515 2022-02-20 22:00:00 2022-02-19 22:00:00\n",
       "135   GOOG          143782 2022-02-20 23:00:00 2022-02-19 23:00:00\n",
       "136   GOOG          141101 2022-02-21 00:00:00 2022-02-20 00:00:00\n",
       "137   GOOG          139958 2022-02-21 01:00:00 2022-02-20 01:00:00\n",
       "138   GOOG          141518 2022-02-21 02:00:00 2022-02-20 02:00:00\n",
       "139   GOOG          142505 2022-02-21 03:00:00 2022-02-20 03:00:00\n",
       "140   GOOG          142602 2022-02-21 04:00:00 2022-02-20 04:00:00\n",
       "141   GOOG          144176 2022-02-21 05:00:00 2022-02-20 05:00:00\n",
       "142   GOOG          143169 2022-02-21 06:00:00 2022-02-20 06:00:00\n",
       "143   GOOG          143668 2022-02-21 07:00:00 2022-02-20 07:00:00\n",
       "144   GOOG          144717 2022-02-21 08:00:00 2022-02-20 08:00:00\n",
       "145   GOOG          142765 2022-02-21 09:00:00 2022-02-20 09:00:00\n",
       "146   GOOG          142024 2022-02-21 10:00:00 2022-02-20 10:00:00\n",
       "147   GOOG          141358 2022-02-21 11:00:00 2022-02-20 11:00:00\n",
       "148   GOOG          143203 2022-02-21 12:00:00 2022-02-20 12:00:00\n",
       "149   GOOG          141534 2022-02-21 13:00:00 2022-02-20 13:00:00\n",
       "150   GOOG          142422 2022-02-21 14:00:00 2022-02-20 14:00:00\n",
       "151   GOOG          141804 2022-02-21 15:00:00 2022-02-20 15:00:00\n",
       "152   GOOG          139496 2022-02-21 16:00:00 2022-02-20 16:00:00\n",
       "153   GOOG          138703 2022-02-21 17:00:00 2022-02-20 17:00:00\n",
       "154   GOOG          142090 2022-02-21 18:00:00 2022-02-20 18:00:00\n",
       "155   GOOG          140277 2022-02-21 19:00:00 2022-02-20 19:00:00\n",
       "156   GOOG          140332 2022-02-21 20:00:00 2022-02-20 20:00:00\n",
       "157   GOOG          140435 2022-02-21 21:00:00 2022-02-20 21:00:00\n",
       "158   GOOG          138622 2022-02-21 22:00:00 2022-02-20 22:00:00\n",
       "159   GOOG          138299 2022-02-21 23:00:00 2022-02-20 23:00:00\n",
       "160   GOOG          139073 2022-02-22 00:00:00 2022-02-21 00:00:00\n",
       "161   GOOG          140402 2022-02-22 01:00:00 2022-02-21 01:00:00\n",
       "162   GOOG          139511 2022-02-22 02:00:00 2022-02-21 02:00:00\n",
       "163   GOOG          140201 2022-02-22 03:00:00 2022-02-21 03:00:00\n",
       "164   GOOG          138992 2022-02-22 04:00:00 2022-02-21 04:00:00\n",
       "165   GOOG          138937 2022-02-22 05:00:00 2022-02-21 05:00:00\n",
       "166   GOOG          139169 2022-02-22 06:00:00 2022-02-21 06:00:00\n",
       "167   GOOG          137208 2022-02-22 07:00:00 2022-02-21 07:00:00\n",
       "168   GOOG          138627 2022-02-22 08:00:00 2022-02-21 08:00:00\n",
       "169   GOOG          139072 2022-02-22 09:00:00 2022-02-21 09:00:00\n",
       "170   GOOG          139710 2022-02-22 10:00:00 2022-02-21 10:00:00\n",
       "171   GOOG          138205 2022-02-22 11:00:00 2022-02-21 11:00:00\n",
       "172   GOOG          137780 2022-02-22 12:00:00 2022-02-21 12:00:00\n",
       "173   GOOG          137257 2022-02-22 13:00:00 2022-02-21 13:00:00\n",
       "174   GOOG          137494 2022-02-22 14:00:00 2022-02-21 14:00:00\n",
       "175   GOOG          138911 2022-02-22 15:00:00 2022-02-21 15:00:00\n",
       "176   GOOG          142124 2022-02-22 16:00:00 2022-02-21 16:00:00\n",
       "177   GOOG          143410 2022-02-22 17:00:00 2022-02-21 17:00:00\n",
       "178   GOOG          139614 2022-02-22 18:00:00 2022-02-21 18:00:00\n",
       "179   GOOG          139631 2022-02-22 19:00:00 2022-02-21 19:00:00\n",
       "180   GOOG          140494 2022-02-22 20:00:00 2022-02-21 20:00:00\n",
       "181   GOOG          142147 2022-02-22 21:00:00 2022-02-21 21:00:00\n",
       "182   GOOG          141997 2022-02-22 22:00:00 2022-02-21 22:00:00\n",
       "183   GOOG          142900 2022-02-22 23:00:00 2022-02-21 23:00:00\n",
       "184   GOOG          141246 2022-02-23 00:00:00 2022-02-22 00:00:00\n",
       "185   GOOG          140744 2022-02-23 01:00:00 2022-02-22 01:00:00\n",
       "186   GOOG          141719 2022-02-23 02:00:00 2022-02-22 02:00:00\n",
       "187   GOOG          141292 2022-02-23 03:00:00 2022-02-22 03:00:00\n",
       "188   GOOG          144324 2022-02-23 04:00:00 2022-02-22 04:00:00\n",
       "189   GOOG          144624 2022-02-23 05:00:00 2022-02-22 05:00:00\n",
       "190   GOOG          140536 2022-02-23 06:00:00 2022-02-22 06:00:00\n",
       "191   GOOG          137047 2022-02-23 07:00:00 2022-02-22 07:00:00\n",
       "192   GOOG          129927 2022-02-23 08:00:00 2022-02-22 08:00:00\n",
       "193   GOOG          123786 2022-02-23 09:00:00 2022-02-22 09:00:00\n",
       "194   GOOG          116817 2022-02-23 10:00:00 2022-02-22 10:00:00\n",
       "195   GOOG          111408 2022-02-23 11:00:00 2022-02-22 11:00:00\n",
       "196   GOOG          105688 2022-02-23 12:00:00 2022-02-22 12:00:00\n",
       "197   GOOG          100320 2022-02-23 13:00:00 2022-02-22 13:00:00\n",
       "198   GOOG           94215 2022-02-23 14:00:00 2022-02-22 14:00:00\n",
       "199   GOOG           87154 2022-02-23 15:00:00 2022-02-22 15:00:00\n",
       "200   GOOG           80021 2022-02-23 16:00:00 2022-02-22 16:00:00\n",
       "201   GOOG           73025 2022-02-23 17:00:00 2022-02-22 17:00:00\n",
       "202   GOOG           68834 2022-02-23 18:00:00 2022-02-22 18:00:00\n",
       "203   GOOG           62931 2022-02-23 19:00:00 2022-02-22 19:00:00\n",
       "204   GOOG           57025 2022-02-23 20:00:00 2022-02-22 20:00:00\n",
       "205   GOOG           50590 2022-02-23 21:00:00 2022-02-22 21:00:00\n",
       "206   GOOG           45662 2022-02-23 22:00:00 2022-02-22 22:00:00\n",
       "207   GOOG           39248 2022-02-23 23:00:00 2022-02-22 23:00:00\n",
       "208   GOOG           34910 2022-02-24 00:00:00 2022-02-23 00:00:00\n",
       "209   GOOG           28280 2022-02-24 01:00:00 2022-02-23 01:00:00\n",
       "210   GOOG           21639 2022-02-24 02:00:00 2022-02-23 02:00:00\n",
       "211   GOOG           16534 2022-02-24 03:00:00 2022-02-23 03:00:00\n",
       "212   GOOG            8406 2022-02-24 04:00:00 2022-02-23 04:00:00\n",
       "213   GOOG            1764 2022-02-24 05:00:00 2022-02-23 05:00:00\n",
       "237   MSFT            4453 2022-02-19 19:00:00 2022-02-18 19:00:00\n",
       "236   MSFT           10079 2022-02-19 20:00:00 2022-02-18 20:00:00\n",
       "235   MSFT           16260 2022-02-19 21:00:00 2022-02-18 21:00:00\n",
       "234   MSFT           23727 2022-02-19 22:00:00 2022-02-18 22:00:00\n",
       "233   MSFT           30254 2022-02-19 23:00:00 2022-02-18 23:00:00\n",
       "232   MSFT           36734 2022-02-20 00:00:00 2022-02-19 00:00:00\n",
       "231   MSFT           41980 2022-02-20 01:00:00 2022-02-19 01:00:00\n",
       "230   MSFT           47295 2022-02-20 02:00:00 2022-02-19 02:00:00\n",
       "229   MSFT           53789 2022-02-20 03:00:00 2022-02-19 03:00:00\n",
       "228   MSFT           60349 2022-02-20 04:00:00 2022-02-19 04:00:00\n",
       "227   MSFT           64717 2022-02-20 05:00:00 2022-02-19 05:00:00\n",
       "226   MSFT           72803 2022-02-20 06:00:00 2022-02-19 06:00:00\n",
       "225   MSFT           80336 2022-02-20 07:00:00 2022-02-19 07:00:00\n",
       "224   MSFT           88263 2022-02-20 08:00:00 2022-02-19 08:00:00\n",
       "223   MSFT           94653 2022-02-20 09:00:00 2022-02-19 09:00:00\n",
       "222   MSFT          100791 2022-02-20 10:00:00 2022-02-19 10:00:00\n",
       "221   MSFT          106664 2022-02-20 11:00:00 2022-02-19 11:00:00\n",
       "220   MSFT          111972 2022-02-20 12:00:00 2022-02-19 12:00:00\n",
       "219   MSFT          119034 2022-02-20 13:00:00 2022-02-19 13:00:00\n",
       "218   MSFT          123402 2022-02-20 14:00:00 2022-02-19 14:00:00\n",
       "217   MSFT          129229 2022-02-20 15:00:00 2022-02-19 15:00:00\n",
       "216   MSFT          136202 2022-02-20 16:00:00 2022-02-19 16:00:00\n",
       "215   MSFT          140907 2022-02-20 17:00:00 2022-02-19 17:00:00\n",
       "214   MSFT          147818 2022-02-20 18:00:00 2022-02-19 18:00:00\n",
       "238   MSFT          149605 2022-02-20 19:00:00 2022-02-19 19:00:00\n",
       "239   MSFT          149483 2022-02-20 20:00:00 2022-02-19 20:00:00\n",
       "240   MSFT          148522 2022-02-20 21:00:00 2022-02-19 21:00:00\n",
       "241   MSFT          148338 2022-02-20 22:00:00 2022-02-19 22:00:00\n",
       "242   MSFT          148064 2022-02-20 23:00:00 2022-02-19 23:00:00\n",
       "243   MSFT          146417 2022-02-21 00:00:00 2022-02-20 00:00:00\n",
       "244   MSFT          147801 2022-02-21 01:00:00 2022-02-20 01:00:00\n",
       "245   MSFT          148208 2022-02-21 02:00:00 2022-02-20 02:00:00\n",
       "246   MSFT          147496 2022-02-21 03:00:00 2022-02-20 03:00:00\n",
       "247   MSFT          147514 2022-02-21 04:00:00 2022-02-20 04:00:00\n",
       "248   MSFT          148280 2022-02-21 05:00:00 2022-02-20 05:00:00\n",
       "249   MSFT          147151 2022-02-21 06:00:00 2022-02-20 06:00:00\n",
       "250   MSFT          144821 2022-02-21 07:00:00 2022-02-20 07:00:00\n",
       "251   MSFT          144226 2022-02-21 08:00:00 2022-02-20 08:00:00\n",
       "252   MSFT          144353 2022-02-21 09:00:00 2022-02-20 09:00:00\n",
       "253   MSFT          144213 2022-02-21 10:00:00 2022-02-20 10:00:00\n",
       "254   MSFT          143792 2022-02-21 11:00:00 2022-02-20 11:00:00\n",
       "255   MSFT          144963 2022-02-21 12:00:00 2022-02-20 12:00:00\n",
       "256   MSFT          143910 2022-02-21 13:00:00 2022-02-20 13:00:00\n",
       "257   MSFT          145031 2022-02-21 14:00:00 2022-02-20 14:00:00\n",
       "258   MSFT          146060 2022-02-21 15:00:00 2022-02-20 15:00:00\n",
       "259   MSFT          144470 2022-02-21 16:00:00 2022-02-20 16:00:00\n",
       "260   MSFT          146001 2022-02-21 17:00:00 2022-02-20 17:00:00\n",
       "261   MSFT          145270 2022-02-21 18:00:00 2022-02-20 18:00:00\n",
       "262   MSFT          145588 2022-02-21 19:00:00 2022-02-20 19:00:00\n",
       "263   MSFT          146548 2022-02-21 20:00:00 2022-02-20 20:00:00\n",
       "264   MSFT          149363 2022-02-21 21:00:00 2022-02-20 21:00:00\n",
       "265   MSFT          149496 2022-02-21 22:00:00 2022-02-20 22:00:00\n",
       "266   MSFT          149227 2022-02-21 23:00:00 2022-02-20 23:00:00\n",
       "267   MSFT          149803 2022-02-22 00:00:00 2022-02-21 00:00:00\n",
       "268   MSFT          148586 2022-02-22 01:00:00 2022-02-21 01:00:00\n",
       "269   MSFT          148363 2022-02-22 02:00:00 2022-02-21 02:00:00\n",
       "270   MSFT          148776 2022-02-22 03:00:00 2022-02-21 03:00:00\n",
       "271   MSFT          147901 2022-02-22 04:00:00 2022-02-21 04:00:00\n",
       "272   MSFT          148100 2022-02-22 05:00:00 2022-02-21 05:00:00\n",
       "273   MSFT          145399 2022-02-22 06:00:00 2022-02-21 06:00:00\n",
       "274   MSFT          147365 2022-02-22 07:00:00 2022-02-21 07:00:00\n",
       "275   MSFT          146449 2022-02-22 08:00:00 2022-02-21 08:00:00\n",
       "276   MSFT          146354 2022-02-22 09:00:00 2022-02-21 09:00:00\n",
       "277   MSFT          148072 2022-02-22 10:00:00 2022-02-21 10:00:00\n",
       "278   MSFT          147915 2022-02-22 11:00:00 2022-02-21 11:00:00\n",
       "279   MSFT          147714 2022-02-22 12:00:00 2022-02-21 12:00:00\n",
       "280   MSFT          146672 2022-02-22 13:00:00 2022-02-21 13:00:00\n",
       "281   MSFT          146592 2022-02-22 14:00:00 2022-02-21 14:00:00\n",
       "282   MSFT          144455 2022-02-22 15:00:00 2022-02-21 15:00:00\n",
       "283   MSFT          145961 2022-02-22 16:00:00 2022-02-21 16:00:00\n",
       "284   MSFT          146253 2022-02-22 17:00:00 2022-02-21 17:00:00\n",
       "285   MSFT          145879 2022-02-22 18:00:00 2022-02-21 18:00:00\n",
       "286   MSFT          146811 2022-02-22 19:00:00 2022-02-21 19:00:00\n",
       "287   MSFT          147633 2022-02-22 20:00:00 2022-02-21 20:00:00\n",
       "288   MSFT          146821 2022-02-22 21:00:00 2022-02-21 21:00:00\n",
       "289   MSFT          144789 2022-02-22 22:00:00 2022-02-21 22:00:00\n",
       "290   MSFT          145560 2022-02-22 23:00:00 2022-02-21 23:00:00\n",
       "291   MSFT          147090 2022-02-23 00:00:00 2022-02-22 00:00:00\n",
       "292   MSFT          147620 2022-02-23 01:00:00 2022-02-22 01:00:00\n",
       "293   MSFT          147379 2022-02-23 02:00:00 2022-02-22 02:00:00\n",
       "294   MSFT          146556 2022-02-23 03:00:00 2022-02-22 03:00:00\n",
       "295   MSFT          146086 2022-02-23 04:00:00 2022-02-22 04:00:00\n",
       "296   MSFT          146869 2022-02-23 05:00:00 2022-02-22 05:00:00\n",
       "297   MSFT          144729 2022-02-23 06:00:00 2022-02-22 06:00:00\n",
       "298   MSFT          137560 2022-02-23 07:00:00 2022-02-22 07:00:00\n",
       "299   MSFT          131144 2022-02-23 08:00:00 2022-02-22 08:00:00\n",
       "300   MSFT          124722 2022-02-23 09:00:00 2022-02-22 09:00:00\n",
       "301   MSFT          117006 2022-02-23 10:00:00 2022-02-22 10:00:00\n",
       "302   MSFT          111711 2022-02-23 11:00:00 2022-02-22 11:00:00\n",
       "303   MSFT          105433 2022-02-23 12:00:00 2022-02-22 12:00:00\n",
       "304   MSFT          100466 2022-02-23 13:00:00 2022-02-22 13:00:00\n",
       "305   MSFT           95057 2022-02-23 14:00:00 2022-02-22 14:00:00\n",
       "306   MSFT           90338 2022-02-23 15:00:00 2022-02-22 15:00:00\n",
       "307   MSFT           83449 2022-02-23 16:00:00 2022-02-22 16:00:00\n",
       "308   MSFT           76921 2022-02-23 17:00:00 2022-02-22 17:00:00\n",
       "309   MSFT           71115 2022-02-23 18:00:00 2022-02-22 18:00:00\n",
       "310   MSFT           63625 2022-02-23 19:00:00 2022-02-22 19:00:00\n",
       "311   MSFT           56339 2022-02-23 20:00:00 2022-02-22 20:00:00\n",
       "312   MSFT           49116 2022-02-23 21:00:00 2022-02-22 21:00:00\n",
       "313   MSFT           43732 2022-02-23 22:00:00 2022-02-22 22:00:00\n",
       "314   MSFT           36977 2022-02-23 23:00:00 2022-02-22 23:00:00\n",
       "315   MSFT           30038 2022-02-24 00:00:00 2022-02-23 00:00:00\n",
       "316   MSFT           24095 2022-02-24 01:00:00 2022-02-23 01:00:00\n",
       "317   MSFT           18837 2022-02-24 02:00:00 2022-02-23 02:00:00\n",
       "318   MSFT           13465 2022-02-24 03:00:00 2022-02-23 03:00:00\n",
       "319   MSFT            8232 2022-02-24 04:00:00 2022-02-23 04:00:00\n",
       "320   MSFT            2116 2022-02-24 05:00:00 2022-02-23 05:00:00"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# pip install apache_beam[dataframe]\n",
    "from typing import NamedTuple\n",
    "from apache_beam import window\n",
    "from apache_beam.dataframe.convert import to_dataframe\n",
    "from apache_beam.dataframe.convert import to_pcollection\n",
    "\n",
    "import json\n",
    "from apache_beam.io import ReadFromText, WriteToText\n",
    "from apache_beam.transforms.window import FixedWindows, SlidingWindows\n",
    "from datetime import datetime\n",
    "\n",
    "import apache_beam as beam\n",
    "import apache_beam.runners.interactive.interactive_beam as ib\n",
    "from apache_beam.runners.interactive.interactive_runner import InteractiveRunner\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "class Trade(NamedTuple):\n",
    "    key: str\n",
    "    timestamp: float\n",
    "    symbol: str\n",
    "    event_time: datetime\n",
    "    price: float\n",
    "    quantity: int\n",
    "beam.coders.registry.register_coder(Trade, beam.coders.RowCoder)\n",
    "    \n",
    "def convert_dict_to_stock(record):\n",
    "    x = json.loads(record)\n",
    "    msg = x['value']\n",
    "    msg['key'] = x['key']\n",
    "    msg['timestamp'] = x['timestamp']\n",
    "    return Trade(**msg)\n",
    "    \n",
    "@beam.typehints.with_output_types(Trade)\n",
    "class ParseStockMessage(beam.DoFn):\n",
    "    def process(self, record):\n",
    "        x = json.loads(record)\n",
    "        msg = x['value']\n",
    "        msg['event_time'] = datetime.strptime(msg['event_time'], \"%Y-%m-%d %H:%M:%S\")\n",
    "        msg['key'] = x['key']\n",
    "        msg['timestamp'] = x['timestamp']\n",
    "        t = Trade(**msg)\n",
    "        yield t\n",
    "        \n",
    "def add_timestamp(element):\n",
    "    unix_timestamp = element.timestamp\n",
    "    event_time = element.event_time\n",
    "    return beam.window.TimestampedValue(element, unix_timestamp)\n",
    "        \n",
    "\n",
    "class TradeAggregate(NamedTuple):\n",
    "    window_start: datetime\n",
    "    window_end: datetime\n",
    "    symbol: str\n",
    "    total_quantity: int\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f'{self.window_start.strftime(\"%Y-%m-%d %H:%M:%S\")} - {self.window_end.strftime(\"%Y-%m-%d %H:%M:%S\")} {self.symbol} {self.total_quantity}'\n",
    "\n",
    "    def toRow(self):\n",
    "        return beam.Row(window_start = self.window_start, window_end = self.window_end\n",
    "                        , symbol = self.symbol, total_quantity = self.total_quantity)\n",
    "    \n",
    "beam.coders.registry.register_coder(TradeAggregate, beam.coders.RowCoder)\n",
    "\n",
    "@beam.typehints.with_output_types(TradeAggregate)\n",
    "class AddWindowRange(beam.DoFn):\n",
    "    def process(self, element,  window=beam.DoFn.WindowParam):\n",
    "        window_start = window.start.to_utc_datetime()\n",
    "        window_end = window.end.to_utc_datetime()\n",
    "        yield TradeAggregate(window_start, window_end, element.symbol, element.total_quantity)\n",
    "\n",
    "        \n",
    "stocks_filename = 'trades.txt'\n",
    "with beam.Pipeline(InteractiveRunner()) as p:\n",
    "    s = ( p\n",
    "          | 'Read from Kafka' >> ReadFromText(stocks_filename)\n",
    "          | 'Convert to Trade Object' >> beam.Map(convert_dict_to_stock)\n",
    "          | 'Timestamp the Trade' >> beam.Map(add_timestamp)\n",
    "          | 'Window' >> beam.WindowInto(SlidingWindows(60 * 60 * 24, 60 * 60))\n",
    "          | 'Aggregate 1' >> beam.GroupBy('symbol').aggregate_field('quantity', sum, 'total_quantity')\n",
    "          | 'AddWindowEndTimestamp' >> (beam.ParDo(AddWindowRange()))\n",
    "          | 'To Row' >> beam.Map(lambda x : x.toRow())\n",
    "    )\n",
    "\n",
    "\n",
    "    # Collect the Beam DataFrame into a Pandas DataFrame.\n",
    "    df = ib.collect(s)\n",
    "    df.columns = ['symbol','total_quantity', 'window_start', 'window_end']\n",
    "\n",
    "    # We can now use any Pandas transforms with our data.\n",
    "    df2 = df.sort_values([\"symbol\", \"window_start\"], ascending = (True, True))\n",
    "    display(df2, 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4474ea0f-3284-4d58-b195-b4540f724d4d",
   "metadata": {},
   "source": [
    "## Some Beam runners support the use of SQL and which features they support can also vary. Here's some examples of how to do the same thing using Beam SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be5db6d-671a-4034-862c-cb07e1635066",
   "metadata": {},
   "outputs": [],
   "source": [
    "from apache_beam import window\n",
    "from apache_beam.transforms.sql import SqlTransform\n",
    "\n",
    "import apache_beam as beam\n",
    "import json\n",
    "from apache_beam.io import ReadFromText, WriteToText\n",
    "from apache_beam.transforms.window import FixedWindows\n",
    "from datetime import datetime\n",
    "\n",
    "class Trade(NamedTuple):\n",
    "    key: str\n",
    "    timestamp: float\n",
    "    symbol: str\n",
    "    event_time: str\n",
    "    price: float\n",
    "    quantity: int\n",
    "beam.coders.registry.register_coder(Trade, beam.coders.RowCoder)\n",
    "    \n",
    "@beam.typehints.with_output_types(Trade)\n",
    "class ParseStockMessage(beam.DoFn):\n",
    "    def process(self, record):\n",
    "        x = json.loads(record)\n",
    "        msg = x['value']\n",
    "        msg['key'] = x['key']\n",
    "        msg['timestamp'] = x['timestamp']\n",
    "        t = Trade(**msg)\n",
    "        yield t\n",
    "\n",
    "\n",
    "stocks_filename = 'trades.txt'\n",
    "sql = \"\"\"\n",
    "SELECT \n",
    "  symbol, \n",
    "  TUMBLE_START('INTERVAL 10 MINUTE') as period_start,\n",
    "  TUMBLE_END('INTERVAL 10 MINUTE') as period_end,\n",
    "  SUM(quantity) AS total_quantity\n",
    "FROM PCOLLECTION\n",
    "GROUP BY symbol,\n",
    "  TUMBLE(timestamp, 'INTERVAL 10 MINUTE')\n",
    "\"\"\"\n",
    "\n",
    "# sql = \"\"\"\n",
    "# SELECT symbol, \n",
    "#   SUM(quantity) AS total_quantity\n",
    "# FROM PCOLLECTION\n",
    "# GROUP BY\n",
    "#   symbol\n",
    "# \"\"\"\n",
    "\n",
    "with beam.Pipeline() as p:\n",
    "    k = ( p\n",
    "          | 'Read from Kafka' >> ReadFromText(stocks_filename)\n",
    "          | 'Convert to Trade Object' >> beam.ParDo(ParseStockMessage())\n",
    "          | 'SQL' >> SqlTransform(sql)\n",
    "          | 'Print' >> beam.Map(print)\n",
    "        )\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8586c889-1dcb-43be-a82a-cbee16e6a6b4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Sometimes we need to manually create a schema for a nested repeating because it cannot use a simple string. In this case, we don't really need it but it's included here as a reference in case we do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f17f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from apache_beam.io.gcp.internal.clients import bigquery as bq\n",
    "region_territory_schema = bq.TableSchema()\n",
    "regionid = bq.TableFieldSchema(name = 'regionid', type = 'string', mode = 'required')\n",
    "region_territory_schema.fields.append(regionid)\n",
    "regionname = bq.TableFieldSchema(name = 'regionname', type = 'string', mode='required')\n",
    "region_territory_schema.fields.append(regionname)\n",
    "\n",
    "# A nested field\n",
    "territories = bq.TableFieldSchema(name = 'territories', type = 'record', mode = 'nullable')\n",
    "territoryid = bq.TableFieldSchema(name = 'territoryid', type = 'string', mode = 'required')\n",
    "territories.fields.append(territoryid)\n",
    "territoryname = bq.TableFieldSchema(name = 'territoryname', type = 'string', mode = 'required')\n",
    "territories.fields.append(territoryname)\n",
    "\n",
    "region_territory_schema.fields.append(territories)\n",
    "\n",
    "print(region_territory_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c9db37-f11f-491a-b34c-6a52cebb5cbe",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Incomplete AVRO example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4d2295-86ea-4a50-a094-d4d8eaff9145",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%java nooutput\n",
    "package samples.quickstart;\n",
    "\n",
    "import org.apache.beam.sdk.Pipeline;\n",
    "import org.apache.beam.sdk.values.PCollection;\n",
    "import org.apache.beam.sdk.values.TypeDescriptors;\n",
    "import org.apache.beam.sdk.io.TextIO;\n",
    "import org.apache.beam.sdk.transforms.ParDo;\n",
    "import org.apache.beam.sdk.transforms.DoFn;\n",
    "import org.apache.beam.sdk.transforms.Filter;\n",
    "import org.apache.beam.sdk.transforms.DoFn.ProcessContext;\n",
    "import org.apache.beam.sdk.coders.DefaultCoder;\n",
    "import org.apache.beam.sdk.coders.AvroCoder;\n",
    "import org.apache.beam.sdk.transforms.SerializableFunction;\n",
    "import org.apache.beam.sdk.values.TupleTag;\n",
    "import org.apache.beam.sdk.values.PCollectionTuple;\n",
    "import org.apache.beam.sdk.values.TupleTagList;\n",
    "\n",
    "import org.slf4j.Logger;\n",
    "import org.slf4j.LoggerFactory;\n",
    "\n",
    " Pipeline p = ...;\n",
    "\n",
    "#  // Read Avro-generated classes from files on GCS\n",
    "#  PCollection<AvroAutoGenClass> records =\n",
    "#      p.apply(AvroIO.read(AvroAutoGenClass.class).from(\"gs://my_bucket/path/to/records-*.avro\"));\n",
    "\n",
    "#  // Read GenericRecord's of the given schema from files on GCS\n",
    "#  Schema schema = new Schema.Parser().parse(new File(\"schema.avsc\"));\n",
    "#  PCollection<GenericRecord> records =\n",
    "#      p.apply(AvroIO.readGenericRecords(schema)\n",
    "#                 .from(\"gs://my_bucket/path/to/records-*.avro\"));\n",
    " \n",
    "    \n",
    "  pipeline.apply(\"Read Avro files\",\n",
    "      AvroIO.readGenericRecords(schemaJson).from(options.getInputFile()))\n",
    "      .apply(\"Convert Avro to CSV formatted data\",\n",
    "          ParDo.of(new ConvertAvroToCsv(schemaJson, options.getCsvDelimiter())))\n",
    "      .apply(\"Write CSV formatted data\", TextIO.write().to(options.getOutput())\n",
    "          .withSuffix(\".csv\"));\n",
    "    \n",
    "    \n",
    "    \n",
    "pipeline\n",
    "      .apply(\"Read from Avro\", AvroIO.read(BigtableRow.class).from(options.getInputFilePattern()))\n",
    "      .apply(\n",
    "          \"Transform to Bigtable\",\n",
    "          ParDo.of(\n",
    "              AvroToBigtableFn.createWithSplitLargeRows(\n",
    "                  options.getSplitLargeRows(), MAX_MUTATIONS_PER_ROW)))\n",
    "      .apply(\"Write to Bigtable\", write);\n",
    "\n",
    "  return pipeline.run();\n",
    "\n",
    "\n",
    "\n",
    "public class ReadTerritoriesAvro {\n",
    "\n",
    "    public static void main(String[] args) {\n",
    "        Pipeline p = Pipeline.create();\n",
    "\n",
    "        String territoriesInputFileName = \"datasets/northwind/AVRO/territories/territories.avro\";\n",
    "        String outputsPrefix = \"/tmp/outputs\";\n",
    "\n",
    "        PCollection<AvroAutoGenClass> records= p\n",
    "            .apply(\"Read Avro\", AvroIO.read(AvroAutoGenClass.class).from(territoriesInputFileName));\n",
    "/*\n",
    "            .apply(\"Read\", TextIO.read().from(territoriesInputFileName))\n",
    "            .apply(\"Parse Territory\", ParDo.of(new ParseTerritories()))\n",
    "*/\n",
    "        ;                   \n",
    "        \n",
    "/*            \n",
    "        territories\n",
    "            .apply(\"Upper\", ParDo.of(new DoFn<Territory, Territory>() {\n",
    "                @ProcessElement\n",
    "                public void process(ProcessContext c) {\n",
    "                    Territory t = c.element();\n",
    "                    c.output(new Territory(t.territoryID, t.territoryName.toUpperCase(), t.regionID));\n",
    "                }\n",
    "            }))\n",
    "             .apply(TextIO.<Territory>writeCustomType().to(\"/tmp/territories_upper\").withFormatFunction(new SerializeTerritory()));\n",
    "\n",
    "        territories\n",
    "            .apply(\"Lower\", ParDo.of(new DoFn<Territory, Territory>() {\n",
    "                @ProcessElement\n",
    "                public void process(ProcessContext c) {\n",
    "                    Territory t = c.element();\n",
    "                    c.output(new Territory(t.territoryID, t.territoryName.toLowerCase(), t.regionID));\n",
    "                }\n",
    "            }))\n",
    "             .apply(TextIO.<Territory>writeCustomType().to(\"/tmp/territories_lower\").withFormatFunction(new SerializeTerritory()));\n",
    "\n",
    "        \n",
    "        p.run().waitUntilFinish();\n",
    "    }\n",
    "    \n",
    "    @DefaultCoder(AvroCoder.class)\n",
    "    static class Territory {\n",
    "        Long territoryID;\n",
    "        String territoryName;\n",
    "        Long regionID;\n",
    "        \n",
    "        Territory() {}\n",
    "        \n",
    "        Territory(long territoryID, String territoryName, long regionID) {\n",
    "            this.territoryID = territoryID;\n",
    "            this.territoryName = territoryName;\n",
    "            this.regionID = regionID;\n",
    "        }\n",
    "        \n",
    "        @Override\n",
    "        public String toString() {\n",
    "            return String.format(\"(territoryID = %d, territoryName = %s, regionID = %d)\", territoryID, territoryName, regionID);\n",
    "        }\n",
    "\n",
    "    }\n",
    "    \n",
    "    static class SerializeTerritory implements SerializableFunction<Territory, String> {\n",
    "        @Override\n",
    "        public String apply(Territory input) {\n",
    "          return input.toString();\n",
    "        }\n",
    "    }\n",
    "\n",
    "    static class ParseTerritories extends DoFn<String, Territory> {\n",
    "        private static final Logger LOG = LoggerFactory.getLogger(ParseTerritories.class);\n",
    "\n",
    "        @ProcessElement\n",
    "        public void process(ProcessContext c) {\n",
    "            String[] columns = c.element().split(\",\");\n",
    "            try {\n",
    "                Long territoryID = Long.parseLong(columns[0].trim());\n",
    "                String territoryName = columns[1].trim();\n",
    "                Long regionID = Long.parseLong(columns[2].trim());\n",
    "                c.output(new Territory(territoryID, territoryName, regionID));\n",
    "            } catch (ArrayIndexOutOfBoundsException | NumberFormatException e) {\n",
    "                LOG.info(\"ParseTerritoriesOddEvenSplit: parse error on '\" + c.element() + \"': \" + e.getMessage());\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    */\n",
    "\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71c6953-65a2-4cbd-8f77-92c6f87cfac6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TableReference tableRef = new TableReference();\n",
    "tableRef.setProjectId(\"project-id\");\n",
    "tableRef.setDatasetId(\"dataset-name\");\n",
    "tableRef.setTableId(\"table-name\");\n",
    "\n",
    "List<TableFieldSchema> fieldDefs = new ArrayList<>();\n",
    "fieldDefs.add(new TableFieldSchema().setName(\"column1\").setType(\"STRING\"));\n",
    "fieldDefs.add(new TableFieldSchema().setName(\"column2\").setType(\"FLOAT\"));  \n",
    "For the Pipeline steps,\n",
    "\n",
    "Pipeline pipeLine = Pipeline.create(options);\n",
    "pipeLine\n",
    ".apply(\"ReadMyFile\", \n",
    "        TextIO.read().from(\"path-to-json-file\")) \n",
    "\n",
    ".apply(\"MapToTableRow\", ParDo.of(new DoFn<String, TableRow>() {\n",
    "    @ProcessElement\n",
    "    public void processElement(ProcessContext c) { \n",
    "        Gson gson = new GsonBuilder().create();\n",
    "        HashMap<String, Object> parsedMap = gson.fromJson(c.element().toString(), HashMap.class);\n",
    "\n",
    "        TableRow row = new TableRow();\n",
    "        row.set(\"column1\", parsedMap.get(\"col1\").toString());\n",
    "        row.set(\"column2\", Double.parseDouble(parsedMap.get(\"col2\").toString()));\n",
    "        c.output(row);\n",
    "    }\n",
    "}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488c143c-2b29-4081-b34e-a4bbcef40522",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1cb35e-fdcc-4cd8-a04a-1c65b4531598",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
