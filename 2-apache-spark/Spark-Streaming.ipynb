{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Initialize PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing pyspark\n",
      "packages ['kafka', 'kafka-sql', 'spark-avro']\n",
      "--packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.0.2,org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.1,org.apache.spark:spark-avro_2.12:3.2.1 pyspark-shell\n",
      "pyspark initialized\n"
     ]
    }
   ],
   "source": [
    "import os, sys, json, io\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.utils import StreamingQueryException\n",
    "import sys\n",
    "import json\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = '/usr/bin/python3'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = '/usr/bin/python3'\n",
    "sys.path.append('/class')\n",
    "\n",
    "# Kafka variables\n",
    "brokers = 'localhost:9092'\n",
    "kafka_topic = 'stocks-json'\n",
    "receiver_sleep_time = 4\n",
    "\n",
    "# Connect to Spark \n",
    "if not 'sc' in locals():\n",
    "    from initspark import initspark\n",
    "    sc, spark, config = initspark(packages = ['kafka', 'kafka-sql', 'spark-avro'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Basic batch source example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! hadoop fs -rm -r /territories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "territories = spark.read.csv('file:///class/2-apache-spark/territories.csv', header=True)\n",
    "print(territories)\n",
    "territories.show()\n",
    "territories.write.csv('hdfs://localhost:9000/territories')\n",
    "                      \n",
    "# territories.where('RegionID = 1').show()\n",
    "# territories.groupby('RegionID').count().show()\n",
    "# territories.show()\n",
    "# territories.createOrReplaceTempView('territories')\n",
    "# spark.sql('SELECT regionid, count(*) as cnt from territories group by regionid order by cnt desc').show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create a helper function to stream to a memory table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_memory(df, queryname = 'debug', mode = \"append\"):\n",
    "    # modes are: complete, update, append\n",
    "\n",
    "    # if queryname in spark.catalog.listTables():\n",
    "    #     spark.catalog.dropTempView(queryname)\n",
    "    \n",
    "    query = (df.writeStream \n",
    "            .format(\"memory\")\n",
    "            .queryName(queryname)\n",
    "            .outputMode(mode)\n",
    "            .start()\n",
    "            )\n",
    "    return query\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Define a streaming source and create a temp view to receive the results for debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (spark.readStream \n",
    "    .format(\"kafka\") \n",
    "    .option(\"kafka.bootstrap.servers\", brokers) \n",
    "    .option(\"subscribe\", kafka_topic) \n",
    "    .option(\"startingOffsets\", \"earliest\")\n",
    "    .option(\"failOnDataLoss\", False)\n",
    "    .load()\n",
    "    )\n",
    "\n",
    "# df.createOrReplaceTempView('table')\n",
    "# df1 = spark.sql(\"\"\"SELECT 'new data' as newfield, * from table\"\"\")\n",
    "\n",
    "if 'debug1' in locals():\n",
    "    debug1.stop()\n",
    "\n",
    "df1 = df.selectExpr(\"UPPER(CAST(value AS STRING)) as value\")\n",
    "\n",
    "debug1 = write_memory(df1, 'debug1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Query from the memory stream like it's a temporary view using `spark.sql`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from debug1\").take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## You can stop and restart a memory stream whenever you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug1.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug1 = write_memory(df1, 'debug1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from debug1\").take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Spark SQL magic is also quite helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext sparksql_magic\n",
    "# pip install sparksql-magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sparksql\n",
    "select * from debug1 order by value limit 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Stop a memory stream when you don't need it, as it can consume a lot of memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug1.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's try reading AVRO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stock_schema {\n",
      "    \"namespace\": \"stock.avro\",\n",
      "    \"type\": \"record\",\n",
      "    \"name\": \"Stock\",\n",
      "    \"fields\": [\n",
      "        {\"name\": \"event_time\", \"type\": \"string\"},\n",
      "        {\"name\": \"symbol\",  \"type\": \"string\"},\n",
      "        {\"name\": \"price\", \"type\": \"float\"},\n",
      "        {\"name\": \"quantity\", \"type\": \"int\"}\n",
      "    ]\n",
      "}\n",
      "stock_struct StructType(List(StructField(event_time,StringType,true),StructField(symbol,StringType,true),StructField(price,FloatType,true),StructField(quantity,IntegerType,true)))\n"
     ]
    }
   ],
   "source": [
    "stock_schema = open(\"stock.avsc\", \"r\").read()\n",
    "print('stock_schema', stock_schema)\n",
    "stock_struct = spark.read.format(\"avro\").option(\"avroSchema\", stock_schema).load().schema\n",
    "print('stock_struct', stock_struct)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df DataFrame[key: binary, value: binary, topic: string, partition: int, offset: bigint, timestamp: timestamp, timestampType: int]\n"
     ]
    }
   ],
   "source": [
    "brokers = 'localhost:9092'\n",
    "kafka_topic = 'stocks-avro'\n",
    "receiver_sleep_time = 4\n",
    "\n",
    "df = (spark.readStream \n",
    "    .format(\"kafka\") \n",
    "    .option(\"kafka.bootstrap.servers\", brokers) \n",
    "    .option(\"subscribe\", kafka_topic) \n",
    "    .option(\"startingOffsets\", \"earliest\")\n",
    "    .option(\"failOnDataLoss\", False)\n",
    "    .load()\n",
    "    )\n",
    "print('df', df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'debug2' in locals():\n",
    "    debug2.stop()\n",
    "debug2 = write_memory(df, 'debug2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr style=\"border-bottom: 1px solid\"><td style=\"font-weight: bold\">timestamp</td><td style=\"font-weight: bold\">key</td><td style=\"font-weight: bold\">value</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sparksql\n",
    "select timestamp, key, value from debug2 order by timestamp desc limit 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug2.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "import uuid\n",
    "\n",
    "def convert_uuid(value):\n",
    "    # value is a bytearray in this case coming from spark\n",
    "    ret = uuid.UUID(bytes = bytes(value))\n",
    "    return str(ret)\n",
    "\n",
    "convert_uuid_udf = udf(convert_uuid, StringType())\n",
    "    \n",
    "from pyspark.sql.avro.functions import from_avro, to_avro\n",
    "stock_schema = \"\"\"{\n",
    "    \"type\": \"record\",\n",
    "    \"name\": \"Stock\",\n",
    "    \"fields\": [\n",
    "        {\"name\": \"event_time\", \"type\": \"string\"},\n",
    "        {\"name\": \"symbol\",  \"type\": \"string\"},\n",
    "        {\"name\": \"price\", \"type\": \"float\"},\n",
    "        {\"name\": \"quantity\", \"type\": \"int\"}\n",
    "    ]\n",
    "}\"\"\"\n",
    "\n",
    "#df3 = df.select(\"timestamp\", \"key\", \"value\")\n",
    "\n",
    "df3 = df.select(\"timestamp\"\n",
    "                , convert_uuid_udf(col(\"key\")).alias(\"key\")\n",
    "                , from_avro(df.value, stock_schema, options = {\"mode\":\"PERMISSIVE\"}).alias(\"value\"))\n",
    "\n",
    "df3 = df3.select(*(df3.columns), col(\"value.*\")).drop('value')\n",
    "#df3 = df3.where(\"symbol = 'GOOG'\")\n",
    "df3.createOrReplaceTempView('stocks')\n",
    "#df3 = spark.sql('SELECT symbol, count(*) as cnt, sum(quantity) as qty from stocks group by symbol')\n",
    "\n",
    "print('df3', df3)\n",
    "if 'debug3' in locals():\n",
    "    debug3.stop()\n",
    "debug3 = write_memory(df3, 'debug3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sparksql\n",
    "select * from debug3 order by timestamp desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug3.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here's the same thing for the JSON stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stock_schema {\n",
      "    \"namespace\": \"stock.avro\",\n",
      "    \"type\": \"record\",\n",
      "    \"name\": \"Stock\",\n",
      "    \"fields\": [\n",
      "        {\"name\": \"event_time\", \"type\": \"string\"},\n",
      "        {\"name\": \"symbol\",  \"type\": \"string\"},\n",
      "        {\"name\": \"price\", \"type\": \"float\"},\n",
      "        {\"name\": \"quantity\", \"type\": \"int\"}\n",
      "    ]\n",
      "}\n",
      "stock_struct StructType(List(StructField(event_time,StringType,true),StructField(symbol,StringType,true),StructField(price,FloatType,true),StructField(quantity,IntegerType,true)))\n",
      "df DataFrame[key: binary, value: binary, topic: string, partition: int, offset: bigint, timestamp: timestamp, timestampType: int]\n",
      "df1 DataFrame[key: string, timestamp: timestamp, value: string]\n",
      "df2 DataFrame[key: string, timestamp: timestamp, value2: struct<event_time:string,symbol:string,price:float,quantity:int>]\n",
      "df4 DataFrame[key: string, timestamp: timestamp, event_time: string, symbol: string, price: float, quantity: int]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "import uuid\n",
    "\n",
    "brokers = 'localhost:9092'\n",
    "kafka_topic = 'stocks-json'\n",
    "receiver_sleep_time = 4\n",
    "\n",
    "stock_schema = open(\"stock.avsc\", \"r\").read()\n",
    "print('stock_schema', stock_schema)\n",
    "\n",
    "stock_struct = spark.read.format(\"avro\").option(\"avroSchema\", stock_schema).load().schema\n",
    "print('stock_struct', stock_struct)\n",
    "\n",
    "df = (spark.readStream \n",
    "    .format(\"kafka\") \n",
    "    .option(\"kafka.bootstrap.servers\", brokers) \n",
    "    .option(\"subscribe\", kafka_topic) \n",
    "    .option(\"startingOffsets\", \"earliest\")\n",
    "    .option(\"failOnDataLoss\", False)\n",
    "#    .option(\"kafka.group.id\", \"stock-json-spark-group\")\n",
    "    .load()\n",
    "    )\n",
    "print('df', df)\n",
    "\n",
    "\n",
    "def convert_uuid(value):\n",
    "    # value is a bytearray in this case coming from spark\n",
    "    ret = uuid.UUID(bytes = bytes(value))\n",
    "    return str(ret)\n",
    "\n",
    "convert_uuid_udf = udf(convert_uuid, StringType())\n",
    "\n",
    "# keep the key and timestamp and convert the value from bytes to string\n",
    "#df1 = df.select(col(\"key\"), \"timestamp\", expr(\"CAST(value AS STRING) as value\"))\n",
    "df1 = df.select(convert_uuid_udf(col(\"key\")).alias(\"key\"), \"timestamp\", expr(\"CAST(value AS STRING) as value\"))\n",
    "print('df1', df1)\n",
    "\n",
    "# cast the string json to a struct\n",
    "# keep all the columns we selected and convery the JSON string into a struct object and remove the string version\n",
    "df2 = df1.select(*df1.columns, from_json(df1.value, stock_struct).alias(\"value2\")).drop('value')\n",
    "print('df2', df2)\n",
    "\n",
    "# flatten the struct to a normal DataFrame\n",
    "df4 = df2.select(*(df2.columns), col(\"value2.*\")).drop('value2')\n",
    "print('df4', df4)\n",
    "\n",
    "if 'debug4' in locals():\n",
    "    debug4.stop()\n",
    "    \n",
    "debug4 = write_memory(df4, 'debug4')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "only showing top 20 row(s)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table><tr style=\"border-bottom: 1px solid\"><td style=\"font-weight: bold\">key</td><td style=\"font-weight: bold\">timestamp</td><td style=\"font-weight: bold\">event_time</td><td style=\"font-weight: bold\">symbol</td><td style=\"font-weight: bold\">price</td><td style=\"font-weight: bold\">quantity</td></tr><tr><td>ffd48443-b110-4207-a1f1-d178dc213f4b</td><td>2022-03-07 20:35:44.543000</td><td>2022-03-07 20:35:44</td><td>AAPL</td><td>214.05999755859375</td><td>192</td></tr><tr><td>c1d3c92e-8226-47b8-8cad-d229c20bfe52</td><td>2022-03-07 20:35:44.542000</td><td>2022-03-07 20:35:44</td><td>GOOG</td><td>121.73999786376953</td><td>381</td></tr><tr><td>972c287e-2c9c-432f-a5ca-4e43ccb98e8f</td><td>2022-03-07 20:35:44.536000</td><td>2022-03-07 20:35:44</td><td>MSFT</td><td>148.6699981689453</td><td>988</td></tr><tr><td>5188cf74-f083-45b0-b629-b2a87253eb89</td><td>2022-03-07 20:35:40.537000</td><td>2022-03-07 20:35:40</td><td>AAPL</td><td>181.80999755859375</td><td>289</td></tr><tr><td>df085c5c-5de3-45d5-bb62-90f99b10de98</td><td>2022-03-07 20:35:40.537000</td><td>2022-03-07 20:35:40</td><td>GOOG</td><td>100.87999725341797</td><td>990</td></tr><tr><td>cf979a9b-7384-43ac-a22f-88041f069a44</td><td>2022-03-07 20:35:40.531000</td><td>2022-03-07 20:35:40</td><td>MSFT</td><td>256.04998779296875</td><td>883</td></tr><tr><td>e2485552-66c9-4a26-b9b9-d083fa523f31</td><td>2022-03-07 20:35:36.533000</td><td>2022-03-07 20:35:36</td><td>AAPL</td><td>194.4600067138672</td><td>646</td></tr><tr><td>f4404ef3-5823-45aa-84a4-4167a2dd2763</td><td>2022-03-07 20:35:36.533000</td><td>2022-03-07 20:35:36</td><td>GOOG</td><td>224.5500030517578</td><td>774</td></tr><tr><td>b4161ac5-4cd0-4eb0-abc6-e98b7145a719</td><td>2022-03-07 20:35:36.526000</td><td>2022-03-07 20:35:36</td><td>MSFT</td><td>172.9499969482422</td><td>412</td></tr><tr><td>bbc69563-aaa5-4dff-8e5b-9ec6f89fade0</td><td>2022-03-07 20:35:32.529000</td><td>2022-03-07 20:35:32</td><td>GOOG</td><td>238.22000122070312</td><td>488</td></tr><tr><td>543579a5-3fa8-4a9b-bfde-4c1d45ee43c5</td><td>2022-03-07 20:35:32.528000</td><td>2022-03-07 20:35:32</td><td>AAPL</td><td>137.9199981689453</td><td>962</td></tr><tr><td>a8c99f61-69aa-42a4-b30d-3d7d2027d87f</td><td>2022-03-07 20:35:32.525000</td><td>2022-03-07 20:35:32</td><td>MSFT</td><td>197.8300018310547</td><td>226</td></tr><tr><td>8aa4b13f-a412-42cc-9d3a-458f8152c243</td><td>2022-03-07 20:35:28.524000</td><td>2022-03-07 20:35:28</td><td>AAPL</td><td>235.6199951171875</td><td>15</td></tr><tr><td>30e24a7f-1474-4442-84fb-b5dc1e21ae00</td><td>2022-03-07 20:35:28.525000</td><td>2022-03-07 20:35:28</td><td>GOOG</td><td>168.3699951171875</td><td>202</td></tr><tr><td>3404eea0-9edd-4c3b-ac4a-c34db42c8627</td><td>2022-03-07 20:35:28.520000</td><td>2022-03-07 20:35:28</td><td>MSFT</td><td>171.47000122070312</td><td>429</td></tr><tr><td>50825174-83fe-43af-b7d9-839cf3f11682</td><td>2022-03-07 20:35:24.521000</td><td>2022-03-07 20:35:24</td><td>GOOG</td><td>243.1999969482422</td><td>510</td></tr><tr><td>e0cf1739-55c9-4fab-a15c-35a98684818f</td><td>2022-03-07 20:35:24.522000</td><td>2022-03-07 20:35:24</td><td>AAPL</td><td>102.6500015258789</td><td>612</td></tr><tr><td>a3c5c6e6-f655-4459-a686-eb81e875f044</td><td>2022-03-07 20:35:24.518000</td><td>2022-03-07 20:35:24</td><td>MSFT</td><td>168.08999633789062</td><td>918</td></tr><tr><td>4a95e1d9-2a5a-4275-b7b0-41aef3f9149a</td><td>2022-03-07 20:35:20.513000</td><td>2022-03-07 20:35:20</td><td>MSFT</td><td>291.29998779296875</td><td>749</td></tr><tr><td>01dd77e6-5be7-4a46-8248-020176a44db3</td><td>2022-03-07 20:35:20.516000</td><td>2022-03-07 20:35:20</td><td>GOOG</td><td>111.58000183105469</td><td>441</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sparksql \n",
    "select * from debug4 order by event_time desc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug4.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Now that we have a normal DataFrame, let's manipulate it how we want and write the results out to another stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[window: struct<start:timestamp,end:timestamp>, symbol: string, sum: bigint]\n"
     ]
    }
   ],
   "source": [
    "fixed_window = (df4.select(\"timestamp\", \"symbol\", \"quantity\")\n",
    "        .withWatermark(\"timestamp\", \"10 seconds\") \n",
    "        .groupBy(window(\"timestamp\", \"10 seconds\").alias(\"window\"), \"symbol\") \n",
    "        .agg(sum(\"quantity\").alias(\"sum\"))\n",
    "        )\n",
    "print(fixed_window)\n",
    "\n",
    "if 'debug5' in locals():\n",
    "    debug5.stop()\n",
    "debug5 = write_memory(fixed_window, 'debug5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can see that we get aggregate by symbol every ten seconds. This data can be written off somewhere like a SQL or NoSQL database or forwarded as a new message to create a streaming aggregation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr style=\"border-bottom: 1px solid\"><td style=\"font-weight: bold\">window</td><td style=\"font-weight: bold\">symbol</td><td style=\"font-weight: bold\">sum</td></tr><tr><td>Row(start=datetime.datetime(2022, 3, 7, 20, 35, 30), end=datetime.datetime(2022, 3, 7, 20, 35, 40))</td><td>AAPL</td><td>1608</td></tr><tr><td>Row(start=datetime.datetime(2022, 3, 7, 20, 35, 30), end=datetime.datetime(2022, 3, 7, 20, 35, 40))</td><td>GOOG</td><td>1262</td></tr><tr><td>Row(start=datetime.datetime(2022, 3, 7, 20, 35, 30), end=datetime.datetime(2022, 3, 7, 20, 35, 40))</td><td>MSFT</td><td>638</td></tr><tr><td>Row(start=datetime.datetime(2022, 3, 7, 20, 35, 20), end=datetime.datetime(2022, 3, 7, 20, 35, 30))</td><td>AAPL</td><td>1510</td></tr><tr><td>Row(start=datetime.datetime(2022, 3, 7, 20, 35, 20), end=datetime.datetime(2022, 3, 7, 20, 35, 30))</td><td>GOOG</td><td>1153</td></tr><tr><td>Row(start=datetime.datetime(2022, 3, 7, 20, 35, 20), end=datetime.datetime(2022, 3, 7, 20, 35, 30))</td><td>MSFT</td><td>2096</td></tr><tr><td>Row(start=datetime.datetime(2022, 3, 7, 20, 35, 10), end=datetime.datetime(2022, 3, 7, 20, 35, 20))</td><td>AAPL</td><td>1434</td></tr><tr><td>Row(start=datetime.datetime(2022, 3, 7, 20, 35, 10), end=datetime.datetime(2022, 3, 7, 20, 35, 20))</td><td>GOOG</td><td>1785</td></tr><tr><td>Row(start=datetime.datetime(2022, 3, 7, 20, 35, 10), end=datetime.datetime(2022, 3, 7, 20, 35, 20))</td><td>MSFT</td><td>1032</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sparksql\n",
    "select * from debug5 order by window desc, symbol limit 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug5.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sliding windows are similar except you give it two parameters, the first is the total length of the window and the second is the refresh interval. In this case, the windows will overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[window: struct<start:timestamp,end:timestamp>, symbol: string, sum: bigint]\n"
     ]
    }
   ],
   "source": [
    "sliding_window = (df4.select(\"timestamp\", \"symbol\",\"quantity\")\n",
    "        .withWatermark(\"timestamp\", \"10 seconds\") \n",
    "        .groupBy(window(\"timestamp\", \"30 seconds\", \"10 seconds\").alias(\"window\"), \"symbol\") \n",
    "        .agg(sum(\"quantity\").alias(\"sum\"))\n",
    "        )\n",
    "print(sliding_window)\n",
    "\n",
    "debug6 = write_memory(sliding_window, 'debug6')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "only showing top 20 row(s)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table><tr style=\"border-bottom: 1px solid\"><td style=\"font-weight: bold\">window</td><td style=\"font-weight: bold\">symbol</td><td style=\"font-weight: bold\">sum</td></tr><tr><td>Row(start=datetime.datetime(2022, 3, 7, 20, 36, 20), end=datetime.datetime(2022, 3, 7, 20, 36, 50))</td><td>AAPL</td><td>4353</td></tr><tr><td>Row(start=datetime.datetime(2022, 3, 7, 20, 36, 20), end=datetime.datetime(2022, 3, 7, 20, 36, 50))</td><td>GOOG</td><td>3177</td></tr><tr><td>Row(start=datetime.datetime(2022, 3, 7, 20, 36, 20), end=datetime.datetime(2022, 3, 7, 20, 36, 50))</td><td>MSFT</td><td>3869</td></tr><tr><td>Row(start=datetime.datetime(2022, 3, 7, 20, 36, 10), end=datetime.datetime(2022, 3, 7, 20, 36, 40))</td><td>AAPL</td><td>3463</td></tr><tr><td>Row(start=datetime.datetime(2022, 3, 7, 20, 36, 10), end=datetime.datetime(2022, 3, 7, 20, 36, 40))</td><td>GOOG</td><td>2647</td></tr><tr><td>Row(start=datetime.datetime(2022, 3, 7, 20, 36, 10), end=datetime.datetime(2022, 3, 7, 20, 36, 40))</td><td>MSFT</td><td>3885</td></tr><tr><td>Row(start=datetime.datetime(2022, 3, 7, 20, 36), end=datetime.datetime(2022, 3, 7, 20, 36, 30))</td><td>AAPL</td><td>3539</td></tr><tr><td>Row(start=datetime.datetime(2022, 3, 7, 20, 36), end=datetime.datetime(2022, 3, 7, 20, 36, 30))</td><td>GOOG</td><td>4049</td></tr><tr><td>Row(start=datetime.datetime(2022, 3, 7, 20, 36), end=datetime.datetime(2022, 3, 7, 20, 36, 30))</td><td>MSFT</td><td>4730</td></tr><tr><td>Row(start=datetime.datetime(2022, 3, 7, 20, 35, 50), end=datetime.datetime(2022, 3, 7, 20, 36, 20))</td><td>AAPL</td><td>3473</td></tr><tr><td>Row(start=datetime.datetime(2022, 3, 7, 20, 35, 50), end=datetime.datetime(2022, 3, 7, 20, 36, 20))</td><td>GOOG</td><td>3897</td></tr><tr><td>Row(start=datetime.datetime(2022, 3, 7, 20, 35, 50), end=datetime.datetime(2022, 3, 7, 20, 36, 20))</td><td>MSFT</td><td>3982</td></tr><tr><td>Row(start=datetime.datetime(2022, 3, 7, 20, 35, 40), end=datetime.datetime(2022, 3, 7, 20, 36, 10))</td><td>AAPL</td><td>3209</td></tr><tr><td>Row(start=datetime.datetime(2022, 3, 7, 20, 35, 40), end=datetime.datetime(2022, 3, 7, 20, 36, 10))</td><td>GOOG</td><td>4955</td></tr><tr><td>Row(start=datetime.datetime(2022, 3, 7, 20, 35, 40), end=datetime.datetime(2022, 3, 7, 20, 36, 10))</td><td>MSFT</td><td>4569</td></tr><tr><td>Row(start=datetime.datetime(2022, 3, 7, 20, 35, 30), end=datetime.datetime(2022, 3, 7, 20, 36))</td><td>AAPL</td><td>3704</td></tr><tr><td>Row(start=datetime.datetime(2022, 3, 7, 20, 35, 30), end=datetime.datetime(2022, 3, 7, 20, 36))</td><td>GOOG</td><td>4544</td></tr><tr><td>Row(start=datetime.datetime(2022, 3, 7, 20, 35, 30), end=datetime.datetime(2022, 3, 7, 20, 36))</td><td>MSFT</td><td>3566</td></tr><tr><td>Row(start=datetime.datetime(2022, 3, 7, 20, 35, 20), end=datetime.datetime(2022, 3, 7, 20, 35, 50))</td><td>AAPL</td><td>3819</td></tr><tr><td>Row(start=datetime.datetime(2022, 3, 7, 20, 35, 20), end=datetime.datetime(2022, 3, 7, 20, 35, 50))</td><td>GOOG</td><td>4319</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sparksql\n",
    "select * from debug6 order by window desc, symbol limit 21\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug6.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session Window is similar but used to group data that represents a continuous stream of activity. The time specifies a timeout period or period of inactivity that indicates when a session should end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_window = (df4.select(\"timestamp\", \"symbol\",\"quantity\")\n",
    "        .withWatermark(\"timestamp\", \"10 seconds\") \n",
    "        .groupBy(session_window(\"timestamp\", \"5 minutes\").alias(\"window\"), \"symbol\") \n",
    "        .agg(sum(\"quantity\").alias(\"sum\"))\n",
    "        )\n",
    "print(session_window)\n",
    "\n",
    "debug7 = write_memory(session_window, 'debug7')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's join the streaming aggregation with a static reference table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = sc.parallelize([('AAPL', 'Apple'), ('MSFT', 'Microsoft'), ('GOOG','Google')])\n",
    "stocks = spark.createDataFrame(x, 'symbol:string, name:string')\n",
    "stocks.createOrReplaceTempView('stocks')\n",
    "fixed_window.createOrReplaceTempView('trades')\n",
    "\n",
    "joined_aggregate = spark.sql(\"\"\"\n",
    "SELECT t.*, s.name\n",
    "FROM trades as t\n",
    "JOIN stocks as s on t.symbol = s.symbol\n",
    "\"\"\")\n",
    "\n",
    "debug8 = write_memory(joined_aggregate, 'debug8')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr style=\"border-bottom: 1px solid\"><td style=\"font-weight: bold\">window</td><td style=\"font-weight: bold\">symbol</td><td style=\"font-weight: bold\">sum</td><td style=\"font-weight: bold\">name</td></tr><tr><td>Row(start=datetime.datetime(2022, 3, 7, 20, 37, 30), end=datetime.datetime(2022, 3, 7, 20, 37, 40))</td><td>AAPL</td><td>1345</td><td>Apple</td></tr><tr><td>Row(start=datetime.datetime(2022, 3, 7, 20, 37, 30), end=datetime.datetime(2022, 3, 7, 20, 37, 40))</td><td>GOOG</td><td>265</td><td>Google</td></tr><tr><td>Row(start=datetime.datetime(2022, 3, 7, 20, 37, 30), end=datetime.datetime(2022, 3, 7, 20, 37, 40))</td><td>MSFT</td><td>1307</td><td>Microsoft</td></tr><tr><td>Row(start=datetime.datetime(2022, 3, 7, 20, 37, 20), end=datetime.datetime(2022, 3, 7, 20, 37, 30))</td><td>AAPL</td><td>1108</td><td>Apple</td></tr><tr><td>Row(start=datetime.datetime(2022, 3, 7, 20, 37, 20), end=datetime.datetime(2022, 3, 7, 20, 37, 30))</td><td>GOOG</td><td>971</td><td>Google</td></tr><tr><td>Row(start=datetime.datetime(2022, 3, 7, 20, 37, 20), end=datetime.datetime(2022, 3, 7, 20, 37, 30))</td><td>MSFT</td><td>1448</td><td>Microsoft</td></tr><tr><td>Row(start=datetime.datetime(2022, 3, 7, 20, 37, 10), end=datetime.datetime(2022, 3, 7, 20, 37, 20))</td><td>AAPL</td><td>1312</td><td>Apple</td></tr><tr><td>Row(start=datetime.datetime(2022, 3, 7, 20, 37, 10), end=datetime.datetime(2022, 3, 7, 20, 37, 20))</td><td>GOOG</td><td>511</td><td>Google</td></tr><tr><td>Row(start=datetime.datetime(2022, 3, 7, 20, 37, 10), end=datetime.datetime(2022, 3, 7, 20, 37, 20))</td><td>MSFT</td><td>1078</td><td>Microsoft</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sparksql\n",
    "select * from debug8 order by window desc, symbol limit 9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug8.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ConsumerRecord(topic='stocks-avro', partition=0, offset=40679, timestamp=1645243362535, timestamp_type=0, key=b'\\xd6\\x0cgMs<By\\xb8\\xcaR\\x02\\xe0\\xfa\\x93\\x14', value=b'Obj\\x01\\x04\\x14avro.codec\\x08null\\x16avro.schema\\xc6\\x03{\"type\": \"record\", \"name\": \"Stock\", \"namespace\": \"stock.avro\", \"fields\": [{\"type\": \"string\", \"name\": \"event_time\"}, {\"type\": \"string\", \"name\": \"symbol\"}, {\"type\": \"float\", \"name\": \"price\"}, {\"type\": \"int\", \"name\": \"quantity\"}]}\\x00$\\x7f#w\\xeaD\\xdchK\\xc5!\\xf85\\x10\\xc8J\\x02>&2022-02-19 04:02:42\\x08MSFT\\xa4p\\xdfB\\xf0\\t$\\x7f#w\\xeaD\\xdchK\\xc5!\\xf85\\x10\\xc8J', headers=[], checksum=None, serialized_key_size=16, serialized_value_size=328, serialized_header_size=-1)\n",
    "ConsumerRecord(topic='stocks-avro', partition=0, offset=40679, timestamp=1645243362535, timestamp_type=0, key=b'\\xd6\\x0cgMs<By\\xb8\\xcaR\\x02\\xe0\\xfa\\x93\\x14', value=b'Obj\\x01\\x04\\x14avro.codec\\x08null\\x16avro.schema\\xc6\\x03{\"type\": \"record\", \"name\": \"Stock\", \"namespace\": \"stock.avro\", \"fields\": [{\"type\": \"string\", \"name\": \"event_time\"}, {\"type\": \"string\", \"name\": \"symbol\"}, {\"type\": \"float\", \"name\": \"price\"}, {\"type\": \"int\", \"name\": \"quantity\"}]}\\x00$\\x7f#w\\xeaD\\xdchK\\xc5!\\xf85\\x10\\xc8J\\x02>&2022-02-19 04:02:42\\x08MSFT\\xa4p\\xdfB\\xf0\\t$\\x7f#w\\xeaD\\xdchK\\xc5!\\xf85\\x10\\xc8J', headers=[], checksum=None, serialized_key_size=16, serialized_value_size=328, serialized_header_size=-1)\n",
    "\n",
    "ConsumerRecord(topic='stocks-avro', partition=0, offset=40814, timestamp=1645243470645, timestamp_type=0, key=b'\\xc8\\xeb\\xc2\\xe9O\\xaaJ\\x86\\x83\\x85\\xb9\\xd7\\xf46\\xea\\x8f', value=b'Obj\\x01\\x04\\x14avro.codec\\x08null\\x16avro.schema\\xc6\\x03{\"type\": \"record\", \"name\": \"Stock\", \"namespace\": \"stock.avro\", \"fields\": [{\"type\": \"string\", \"name\": \"event_time\"}, {\"type\": \"string\", \"name\": \"symbol\"}, {\"type\": \"float\", \"name\": \"price\"}, {\"type\": \"int\", \"name\": \"quantity\"}]}\\x00\\xe8\\xf5x\\r\\xbf\\x8aC\\x98&\\xaf\\x13iz\\x9dp\\x13\\x02>&2022-02-19 04:04:30\\x08MSFT\\xc3\\xf5\\x11C\\xac\\x04\\xe8\\xf5x\\r\\xbf\\x8aC\\x98&\\xaf\\x13iz\\x9dp\\x13', headers=[], checksum=None, serialized_key_size=16, serialized_value_size=328, serialized_header_size=-1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
