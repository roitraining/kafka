{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Initialize PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing pyspark\n",
      "packages ['kafka', 'kafka-sql', 'spark-avro']\n",
      "--packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.0.2,org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.1,org.apache.spark:spark-avro_2.12:3.2.1 pyspark-shell\n",
      "pyspark initialized\n"
     ]
    }
   ],
   "source": [
    "import os, sys, json, io\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.utils import StreamingQueryException\n",
    "import sys\n",
    "import json\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = '/usr/bin/python3'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = '/usr/bin/python3'\n",
    "sys.path.append('/class')\n",
    "\n",
    "# Kafka variables\n",
    "brokers = 'localhost:9092'\n",
    "kafka_topic = 'stocks-json'\n",
    "receiver_sleep_time = 4\n",
    "\n",
    "# Connect to Spark \n",
    "if not 'sc' in locals():\n",
    "    from initspark import initspark\n",
    "    sc, spark, config = initspark(packages = ['kafka', 'kafka-sql', 'spark-avro'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Basic batch source example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /territories\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -rm -r /territories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method csv in module pyspark.sql.readwriter:\n",
      "\n",
      "csv(path, schema=None, sep=None, encoding=None, quote=None, escape=None, comment=None, header=None, inferSchema=None, ignoreLeadingWhiteSpace=None, ignoreTrailingWhiteSpace=None, nullValue=None, nanValue=None, positiveInf=None, negativeInf=None, dateFormat=None, timestampFormat=None, maxColumns=None, maxCharsPerColumn=None, maxMalformedLogPerPartition=None, mode=None, columnNameOfCorruptRecord=None, multiLine=None, charToEscapeQuoteEscaping=None, samplingRatio=None, enforceSchema=None, emptyValue=None, locale=None, lineSep=None, pathGlobFilter=None, recursiveFileLookup=None, modifiedBefore=None, modifiedAfter=None, unescapedQuoteHandling=None) method of pyspark.sql.readwriter.DataFrameReader instance\n",
      "    Loads a CSV file and returns the result as a  :class:`DataFrame`.\n",
      "    \n",
      "    This function will go through the input once to determine the input schema if\n",
      "    ``inferSchema`` is enabled. To avoid going through the entire data once, disable\n",
      "    ``inferSchema`` option or specify the schema explicitly using ``schema``.\n",
      "    \n",
      "    .. versionadded:: 2.0.0\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    path : str or list\n",
      "        string, or list of strings, for input path(s),\n",
      "        or RDD of Strings storing CSV rows.\n",
      "    schema : :class:`pyspark.sql.types.StructType` or str, optional\n",
      "        an optional :class:`pyspark.sql.types.StructType` for the input schema\n",
      "        or a DDL-formatted string (For example ``col0 INT, col1 DOUBLE``).\n",
      "    \n",
      "    Other Parameters\n",
      "    ----------------\n",
      "    Extra options\n",
      "        For the extra options, refer to\n",
      "        `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-csv.html#data-source-option>`_\n",
      "        in the version you use.\n",
      "    \n",
      "        .. # noqa\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> df = spark.read.csv('python/test_support/sql/ages.csv')\n",
      "    >>> df.dtypes\n",
      "    [('_c0', 'string'), ('_c1', 'string')]\n",
      "    >>> rdd = sc.textFile('python/test_support/sql/ages.csv')\n",
      "    >>> df2 = spark.read.csv(rdd)\n",
      "    >>> df2.dtypes\n",
      "    [('_c0', 'string'), ('_c1', 'string')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(spark.read.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+\n",
      "|regionid|cnt|\n",
      "+--------+---+\n",
      "|       2| 11|\n",
      "|       4|  6|\n",
      "|       1|  5|\n",
      "|       3|  5|\n",
      "+--------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "territories = spark.read.csv('file:///class/2-apache-spark/territories.csv', header=True, inferSchema = True)\n",
    "#print(territories)\n",
    "#territories.show()\n",
    "#territories.write.csv('hdfs://localhost:9000/territories', sep = '|')\n",
    "#territories.write.json('hdfs://localhost:9000/territories_json')\n",
    "#territories.write.parquet('hdfs://localhost:9000/territories_parquet')\n",
    "                      \n",
    "# territories.where('RegionID = 1').show()\n",
    "# territories.groupby('RegionID').count().show()\n",
    "# t2 = territories.where(\"TerritoryName like '%a%'\")\n",
    "# t3 = t2.groupby('RegionID').count()\n",
    "# t4 = t3.filter('count > 5')\n",
    "\n",
    "# (spark.read.csv('file:///class/2-apache-spark/territories.csv'\n",
    "#                , header=True, inferSchema = True)\n",
    "#       .where(\"TerritoryName like '%a%'\")\n",
    "#       .groupby('RegionID').count()\n",
    "#       .filter('count > 5')\n",
    "#       .show()\n",
    "# )\n",
    "\n",
    "#t4.show()\n",
    "# territories.show()\n",
    "territories.createOrReplaceTempView('territories')\n",
    "spark.sql(\"\"\"SELECT regionid, count(*) as cnt \n",
    "          from territories \n",
    "          where territoryname like '%a%' \n",
    "          group by regionid \n",
    "          order by cnt desc\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LAB: ## \n",
    "### The folder /class/datasets/northwind/ contains sample data in a variety of formats. CSV contains comma separated data without headers, CSVHeaders is the same data with headers. JSON, AVO, ORC, PARQUET folders have the data in those formats. \n",
    "1. Read the CSVHeaders version of Categories into a DataFrame variable called categories. Print and show it to see what the data looks like.\n",
    "2. Read the JSON version of Products into a DataFrame variable called products. Print and show it to see what the data looks like.\n",
    "3. Using spark sql, turn each DataFrame variable into a temporary view and write a SQL statement to join the two into a new DataFrame variable that shows the ProductID, ProductName, CategoryID and CategoryName.\n",
    "4. Using dot syntax take the joined DataFrame and count how many items are in each category.\n",
    "5. Write the results to HDFS in a folder called /category_count\n",
    "<p></p>\n",
    "\n",
    "<details><summary>Click for <b>hint</b></summary>\n",
    "<p>1. Use spark.read.csv and tell it to the file has headers and infer the schema. Use file:/// prefix to point to the files.</p>\n",
    "    <p>2. Use spark.read.json</p>\n",
    "    <p>3. Turn both DataFrames into a temporary view and write a standard SQL JOIN</p>\n",
    "    <p>4. Use .grouby and .count</p>\n",
    "    <p>5. Take the DataFrame and call .write.csv and save the results using hdfs:// prefix</p>\n",
    "</details>\n",
    "\n",
    "<details><summary>Click for <b>code</b></summary>\n",
    "<p>\n",
    "\n",
    "```python\n",
    "categories = spark.read.csv('file:///class/datasets/northwind/CSVHeaders/categories', header = True, inferSchema = True)\n",
    "products = spark.read.json('file:///class/datasets/northwind/JSON/products')\n",
    "categories.createOrReplaceTempView('categories')\n",
    "products.createOrReplaceTempView('products')\n",
    "prod_cat = spark.sql(\"\"\"SELECT c.CategoryID, c.CategoryName, p.ProductID, p.ProductName\n",
    "FROM categories AS c\n",
    "JOIN Products AS p ON c.CategoryID = p.CategoryID\n",
    "\"\"\")\n",
    "category_count = prod_cat.groupby('CategoryName').count()\n",
    "category_count.show()\n",
    "category_count.write.csv('hdfs://localhost:9000/category_count')\n",
    "```\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create a helper function to stream to a memory table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_memory(df, queryname = 'debug', mode = \"append\"):\n",
    "    # modes are: complete, update, append\n",
    "\n",
    "    # if queryname in spark.catalog.listTables():\n",
    "    #     spark.catalog.dropTempView(queryname)\n",
    "    \n",
    "    query = (df.writeStream \n",
    "            .format(\"memory\")\n",
    "            .queryName(queryname)\n",
    "            .outputMode(mode)\n",
    "            .start()\n",
    "            )\n",
    "    return query\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Define a streaming source and create a temp view to receive the results for debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (spark.readStream \n",
    "    .format(\"kafka\") \n",
    "    .option(\"kafka.bootstrap.servers\", brokers) \n",
    "    .option(\"subscribe\", kafka_topic) \n",
    "    .option(\"startingOffsets\", \"earliest\")\n",
    "    .option(\"failOnDataLoss\", False)\n",
    "    .load()\n",
    "    )\n",
    "\n",
    "# df.createOrReplaceTempView('table')\n",
    "# df1 = spark.sql(\"\"\"SELECT 'new data' as newfield, * from table\"\"\")\n",
    "\n",
    "if 'debug1' in locals():\n",
    "    debug1.stop()\n",
    "\n",
    "df1 = df.selectExpr(\"UPPER(CAST(value AS STRING)) as value\")\n",
    "\n",
    "debug1 = write_memory(df1, 'debug1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Query from the memory stream like it's a temporary view using `spark.sql`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"select * from debug1\").take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## You can stop and restart a memory stream whenever you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug1.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug1 = write_memory(df1, 'debug1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"select * from debug1\").take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Spark SQL magic is also quite helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext sparksql_magic\n",
    "# pip install sparksql-magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr style=\"border-bottom: 1px solid\"><td style=\"font-weight: bold\">value</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sparksql\n",
    "select * from debug1 order by value limit 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Stop a memory stream when you don't need it, as it can consume a lot of memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug1.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's try reading AVRO. First we are using schemaless AVRO messages so we need to read in a schema from a file or repository to apply to the message body to parse it into a structured format. This trick will take an AVRO schema file and turn it into a JSON string which can then be converted into a Spark struct object suitable for use in the deserializing process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stock_schema {\n",
      "    \"namespace\": \"stock.avro\",\n",
      "    \"type\": \"record\",\n",
      "    \"name\": \"Stock\",\n",
      "    \"fields\": [\n",
      "        {\"name\": \"event_time\", \"type\": \"string\"},\n",
      "        {\"name\": \"symbol\",  \"type\": \"string\"},\n",
      "        {\"name\": \"price\", \"type\": \"float\"},\n",
      "        {\"name\": \"quantity\", \"type\": \"int\"}\n",
      "    ]\n",
      "}\n",
      "stock_struct StructType(List(StructField(event_time,StringType,true),StructField(symbol,StringType,true),StructField(price,FloatType,true),StructField(quantity,IntegerType,true)))\n"
     ]
    }
   ],
   "source": [
    "stock_schema = open(\"stock.avsc\", \"r\").read()\n",
    "print('stock_schema', stock_schema)\n",
    "stock_struct = spark.read.format(\"avro\").option(\"avroSchema\", stock_schema).load().schema\n",
    "print('stock_struct', stock_struct)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## LAB: ## \n",
    "### Using the stocks-json example do the following to start up and read an AVRO stream instead:\n",
    "1. Open a new terminal window and cd /class/1-producers-and-consumers\n",
    "2. Run the 3-python-kafka-avro-producer.py to start making messages\n",
    "3. Open another terminal window and cd /class//1-producers-and-consumers\n",
    "4. Run 4-python-kafka-avro-consumer.py to show the messages are being created and sent.\n",
    "5. In the cells below write Spark code based on the JSON example that can read the AVRO stream and simple display it. \n",
    "\n",
    "<p></p>\n",
    "\n",
    "<details><summary>Click for <b>code</b></summary>\n",
    "<p>\n",
    "\n",
    "```python\n",
    "brokers = 'localhost:9092'\n",
    "kafka_topic = 'stocks-avro'\n",
    "receiver_sleep_time = 4\n",
    "\n",
    "df = (spark.readStream \n",
    "    .format(\"kafka\") \n",
    "    .option(\"kafka.bootstrap.servers\", brokers) \n",
    "    .option(\"subscribe\", kafka_topic) \n",
    "    .option(\"startingOffsets\", \"earliest\")\n",
    "    .option(\"failOnDataLoss\", False)\n",
    "    .load()\n",
    "    )\n",
    "print('df', df)\n",
    "\n",
    "if 'debug2' in locals():\n",
    "    debug2.stop()\n",
    "debug2 = write_memory(df, 'debug2')\n",
    "\n",
    "# In a new cell\n",
    "%%sparksql\n",
    "select timestamp, key, value from debug2 order by timestamp desc limit 10    \n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr style=\"border-bottom: 1px solid\"><td style=\"font-weight: bold\">timestamp</td><td style=\"font-weight: bold\">key</td><td style=\"font-weight: bold\">value</td></tr><tr><td>2022-03-07 20:24:37.801000</td><td>bytearray(b&#x27;qo3F,\\xb5C\\xc3\\x84\\x8e\\x8ay\\xf9\\x05G\\xbb&#x27;)</td><td>bytearray(b&#x27;&amp;2022-03-07 20:24:37\\x08GOOG\\xae\\x87ZC\\xf0\\x04&#x27;)</td></tr><tr><td>2022-03-07 20:24:37.798000</td><td>bytearray(b&#x27;&amp;(\\xcef\\xdd\\tF\\x1b\\xb4\\xb1\\x99qkV\\xfb\\xa7&#x27;)</td><td>bytearray(b&#x27;&amp;2022-03-07 20:24:37\\x08MSFT\\xae\\x07\\x17C\\xc2\\n&#x27;)</td></tr><tr><td>2022-03-07 20:24:37.797000</td><td>bytearray(b&#x27;R?\\xdb\\xebp?E\\xd7\\x828\\xb0h\\x13\\xbcK)&#x27;)</td><td>bytearray(b&#x27;&amp;2022-03-07 20:24:37\\x08AAPL\\x00@tC\\x9e\\x0b&#x27;)</td></tr><tr><td>2022-03-07 20:24:33.797000</td><td>bytearray(b&#x27;\\xa0\\x07Z\\x87M!K\\xf3\\x80\\xdb\\x18\\xd6\\xe0`\\xb5\\x0b&#x27;)</td><td>bytearray(b&#x27;&amp;2022-03-07 20:24:33\\x08GOOG\\x1fEeC\\xca\\x0f&#x27;)</td></tr><tr><td>2022-03-07 20:24:33.794000</td><td>bytearray(b&#x27;\\xa7$\\x0f}\\xe7\\x83E\\x08\\x997\\x9eT\\xa9\\xfb\\x08\\x91&#x27;)</td><td>bytearray(b&#x27;&amp;2022-03-07 20:24:33\\x08AAPL=\\x8a\\xd1B\\xa2\\x08&#x27;)</td></tr><tr><td>2022-03-07 20:24:33.793000</td><td>bytearray(b&#x27;n\\x14\\xf1Xf\\xa8O\\xcc\\xa7\\x89\\xbe`\\x03\\xbd\\x86\\x04&#x27;)</td><td>bytearray(b&#x27;&amp;2022-03-07 20:24:33\\x08MSFT\\xf6\\xa8fC\\xca\\r&#x27;)</td></tr><tr><td>2022-03-07 20:24:29.793000</td><td>bytearray(b&#x27;N4\\x1f\\xb8\\xf1\\x8cKS\\xb3\\x03\\xed\\x1e\\xb5C\\x8c\\x07&#x27;)</td><td>bytearray(b&#x27;&amp;2022-03-07 20:24:29\\x08GOOG\\xcd\\x0c(C\\x84\\r&#x27;)</td></tr><tr><td>2022-03-07 20:24:29.790000</td><td>bytearray(b&#x27;A\\xd8o\\xed\\xf7eE\\x9c\\xb6\\xebVw{\\x9a\\x04\\xf0&#x27;)</td><td>bytearray(b&#x27;&amp;2022-03-07 20:24:29\\x08MSFT\\xcdL\\x13C\\xb6\\x08&#x27;)</td></tr><tr><td>2022-03-07 20:24:29.789000</td><td>bytearray(b&#x27;B\\xe7\\x9b\\xa9\\x1f2I\\xf8\\x99\\x9e@\\x12n\\xf1\\x06\\xeb&#x27;)</td><td>bytearray(b&#x27;&amp;2022-03-07 20:24:29\\x08AAPL\\xcd\\xcc\\x87C\\xe6\\x06&#x27;)</td></tr><tr><td>2022-03-07 20:24:25.789000</td><td>bytearray(b&#x27;\\xd8\\x91\\x98V\\xdc\\x1eI3\\xa5}v\\xc7OJ*\\xab&#x27;)</td><td>bytearray(b&#x27;&amp;2022-03-07 20:24:25\\x08GOOG\\x85+KC\\x80\\n&#x27;)</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop the memory stream object you created for the AVRO lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug2.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps are to parse the message body and keep whatever metadata from the message we're interested in, and turn that into a DataFrame object we can work with and do whatever we want with the results. In this case in involves converting the key back to a UUID and the message body into Python dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df3 DataFrame[timestamp: timestamp, key: string, event_time: string, symbol: string, price: float, quantity: int]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "import uuid\n",
    "\n",
    "def convert_uuid(value):\n",
    "    # value is a bytearray in this case coming from spark\n",
    "    ret = uuid.UUID(bytes = bytes(value))\n",
    "    return str(ret)\n",
    "\n",
    "convert_uuid_udf = udf(convert_uuid, StringType())\n",
    "    \n",
    "from pyspark.sql.avro.functions import from_avro, to_avro\n",
    "\n",
    "# Could have read the schema from a file as shown earlier but just put it here so it's easier to see it.\n",
    "stock_schema = \"\"\"{\n",
    "    \"type\": \"record\",\n",
    "    \"name\": \"Stock\",\n",
    "    \"fields\": [\n",
    "        {\"name\": \"event_time\", \"type\": \"string\"},\n",
    "        {\"name\": \"symbol\",  \"type\": \"string\"},\n",
    "        {\"name\": \"price\", \"type\": \"float\"},\n",
    "        {\"name\": \"quantity\", \"type\": \"int\"}\n",
    "    ]\n",
    "}\"\"\"\n",
    "\n",
    "# Select the three columns we're interested in\n",
    "# df3 = df.select(\"timestamp\", \"key\", \"value\")\n",
    "\n",
    "# Do some manipulating on the columns to make them into something meaningful\n",
    "df3 = df.select(\"timestamp\"\n",
    "                , convert_uuid_udf(col(\"key\")).alias(\"key\")\n",
    "                , from_avro(df.value, stock_schema, options = {\"mode\":\"PERMISSIVE\"}).alias(\"value\"))\n",
    "\n",
    "# We end up with three columns called timestamp, key and value, but value is a single column of the datatype\n",
    "# struct, so this trick will flatten it out so we end up with six normal columns.\n",
    "df3 = df3.select(*(df3.columns), col(\"value.*\")).drop('value')\n",
    "\n",
    "#df3 = df3.where(\"symbol = 'GOOG'\")\n",
    "df3.createOrReplaceTempView('stocks')\n",
    "\n",
    "print('df3', df3)\n",
    "if 'debug3' in locals():\n",
    "    debug3.stop()\n",
    "debug3 = write_memory(df3, 'debug3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr style=\"border-bottom: 1px solid\"><td style=\"font-weight: bold\">timestamp</td><td style=\"font-weight: bold\">key</td><td style=\"font-weight: bold\">event_time</td><td style=\"font-weight: bold\">symbol</td><td style=\"font-weight: bold\">price</td><td style=\"font-weight: bold\">quantity</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sparksql\n",
    "select * from debug3 order by timestamp desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug3.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here's the same thing for the JSON stream using the from_json function instead of from_avro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stock_schema {\n",
      "    \"namespace\": \"stock.avro\",\n",
      "    \"type\": \"record\",\n",
      "    \"name\": \"Stock\",\n",
      "    \"fields\": [\n",
      "        {\"name\": \"event_time\", \"type\": \"string\"},\n",
      "        {\"name\": \"symbol\",  \"type\": \"string\"},\n",
      "        {\"name\": \"price\", \"type\": \"float\"},\n",
      "        {\"name\": \"quantity\", \"type\": \"int\"}\n",
      "    ]\n",
      "}\n",
      "stock_struct StructType(List(StructField(event_time,StringType,true),StructField(symbol,StringType,true),StructField(price,FloatType,true),StructField(quantity,IntegerType,true)))\n",
      "df DataFrame[key: binary, value: binary, topic: string, partition: int, offset: bigint, timestamp: timestamp, timestampType: int]\n",
      "df1 DataFrame[key: string, timestamp: timestamp, value: string]\n",
      "df2 DataFrame[key: string, timestamp: timestamp, value2: struct<event_time:string,symbol:string,price:float,quantity:int>]\n",
      "df4 DataFrame[key: string, timestamp: timestamp, event_time: string, symbol: string, price: float, quantity: int]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "import uuid\n",
    "\n",
    "brokers = 'localhost:9092'\n",
    "kafka_topic = 'stocks-json'\n",
    "receiver_sleep_time = 4\n",
    "\n",
    "stock_schema = open(\"stock.avsc\", \"r\").read()\n",
    "print('stock_schema', stock_schema)\n",
    "\n",
    "stock_struct = spark.read.format(\"avro\").option(\"avroSchema\", stock_schema).load().schema\n",
    "print('stock_struct', stock_struct)\n",
    "\n",
    "df = (spark.readStream \n",
    "    .format(\"kafka\") \n",
    "    .option(\"kafka.bootstrap.servers\", brokers) \n",
    "    .option(\"subscribe\", kafka_topic) \n",
    "    .option(\"startingOffsets\", \"earliest\")\n",
    "    .option(\"failOnDataLoss\", False)\n",
    "#    .option(\"kafka.group.id\", \"stock-json-spark-group\")\n",
    "    .load()\n",
    "    )\n",
    "print('df', df)\n",
    "\n",
    "\n",
    "def convert_uuid(value):\n",
    "    # value is a bytearray in this case coming from spark\n",
    "    ret = uuid.UUID(bytes = bytes(value))\n",
    "    return str(ret)\n",
    "\n",
    "convert_uuid_udf = udf(convert_uuid, StringType())\n",
    "\n",
    "# keep the key and timestamp and convert the value from bytes to string\n",
    "#df1 = df.select(col(\"key\"), \"timestamp\", expr(\"CAST(value AS STRING) as value\"))\n",
    "df1 = df.select(convert_uuid_udf(col(\"key\")).alias(\"key\"), \"timestamp\", expr(\"CAST(value AS STRING) as value\"))\n",
    "print('df1', df1)\n",
    "\n",
    "# cast the string json to a struct\n",
    "# keep all the columns we selected and convery the JSON string into a struct object and remove the string version\n",
    "df2 = df1.select(*df1.columns, from_json(df1.value, stock_struct).alias(\"value2\")).drop('value')\n",
    "print('df2', df2)\n",
    "\n",
    "# flatten the struct to a normal DataFrame\n",
    "df4 = df2.select(*(df2.columns), col(\"value2.*\")).drop('value2')\n",
    "print('df4', df4)\n",
    "\n",
    "if 'debug4' in locals():\n",
    "    debug4.stop()\n",
    "    \n",
    "debug4 = write_memory(df4, 'debug4')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "only showing top 20 row(s)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table><tr style=\"border-bottom: 1px solid\"><td style=\"font-weight: bold\">key</td><td style=\"font-weight: bold\">timestamp</td><td style=\"font-weight: bold\">event_time</td><td style=\"font-weight: bold\">symbol</td><td style=\"font-weight: bold\">price</td><td style=\"font-weight: bold\">quantity</td></tr><tr><td>efefebba-940c-4d09-9ebd-7b369d23a389</td><td>2022-03-09 12:12:54.881000</td><td>2022-03-09 12:12:54</td><td>AAPL</td><td>272.2699890136719</td><td>279</td></tr><tr><td>8a9229ac-2b31-4877-b56a-12f265b4fd25</td><td>2022-03-09 12:12:54.833000</td><td>2022-03-09 12:12:54</td><td>MSFT</td><td>146.0800018310547</td><td>557</td></tr><tr><td>96e822b1-dfd9-4d24-97e3-282578ff1062</td><td>2022-03-09 12:12:54.797000</td><td>2022-03-09 12:12:54</td><td>GOOG</td><td>151.11000061035156</td><td>629</td></tr><tr><td>a1cf5931-cc05-47e0-8aa6-87ee634e6b12</td><td>2022-03-09 12:12:50.793000</td><td>2022-03-09 12:12:50</td><td>GOOG</td><td>237.11000061035156</td><td>514</td></tr><tr><td>9d6d5588-537d-4cbd-b58c-4fb828b588aa</td><td>2022-03-09 12:12:50.877000</td><td>2022-03-09 12:12:50</td><td>AAPL</td><td>151.05999755859375</td><td>332</td></tr><tr><td>c9babdd2-d3d0-4453-ae8a-863b654bc45d</td><td>2022-03-09 12:12:50.829000</td><td>2022-03-09 12:12:50</td><td>MSFT</td><td>140.60000610351562</td><td>998</td></tr><tr><td>b1aa0482-e60d-4007-94c5-cbd5cba2fc3a</td><td>2022-03-09 12:12:46.825000</td><td>2022-03-09 12:12:46</td><td>MSFT</td><td>264.82000732421875</td><td>712</td></tr><tr><td>4d84d6c9-42b2-48dc-a151-cbcc35d437f0</td><td>2022-03-09 12:12:46.874000</td><td>2022-03-09 12:12:46</td><td>AAPL</td><td>291.510009765625</td><td>572</td></tr><tr><td>908c9580-0d95-43d3-bb9d-0f208a51ff04</td><td>2022-03-09 12:12:46.789000</td><td>2022-03-09 12:12:46</td><td>GOOG</td><td>249.1300048828125</td><td>22</td></tr><tr><td>374688d8-a6c2-4dc5-8f31-b30a4872cd1b</td><td>2022-03-09 12:12:42.869000</td><td>2022-03-09 12:12:42</td><td>AAPL</td><td>250.5500030517578</td><td>101</td></tr><tr><td>216b96cc-1b03-48ed-99cf-3a1f3d8903cf</td><td>2022-03-09 12:12:42.819000</td><td>2022-03-09 12:12:42</td><td>MSFT</td><td>254.32000732421875</td><td>74</td></tr><tr><td>a7ecffaf-790a-4880-a087-1e196d23da29</td><td>2022-03-09 12:12:42.784000</td><td>2022-03-09 12:12:42</td><td>GOOG</td><td>247.57000732421875</td><td>641</td></tr><tr><td>69fa113f-0621-477d-b2c0-2008f64ff999</td><td>2022-03-09 12:12:38.817000</td><td>2022-03-09 12:12:38</td><td>MSFT</td><td>139.4499969482422</td><td>627</td></tr><tr><td>afb8cb77-4527-4f18-93a7-3ac40c214ba3</td><td>2022-03-09 12:12:38.866000</td><td>2022-03-09 12:12:38</td><td>AAPL</td><td>123.66000366210938</td><td>774</td></tr><tr><td>a752280e-bc65-4c99-914a-ca3792971acf</td><td>2022-03-09 12:12:38.779000</td><td>2022-03-09 12:12:38</td><td>GOOG</td><td>270.42999267578125</td><td>85</td></tr><tr><td>7e119448-fe0a-44b1-8281-126d35839fe4</td><td>2022-03-09 12:12:34.813000</td><td>2022-03-09 12:12:34</td><td>MSFT</td><td>197.4199981689453</td><td>705</td></tr><tr><td>ec89a89f-1430-4761-8a8e-7948de5bc56e</td><td>2022-03-09 12:12:34.861000</td><td>2022-03-09 12:12:34</td><td>AAPL</td><td>249.25</td><td>138</td></tr><tr><td>625319c3-2431-4223-9e5f-210d4fa8438b</td><td>2022-03-09 12:12:34.775000</td><td>2022-03-09 12:12:34</td><td>GOOG</td><td>270.489990234375</td><td>517</td></tr><tr><td>07808935-5c11-426a-ac06-1258d8bbc11b</td><td>2022-03-09 12:12:30.857000</td><td>2022-03-09 12:12:30</td><td>AAPL</td><td>214.50999450683594</td><td>403</td></tr><tr><td>61f49a85-5920-4fb6-818d-8371666eb691</td><td>2022-03-09 12:12:30.770000</td><td>2022-03-09 12:12:30</td><td>GOOG</td><td>116.9800033569336</td><td>986</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sparksql \n",
    "select * from debug4 order by event_time desc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug4.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Now that we have a normal DataFrame, let's manipulate it how we want and write the results out to another stream. Try the following, it will fail. Read why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[key: string, timestamp: timestamp, event_time: string, symbol: string, price: float, quantity: int]\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "Queries with streaming sources must be executed with writeStream.start();\nkafka",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-d491aef0a8fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdf4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateOrReplaceTempView\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'stocks2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf5\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'SELECT symbol, count(*) as cnt, sum(quantity) as qty from stocks2 group by symbol'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdf5\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 494\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    495\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1284\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1286\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Queries with streaming sources must be executed with writeStream.start();\nkafka"
     ]
    }
   ],
   "source": [
    "print(df4)\n",
    "df4.createOrReplaceTempView('stocks2')\n",
    "spark.sql('SELECT symbol, count(*) as cnt, sum(quantity) as qty from stocks2 group by symbol').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming sources can't be aggregated unles we addd a window to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[window: struct<start:timestamp,end:timestamp>, symbol: string, sum: bigint]\n"
     ]
    }
   ],
   "source": [
    "fixed_window = (df4.select(\"timestamp\", \"symbol\", \"quantity\")\n",
    "        .withWatermark(\"timestamp\", \"10 seconds\") \n",
    "        .groupBy(window(\"timestamp\", \"10 seconds\").alias(\"window\"), \"symbol\") \n",
    "        .agg(sum(\"quantity\").alias(\"sum\"))\n",
    "        )\n",
    "print(fixed_window)\n",
    "\n",
    "if 'debug5' in locals():\n",
    "    debug5.stop()\n",
    "debug5 = write_memory(fixed_window, 'debug5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can see that we get aggregate by symbol every ten seconds. This data can be written off somewhere like a SQL or NoSQL database or forwarded as a new message to create a streaming aggregation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr style=\"border-bottom: 1px solid\"><td style=\"font-weight: bold\">window</td><td style=\"font-weight: bold\">symbol</td><td style=\"font-weight: bold\">sum</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sparksql\n",
    "select * from debug5 order by window desc, symbol limit 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug5.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sliding windows are similar except you give it two parameters, the first is the total length of the window and the second is the refresh interval. In this case, the windows will overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[window: struct<start:timestamp,end:timestamp>, symbol: string, sum: bigint]\n"
     ]
    }
   ],
   "source": [
    "sliding_window = (df4.select(\"timestamp\", \"symbol\",\"quantity\")\n",
    "        .withWatermark(\"timestamp\", \"10 seconds\") \n",
    "        .groupBy(window(\"timestamp\", \"30 seconds\", \"10 seconds\").alias(\"window\"), \"symbol\") \n",
    "        .agg(sum(\"quantity\").alias(\"sum\"))\n",
    "        )\n",
    "print(sliding_window)\n",
    "\n",
    "debug6 = write_memory(sliding_window, 'debug6')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr style=\"border-bottom: 1px solid\"><td style=\"font-weight: bold\">window</td><td style=\"font-weight: bold\">symbol</td><td style=\"font-weight: bold\">sum</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sparksql\n",
    "select * from debug6 order by window desc, symbol limit 21\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug6.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session Window is similar but used to group data that represents a continuous stream of activity. The time specifies a timeout period or period of inactivity that indicates when a session should end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[window: struct<start:timestamp,end:timestamp>, symbol: string, sum: bigint]\n"
     ]
    }
   ],
   "source": [
    "session_window = (df4.select(\"timestamp\", \"symbol\",\"quantity\")\n",
    "        .withWatermark(\"timestamp\", \"10 seconds\") \n",
    "        .groupBy(session_window(\"timestamp\", \"5 minutes\").alias(\"window\"), \"symbol\") \n",
    "        .agg(sum(\"quantity\").alias(\"sum\"))\n",
    "        )\n",
    "print(session_window)\n",
    "\n",
    "debug7 = write_memory(session_window, 'debug7')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's join the streaming aggregation with a static reference table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = sc.parallelize([('AAPL', 'Apple'), ('MSFT', 'Microsoft'), ('GOOG','Google')])\n",
    "stocks = spark.createDataFrame(x, 'symbol:string, name:string')\n",
    "stocks.createOrReplaceTempView('stocks')\n",
    "fixed_window.createOrReplaceTempView('trades')\n",
    "\n",
    "joined_aggregate = spark.sql(\"\"\"\n",
    "SELECT t.*, s.name\n",
    "FROM trades as t\n",
    "JOIN stocks as s on t.symbol = s.symbol\n",
    "\"\"\")\n",
    "\n",
    "debug8 = write_memory(joined_aggregate, 'debug8')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr style=\"border-bottom: 1px solid\"><td style=\"font-weight: bold\">window</td><td style=\"font-weight: bold\">symbol</td><td style=\"font-weight: bold\">sum</td><td style=\"font-weight: bold\">name</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sparksql\n",
    "select * from debug8 order by window desc, symbol limit 9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug8.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample of what a consume record in AVRO looks like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ConsumerRecord(topic='stocks-avro', partition=0, offset=40679, timestamp=1645243362535, timestamp_type=0, key=b'\\xd6\\x0cgMs<By\\xb8\\xcaR\\x02\\xe0\\xfa\\x93\\x14', value=b'Obj\\x01\\x04\\x14avro.codec\\x08null\\x16avro.schema\\xc6\\x03{\"type\": \"record\", \"name\": \"Stock\", \"namespace\": \"stock.avro\", \"fields\": [{\"type\": \"string\", \"name\": \"event_time\"}, {\"type\": \"string\", \"name\": \"symbol\"}, {\"type\": \"float\", \"name\": \"price\"}, {\"type\": \"int\", \"name\": \"quantity\"}]}\\x00$\\x7f#w\\xeaD\\xdchK\\xc5!\\xf85\\x10\\xc8J\\x02>&2022-02-19 04:02:42\\x08MSFT\\xa4p\\xdfB\\xf0\\t$\\x7f#w\\xeaD\\xdchK\\xc5!\\xf85\\x10\\xc8J', headers=[], checksum=None, serialized_key_size=16, serialized_value_size=328, serialized_header_size=-1)\n",
    "ConsumerRecord(topic='stocks-avro', partition=0, offset=40679, timestamp=1645243362535, timestamp_type=0, key=b'\\xd6\\x0cgMs<By\\xb8\\xcaR\\x02\\xe0\\xfa\\x93\\x14', value=b'Obj\\x01\\x04\\x14avro.codec\\x08null\\x16avro.schema\\xc6\\x03{\"type\": \"record\", \"name\": \"Stock\", \"namespace\": \"stock.avro\", \"fields\": [{\"type\": \"string\", \"name\": \"event_time\"}, {\"type\": \"string\", \"name\": \"symbol\"}, {\"type\": \"float\", \"name\": \"price\"}, {\"type\": \"int\", \"name\": \"quantity\"}]}\\x00$\\x7f#w\\xeaD\\xdchK\\xc5!\\xf85\\x10\\xc8J\\x02>&2022-02-19 04:02:42\\x08MSFT\\xa4p\\xdfB\\xf0\\t$\\x7f#w\\xeaD\\xdchK\\xc5!\\xf85\\x10\\xc8J', headers=[], checksum=None, serialized_key_size=16, serialized_value_size=328, serialized_header_size=-1)\n",
    "\n",
    "ConsumerRecord(topic='stocks-avro', partition=0, offset=40814, timestamp=1645243470645, timestamp_type=0, key=b'\\xc8\\xeb\\xc2\\xe9O\\xaaJ\\x86\\x83\\x85\\xb9\\xd7\\xf46\\xea\\x8f', value=b'Obj\\x01\\x04\\x14avro.codec\\x08null\\x16avro.schema\\xc6\\x03{\"type\": \"record\", \"name\": \"Stock\", \"namespace\": \"stock.avro\", \"fields\": [{\"type\": \"string\", \"name\": \"event_time\"}, {\"type\": \"string\", \"name\": \"symbol\"}, {\"type\": \"float\", \"name\": \"price\"}, {\"type\": \"int\", \"name\": \"quantity\"}]}\\x00\\xe8\\xf5x\\r\\xbf\\x8aC\\x98&\\xaf\\x13iz\\x9dp\\x13\\x02>&2022-02-19 04:04:30\\x08MSFT\\xc3\\xf5\\x11C\\xac\\x04\\xe8\\xf5x\\r\\xbf\\x8aC\\x98&\\xaf\\x13iz\\x9dp\\x13', headers=[], checksum=None, serialized_key_size=16, serialized_value_size=328, serialized_header_size=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
