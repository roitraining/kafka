{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Initialize PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, json, io\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.utils import StreamingQueryException\n",
    "import sys\n",
    "import json\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = '/usr/bin/python3'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = '/usr/bin/python3'\n",
    "sys.path.append('/class')\n",
    "\n",
    "# Kafka variables\n",
    "brokers = 'localhost:9092'\n",
    "kafka_topic = 'stocks-json'\n",
    "receiver_sleep_time = 4\n",
    "\n",
    "# Connect to Spark \n",
    "if not 'sc' in locals():\n",
    "    from initspark import initspark\n",
    "    sc, spark, config = initspark(packages = ['kafka', 'kafka-sql', 'spark-avro'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Basic batch source example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! hadoop fs -rm -r /territories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(spark.read.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "territories = spark.read.csv('file:///class/bigdata/1-apache-spark/territories.csv'\n",
    "                             , header=True, inferSchema = True)\n",
    "print(territories)\n",
    "territories.show()\n",
    "#territories.write.csv('hdfs://localhost:9000/territories', sep = '|')\n",
    "#territories.write.json('hdfs://localhost:9000/territories_json')\n",
    "#territories.write.parquet('hdfs://localhost:9000/territories_parquet')\n",
    "                      \n",
    "# territories.where('RegionID = 1').show()\n",
    "# territories.groupby('RegionID').count().show()\n",
    "# t2 = territories.where(\"TerritoryName like '%a%'\")\n",
    "# t3 = t2.groupby('RegionID').count()\n",
    "# t4 = t3.filter('count > 5')\n",
    "\n",
    "# (spark.read.csv('file:///class/2-apache-spark/territories.csv'\n",
    "#                , header=True, inferSchema = True)\n",
    "#       .where(\"TerritoryName like '%a%'\")\n",
    "#       .groupby('RegionID').count()\n",
    "#       .filter('count > 5')\n",
    "#       .show()\n",
    "# )\n",
    "\n",
    "#t4.show()\n",
    "# territories.show()\n",
    "territories.createOrReplaceTempView('territories')\n",
    "spark.sql(\"\"\"SELECT regionid, count(*) as cnt \n",
    "          from territories \n",
    "          where territoryname like '%a%' \n",
    "          group by regionid \n",
    "          order by cnt desc\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LAB: ## \n",
    "### The folder /class/datasets/northwind/ contains sample data in a variety of formats. CSV contains comma separated data without headers, CSVHeaders is the same data with headers. JSON, AVRO, ORC, PARQUET folders have the data in those formats. \n",
    "1. Read the CSVHeaders version of Categories into a DataFrame variable called categories. Print and show it to see what the data looks like.\n",
    "2. Read the JSON version of Products into a DataFrame variable called products. Print and show it to see what the data looks like.\n",
    "3. Using spark sql, turn each DataFrame variable into a temporary view and write a SQL statement to join the two into a new DataFrame variable that shows the ProductID, ProductName, CategoryID and CategoryName.\n",
    "4. Using dot syntax take the joined DataFrame and count how many items are in each category.\n",
    "5. Write the results to HDFS in a folder called /category_count\n",
    "<p></p>\n",
    "\n",
    "<details><summary>Click for <b>hint</b></summary>\n",
    "<p>1. Use spark.read.csv and tell it to the file has headers and infer the schema. Use file:/// prefix to point to the files.</p>\n",
    "    <p>2. Use spark.read.json</p>\n",
    "    <p>3. Turn both DataFrames into a temporary view and write a standard SQL JOIN</p>\n",
    "    <p>4. Use .grouby and .count</p>\n",
    "    <p>5. Take the DataFrame and call .write.csv and save the results using hdfs:// prefix</p>\n",
    "</details>\n",
    "\n",
    "<details><summary>Click for <b>code</b></summary>\n",
    "<p>\n",
    "\n",
    "```python\n",
    "categories = spark.read.csv('file:///class/datasets/northwind/CSVHeaders/categories', header = True, inferSchema = True)\n",
    "products = spark.read.json('file:///class/datasets/northwind/JSON/products')\n",
    "categories.createOrReplaceTempView('categories')\n",
    "products.createOrReplaceTempView('products')\n",
    "prod_cat = spark.sql(\"\"\"SELECT c.CategoryID, c.CategoryName, p.ProductID, p.ProductName\n",
    "FROM categories AS c\n",
    "JOIN Products AS p ON c.CategoryID = p.CategoryID\n",
    "\"\"\")\n",
    "category_count = prod_cat.groupby('CategoryName').count()\n",
    "category_count.show()\n",
    "category_count.write.csv('hdfs://localhost:9000/category_count')\n",
    "```\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create a helper function to stream to a memory table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_memory(df, queryname = 'debug', mode = \"append\"):\n",
    "    # modes are: complete, update, append\n",
    "\n",
    "    # if queryname in spark.catalog.listTables():\n",
    "    #     spark.catalog.dropTempView(queryname)\n",
    "    \n",
    "    query = (df.writeStream \n",
    "            .format(\"memory\")\n",
    "            .queryName(queryname)\n",
    "            .outputMode(mode)\n",
    "            .start()\n",
    "            )\n",
    "    return query\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Define a streaming source and create a temp view to receive the results for debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_topic = 'stocks-json'\n",
    "df = (spark.readStream \n",
    "    .format(\"kafka\") \n",
    "    .option(\"kafka.bootstrap.servers\", brokers) \n",
    "    .option(\"subscribe\", kafka_topic) \n",
    "    .option(\"startingOffsets\", \"earliest\")\n",
    "    .option(\"failOnDataLoss\", False)\n",
    "    .load()\n",
    "    )\n",
    "\n",
    "df.createOrReplaceTempView('table')\n",
    "df1 = spark.sql(\"\"\"SELECT CAST(value as string) as value, 'new data' as newfield from table\"\"\")\n",
    "\n",
    "if 'debug1' in locals():\n",
    "    debug1.stop()\n",
    "\n",
    "#df1 = df.selectExpr(\"UPPER(CAST(value AS STRING)) as value\")\n",
    "\n",
    "debug1 = write_memory(df1, 'debug1')\n",
    "#print(type(debug1))\n",
    "#debug1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Query from the memory stream like it's a temporary view using `spark.sql`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from debug1\").take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## You can stop and restart a memory stream whenever you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug1.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug1 = write_memory(df1, 'debug1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from debug1\").take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Spark SQL magic is also quite helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext sparksql_magic\n",
    "# pip install sparksql-magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sparksql\n",
    "select * from debug1 order by value limit 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Stop a memory stream when you don't need it, as it can consume a lot of memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug1.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's try reading AVRO. First we are using schemaless AVRO messages so we need to read in a schema from a file or repository to apply to the message body to parse it into a structured format. This trick will take an AVRO schema file and turn it into a JSON string which can then be converted into a Spark struct object suitable for use in the deserializing process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_schema = '''{\n",
    "    \"namespace\": \"stock.avro\",\n",
    "    \"type\": \"record\",\n",
    "    \"name\": \"Stock\",\n",
    "    \"fields\": [\n",
    "        {\"name\": \"event_time\", \"type\": \"string\"},\n",
    "        {\"name\": \"symbol\",  \"type\": \"string\"},\n",
    "        {\"name\": \"price\", \"type\": \"float\"},\n",
    "        {\"name\": \"quantity\", \"type\": \"int\"}\n",
    "    ]\n",
    "}'''\n",
    "\n",
    "stock_schema = open(\"stock.avsc\", \"r\").read()\n",
    "print('stock_schema', stock_schema)\n",
    "\n",
    "stock_struct = spark.read.format(\"avro\").option(\"avroSchema\", stock_schema).load().schema\n",
    "print('stock_struct', stock_struct)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## LAB: ## \n",
    "### Using the stocks-json example do the following to start up and read an AVRO stream instead:\n",
    "1. Open a new terminal window and cd /class/1-producers-and-consumers\n",
    "2. Run the 3-python-kafka-avro-producer.py to start making messages\n",
    "3. Open another terminal window and cd /class//1-producers-and-consumers\n",
    "4. Run 4-python-kafka-avro-consumer.py to show the messages are being created and sent.\n",
    "5. In the cells below write Spark code based on the JSON example that can read the AVRO stream and simple display it. \n",
    "\n",
    "<p></p>\n",
    "\n",
    "<details><summary>Click for <b>code</b></summary>\n",
    "<p>\n",
    "\n",
    "```python\n",
    "brokers = 'localhost:9092'\n",
    "kafka_topic = 'stocks-avro'\n",
    "receiver_sleep_time = 4\n",
    "\n",
    "df = (spark.readStream \n",
    "    .format(\"kafka\") \n",
    "    .option(\"kafka.bootstrap.servers\", brokers) \n",
    "    .option(\"subscribe\", kafka_topic) \n",
    "    .option(\"startingOffsets\", \"earliest\")\n",
    "    .option(\"failOnDataLoss\", False)\n",
    "    .load()\n",
    "    )\n",
    "print('df', df)\n",
    "\n",
    "if 'debug2' in locals():\n",
    "    debug2.stop()\n",
    "debug2 = write_memory(df, 'debug2')\n",
    "\n",
    "# In a new cell\n",
    "%%sparksql\n",
    "select timestamp, key, value from debug2 order by timestamp desc limit 10    \n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brokers = 'localhost:9092'\n",
    "kafka_topic = 'stocks-avro'\n",
    "receiver_sleep_time = 4\n",
    "\n",
    "df = (spark.readStream \n",
    "    .format(\"kafka\") \n",
    "    .option(\"kafka.bootstrap.servers\", brokers) \n",
    "    .option(\"subscribe\", kafka_topic) \n",
    "    .option(\"startingOffsets\", \"earliest\")\n",
    "    .option(\"failOnDataLoss\", False)\n",
    "    .load()\n",
    "    )\n",
    "print('df', df)\n",
    "\n",
    "if 'debug2' in locals():\n",
    "    debug2.stop()\n",
    "debug2 = write_memory(df, 'debug2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sparksql\n",
    "select timestamp, key, value from debug2 order by timestamp desc limit 10    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop the memory stream object you created for the AVRO lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug2.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps are to parse the message body and keep whatever metadata from the message we're interested in, and turn that into a DataFrame object we can work with and do whatever we want with the results. In this case in involves converting the key back to a UUID and the message body into Python dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "import uuid\n",
    "\n",
    "def convert_uuid(value: bytes) -> str:\n",
    "    # value is a bytearray in this case coming from spark\n",
    "    ret = uuid.UUID(bytes = bytes(value))\n",
    "    return str(ret)\n",
    "\n",
    "convert_uuid_udf = udf(convert_uuid, StringType())\n",
    "    \n",
    "from pyspark.sql.avro.functions import from_avro, to_avro\n",
    "\n",
    "# Could have read the schema from a file as shown earlier but just put it here so it's easier to see it.\n",
    "stock_schema = \"\"\"{\n",
    "    \"type\": \"record\",\n",
    "    \"name\": \"Stock\",\n",
    "    \"fields\": [\n",
    "        {\"name\": \"event_time\", \"type\": \"string\"},\n",
    "        {\"name\": \"symbol\",  \"type\": \"string\"},\n",
    "        {\"name\": \"price\", \"type\": \"float\"},\n",
    "        {\"name\": \"quantity\", \"type\": \"int\"}\n",
    "    ]\n",
    "}\"\"\"\n",
    "\n",
    "# Select the three columns we're interested in\n",
    "# df3 = df.select(\"timestamp\", \"key\", \"value\")\n",
    "\n",
    "# Do some manipulating on the columns to make them into something meaningful\n",
    "df3 = df.select(\"timestamp\"\n",
    "                , convert_uuid_udf(col(\"key\")).alias(\"key\")\n",
    "                , from_avro(df.value, stock_schema, options = {\"mode\":\"PERMISSIVE\"}).alias(\"value\"))\n",
    "\n",
    "# We end up with three columns called timestamp, key and value, but value is a single column of the datatype\n",
    "# struct, so this trick will flatten it out so we end up with six normal columns.\n",
    "df3 = df3.select(*(df3.columns), col(\"value.*\")).drop('value')\n",
    "\n",
    "#df3 = df3.where(\"symbol = 'GOOG'\")\n",
    "df3.createOrReplaceTempView('stocks')\n",
    "\n",
    "print('df3', df3)\n",
    "if 'debug3' in locals():\n",
    "    debug3.stop()\n",
    "debug3 = write_memory(df3, 'debug3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sparksql\n",
    "select * from debug3 order by timestamp desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug3.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here's the same thing for the JSON stream using the from_json function instead of from_avro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "import uuid\n",
    "\n",
    "brokers = 'localhost:9092'\n",
    "kafka_topic = 'stocks-json'\n",
    "receiver_sleep_time = 4\n",
    "\n",
    "stock_schema = open(\"stock.avsc\", \"r\").read()\n",
    "print('stock_schema', stock_schema)\n",
    "\n",
    "stock_struct = spark.read.format(\"avro\").option(\"avroSchema\", stock_schema).load().schema\n",
    "print('stock_struct', stock_struct)\n",
    "\n",
    "df = (spark.readStream \n",
    "    .format(\"kafka\") \n",
    "    .option(\"kafka.bootstrap.servers\", brokers) \n",
    "    .option(\"subscribe\", kafka_topic) \n",
    "    .option(\"startingOffsets\", \"earliest\")\n",
    "    .option(\"failOnDataLoss\", False)\n",
    "#    .option(\"kafka.group.id\", \"stock-json-spark-group\")\n",
    "    .load()\n",
    "    )\n",
    "print('df', df)\n",
    "\n",
    "\n",
    "def convert_uuid(value):\n",
    "    # value is a bytearray in this case coming from spark\n",
    "    ret = uuid.UUID(bytes = bytes(value))\n",
    "    return str(ret)\n",
    "\n",
    "convert_uuid_udf = udf(convert_uuid, StringType())\n",
    "\n",
    "# keep the key and timestamp and convert the value from bytes to string\n",
    "#df1 = df.select(col(\"key\"), \"timestamp\", expr(\"CAST(value AS STRING) as value\"))\n",
    "df1 = df.select(convert_uuid_udf(col(\"key\")).alias(\"key\"), \"timestamp\", expr(\"CAST(value AS STRING) as value\"))\n",
    "print('df1', df1)\n",
    "\n",
    "# cast the string json to a struct\n",
    "# keep all the columns we selected and convery the JSON string into a struct object and remove the string version\n",
    "df2 = df1.select(*df1.columns, from_json(df1.value, stock_struct).alias(\"value2\")).drop('value')\n",
    "print('df2', df2)\n",
    "\n",
    "# flatten the struct to a normal DataFrame\n",
    "df4 = df2.select(*(df2.columns), col(\"value2.*\")).drop('value2')\n",
    "print('df4', df4)\n",
    "\n",
    "if 'debug4' in locals():\n",
    "    debug4.stop()\n",
    "    \n",
    "debug4 = write_memory(df4, 'debug4')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sparksql \n",
    "select * from debug4 order by event_time desc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug4.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Now that we have a normal DataFrame, let's manipulate it how we want and write the results out to another stream. Try the following, it will fail. Read why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df3)\n",
    "df3.createOrReplaceTempView('stocks2')\n",
    "spark.sql('SELECT symbol, count(*) as cnt, sum(quantity) as qty from stocks2 group by symbol').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming sources can't be aggregated unless we add a window to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_window = (df3.select(\"timestamp\", \"symbol\", \"quantity\")\n",
    "        .withWatermark(\"timestamp\", \"10 seconds\") \n",
    "        .groupBy(\"symbol\", window(\"timestamp\", \"10 seconds\").alias(\"window\")) \n",
    "        .agg(sum(\"quantity\").alias(\"sum\"))\n",
    "        )\n",
    "print(fixed_window)\n",
    "\n",
    "if 'debug5' in locals():\n",
    "    debug5.stop()\n",
    "debug5 = write_memory(fixed_window, 'debug5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can see that we get aggregate by symbol every ten seconds. This data can be written off somewhere like a SQL or NoSQL database or forwarded as a new message to create a streaming aggregation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sparksql\n",
    "select * from debug5 order by window desc, symbol limit 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug5.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sliding windows are similar except you give it two parameters, the first is the total length of the window and the second is the refresh interval. In this case, the windows will overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sliding_window = (df3.select(\"timestamp\", \"symbol\",\"quantity\")\n",
    "        .withWatermark(\"timestamp\", \"10 seconds\") \n",
    "        .groupBy(window(\"timestamp\", \"30 seconds\", \"10 seconds\").alias(\"window\"), \"symbol\") \n",
    "        .agg(sum(\"quantity\").alias(\"sum\"))\n",
    "        )\n",
    "print(sliding_window)\n",
    "\n",
    "debug6 = write_memory(sliding_window, 'debug6')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sparksql\n",
    "select * from debug6 order by window desc, symbol limit 21\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug6.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session Window is similar but used to group data that represents a continuous stream of activity. The time specifies a timeout period or period of inactivity that indicates when a session should end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_window = (df4.select(\"timestamp\", \"symbol\",\"quantity\")\n",
    "        .withWatermark(\"timestamp\", \"10 seconds\") \n",
    "        .groupBy(session_window(\"timestamp\", \"5 minutes\").alias(\"window\"), \"symbol\") \n",
    "        .agg(sum(\"quantity\").alias(\"sum\"))\n",
    "        )\n",
    "print(session_window)\n",
    "\n",
    "debug7 = write_memory(session_window, 'debug7')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's join the streaming aggregation with a static reference table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = sc.parallelize([('AAPL', 'Apple'), ('MSFT', 'Microsoft'), ('GOOG','Google')])\n",
    "stocks = spark.createDataFrame(x, 'symbol:string, name:string')\n",
    "stocks.createOrReplaceTempView('stocks')\n",
    "fixed_window.createOrReplaceTempView('trades')\n",
    "\n",
    "joined_aggregate = spark.sql(\"\"\"\n",
    "SELECT t.*, s.name\n",
    "FROM trades as t\n",
    "JOIN stocks as s on t.symbol = s.symbol\n",
    "\"\"\")\n",
    "\n",
    "debug8 = write_memory(joined_aggregate, 'debug8')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sparksql\n",
    "select * from debug8 order by window desc, symbol limit 9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug8.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
